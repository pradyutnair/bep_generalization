Training dynamics:
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.60732216 |
| loss/dynamics_train_loss   | -8.69      |
| timestep                   | 1          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.49885646 |
| loss/dynamics_train_loss   | -27.2      |
| timestep                   | 2          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.49276668 |
| loss/dynamics_train_loss   | -31.2      |
| timestep                   | 3          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.48454803 |
| loss/dynamics_train_loss   | -33.2      |
| timestep                   | 4          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.46684355 |
| loss/dynamics_train_loss   | -34.5      |
| timestep                   | 5          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4582556 |
| loss/dynamics_train_loss   | -35.4     |
| timestep                   | 6         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.45081568 |
| loss/dynamics_train_loss   | -36.1      |
| timestep                   | 7          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.43217072 |
| loss/dynamics_train_loss   | -36.7      |
| timestep                   | 8          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.42299753 |
| loss/dynamics_train_loss   | -37.2      |
| timestep                   | 9          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.41215557 |
| loss/dynamics_train_loss   | -37.6      |
| timestep                   | 10         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.40451375 |
| loss/dynamics_train_loss   | -38        |
| timestep                   | 11         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3890369 |
| loss/dynamics_train_loss   | -38.3     |
| timestep                   | 12        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.37893656 |
| loss/dynamics_train_loss   | -38.6      |
| timestep                   | 13         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.36359045 |
| loss/dynamics_train_loss   | -38.9      |
| timestep                   | 14         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.35617334 |
| loss/dynamics_train_loss   | -39.2      |
| timestep                   | 15         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3434818 |
| loss/dynamics_train_loss   | -39.5     |
| timestep                   | 16        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3357907 |
| loss/dynamics_train_loss   | -39.7     |
| timestep                   | 17        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.33481932 |
| loss/dynamics_train_loss   | -39.9      |
| timestep                   | 18         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.32105795 |
| loss/dynamics_train_loss   | -40        |
| timestep                   | 19         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.31253734 |
| loss/dynamics_train_loss   | -40.3      |
| timestep                   | 20         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.30839255 |
| loss/dynamics_train_loss   | -40.4      |
| timestep                   | 21         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2994086 |
| loss/dynamics_train_loss   | -40.5     |
| timestep                   | 22        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2905915 |
| loss/dynamics_train_loss   | -40.8     |
| timestep                   | 23        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.28762075 |
| loss/dynamics_train_loss   | -40.8      |
| timestep                   | 24         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2760603 |
| loss/dynamics_train_loss   | -41       |
| timestep                   | 25        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.27415872 |
| loss/dynamics_train_loss   | -41.1      |
| timestep                   | 26         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.25871667 |
| loss/dynamics_train_loss   | -41.2      |
| timestep                   | 27         |
----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.25882  |
| loss/dynamics_train_loss   | -41.4    |
| timestep                   | 28       |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24903631 |
| loss/dynamics_train_loss   | -41.5      |
| timestep                   | 29         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24659339 |
| loss/dynamics_train_loss   | -41.5      |
| timestep                   | 30         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.22768469 |
| loss/dynamics_train_loss   | -41.6      |
| timestep                   | 31         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.22445504 |
| loss/dynamics_train_loss   | -41.7      |
| timestep                   | 32         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21937665 |
| loss/dynamics_train_loss   | -41.8      |
| timestep                   | 33         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20871584 |
| loss/dynamics_train_loss   | -41.9      |
| timestep                   | 34         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20549242 |
| loss/dynamics_train_loss   | -42.1      |
| timestep                   | 35         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20421723 |
| loss/dynamics_train_loss   | -42.2      |
| timestep                   | 36         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19760382 |
| loss/dynamics_train_loss   | -42.2      |
| timestep                   | 37         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19590013 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 38         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1955546 |
| loss/dynamics_train_loss   | -42.3     |
| timestep                   | 39        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19985238 |
| loss/dynamics_train_loss   | -42.4      |
| timestep                   | 40         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1913022 |
| loss/dynamics_train_loss   | -42.4     |
| timestep                   | 41        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19173619 |
| loss/dynamics_train_loss   | -42.5      |
| timestep                   | 42         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18844667 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 43         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18209788 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 44         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18997054 |
| loss/dynamics_train_loss   | -42.7      |
| timestep                   | 45         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18350957 |
| loss/dynamics_train_loss   | -42.8      |
| timestep                   | 46         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17795017 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 47         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1697537 |
| loss/dynamics_train_loss   | -42.9     |
| timestep                   | 48        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17227986 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 49         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17378826 |
| loss/dynamics_train_loss   | -43        |
| timestep                   | 50         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17123716 |
| loss/dynamics_train_loss   | -43.1      |
| timestep                   | 51         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17500475 |
| loss/dynamics_train_loss   | -43.1      |
| timestep                   | 52         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1715127 |
| loss/dynamics_train_loss   | -43.2     |
| timestep                   | 53        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17281727 |
| loss/dynamics_train_loss   | -43.3      |
| timestep                   | 54         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1667569 |
| loss/dynamics_train_loss   | -43.2     |
| timestep                   | 55        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16878065 |
| loss/dynamics_train_loss   | -43.4      |
| timestep                   | 56         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1606273 |
| loss/dynamics_train_loss   | -43.4     |
| timestep                   | 57        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16778451 |
| loss/dynamics_train_loss   | -43.3      |
| timestep                   | 58         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16742288 |
| loss/dynamics_train_loss   | -43.4      |
| timestep                   | 59         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1630245 |
| loss/dynamics_train_loss   | -43.4     |
| timestep                   | 60        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16140163 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 61         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16228664 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 62         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16274655 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 63         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15675375 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 64         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15530226 |
| loss/dynamics_train_loss   | -43.7      |
| timestep                   | 65         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15331669 |
| loss/dynamics_train_loss   | -43.8      |
| timestep                   | 66         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15498035 |
| loss/dynamics_train_loss   | -43.8      |
| timestep                   | 67         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15776983 |
| loss/dynamics_train_loss   | -44        |
| timestep                   | 68         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15303878 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 69         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1526179 |
| loss/dynamics_train_loss   | -44       |
| timestep                   | 70        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15367937 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 71         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15631717 |
| loss/dynamics_train_loss   | -44        |
| timestep                   | 72         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14976767 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 73         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14600201 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 74         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15060051 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 75         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14911929 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 76         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15483376 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 77         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1518956 |
| loss/dynamics_train_loss   | -44.3     |
| timestep                   | 78        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15101963 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 79         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15083556 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 80         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15687343 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 81         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14268559 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 82         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14871588 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 83         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13940883 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 84         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14399353 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 85         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14512992 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 86         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14629504 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 87         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14237589 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 88         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1443372 |
| loss/dynamics_train_loss   | -44.6     |
| timestep                   | 89        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14795892 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 90         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13984771 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 91         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13481112 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 92         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13448346 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 93         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13774048 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 94         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14084044 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 95         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13996339 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 96         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14012979 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 97         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13791855 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 98         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13770354 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 99         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13474256 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 100        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13357642 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 101        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13518834 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 102        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13229734 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 103        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13052562 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 104        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13169672 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 105        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13343453 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 106        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12858464 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 107        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13087009 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 108        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13185826 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 109        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12719342 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 110        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12895465 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 111        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12969957 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 112        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12910834 |
| loss/dynamics_train_loss   | -45.5      |
| timestep                   | 113        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12839772 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 114        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1277713 |
| loss/dynamics_train_loss   | -45.4     |
| timestep                   | 115       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12927856 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 116        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12741001 |
| loss/dynamics_train_loss   | -45.5      |
| timestep                   | 117        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12599818 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 118        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12571916 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 119        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.123784915 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 120         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12722336 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 121        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1230918 |
| loss/dynamics_train_loss   | -45.6     |
| timestep                   | 122       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12437924 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 123        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1264855 |
| loss/dynamics_train_loss   | -45.7     |
| timestep                   | 124       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12538613 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 125        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12466651 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 126        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.122820355 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 127         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12774801 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 128        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13382056 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 129        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12061323 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 130        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.123958625 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 131         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12306696 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 132        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.122781016 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 133         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11884812 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 134        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12018895 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 135        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1229681 |
| loss/dynamics_train_loss   | -45.9     |
| timestep                   | 136       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12029289 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 137        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11684114 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 138        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12144834 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 139        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.120406985 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 140         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115980506 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 141         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11691441 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 142        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11695148 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 143        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1272551 |
| loss/dynamics_train_loss   | -46.2     |
| timestep                   | 144       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11735263 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 145        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11894431 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 146        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11721067 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 147        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12029673 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 148        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11371918 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 149        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11920371 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 150        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11560206 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 151        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.114738844 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 152         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11202037 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 153        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11599924 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 154        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11067416 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 155        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11954522 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 156        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11769029 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 157        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11140798 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 158        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11599536 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 159        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11244086 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 160        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1153091 |
| loss/dynamics_train_loss   | -46.4     |
| timestep                   | 161       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11121124 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 162        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11579075 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 163        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11593008 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 164        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11133468 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 165        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115894996 |
| loss/dynamics_train_loss   | -46.5       |
| timestep                   | 166         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10925069 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 167        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11102633 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 168        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12102302 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 169        |
----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.111084 |
| loss/dynamics_train_loss   | -46.7    |
| timestep                   | 170      |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11076869 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 171        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.111191295 |
| loss/dynamics_train_loss   | -46.8       |
| timestep                   | 172         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11426433 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 173        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10999318 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 174        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11263086 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 175        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11103593 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 176        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10601201 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 177        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10753076 |
| loss/dynamics_train_loss   | -46.8      |
| timestep                   | 178        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.107648775 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 179         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1043918 |
| loss/dynamics_train_loss   | -46.8     |
| timestep                   | 180       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.110340856 |
| loss/dynamics_train_loss   | -46.8       |
| timestep                   | 181         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.111351565 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 182         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10629205 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 183        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10673012 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 184        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1055401 |
| loss/dynamics_train_loss   | -47       |
| timestep                   | 185       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11281085 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 186        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10543933 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 187        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10585457 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 188        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.104278184 |
| loss/dynamics_train_loss   | -46.9       |
| timestep                   | 189         |
-----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.104847 |
| loss/dynamics_train_loss   | -46.9    |
| timestep                   | 190      |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11063103 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 191        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11300492 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 192        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10498359 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 193        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10498302 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 194        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10540299 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 195        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1128211 |
| loss/dynamics_train_loss   | -47       |
| timestep                   | 196       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10181783 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 197        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10468265 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 198        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10801669 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 199        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.102869175 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 200         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10246028 |
| loss/dynamics_train_loss   | -47        |
| timestep                   | 201        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.099730305 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 202         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10534439 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 203        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1056041 |
| loss/dynamics_train_loss   | -47.2     |
| timestep                   | 204       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10185443 |
| loss/dynamics_train_loss   | -47.2      |
| timestep                   | 205        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10943393 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 206        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.116523504 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 207         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.104324736 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 208         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10453614 |
| loss/dynamics_train_loss   | -47.1      |
| timestep                   | 209        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10091575 |
| loss/dynamics_train_loss   | -47.3      |
| timestep                   | 210        |
----------------------------------------------------------------------------
elites:[0, 5, 4, 2, 3] , holdout loss: 0.09623624384403229
num rollout transitions: 250000, reward mean: 4.8253
----------------------------------------------------------------------------------
| alpha                              | 0.952    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -16.1    |
| loss/alpha                         | -0.501   |
| loss/critic1                       | 3.22     |
| loss/critic2                       | 3.21     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9030
----------------------------------------------------------------------------------
| alpha                              | 0.861    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -32.2    |
| loss/alpha                         | -1.49    |
| loss/critic1                       | 3.49     |
| loss/critic2                       | 3.49     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9109
----------------------------------------------------------------------------------
| alpha                              | 0.78     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -46.3    |
| loss/alpha                         | -2.4     |
| loss/critic1                       | 4.87     |
| loss/critic2                       | 4.93     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9097
----------------------------------------------------------------------------------
| alpha                              | 0.709    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.3      |
| loss/actor                         | -59.3    |
| loss/alpha                         | -3.04    |
| loss/critic1                       | 7.06     |
| loss/critic2                       | 7.18     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 4000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8848
----------------------------------------------------------------------------------
| alpha                              | 0.647    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.38     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -72.8    |
| loss/alpha                         | -3.26    |
| loss/critic1                       | 9.36     |
| loss/critic2                       | 9.67     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 5000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8340
----------------------------------------------------------------------------------
| alpha                              | 0.595    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.606   |
| eval/normalized_episode_reward_std | 4.59     |
| loss/actor                         | -88.8    |
| loss/alpha                         | -3.07    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 6000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7988
----------------------------------------------------------------------------------
| alpha                              | 0.551    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.846   |
| eval/normalized_episode_reward_std | 4.25     |
| loss/actor                         | -107     |
| loss/alpha                         | -2.83    |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7702
----------------------------------------------------------------------------------
| alpha                              | 0.509    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.793   |
| eval/normalized_episode_reward_std | 4.86     |
| loss/actor                         | -124     |
| loss/alpha                         | -2.69    |
| loss/critic1                       | 20       |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 8000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7479
----------------------------------------------------------------------------------
| alpha                              | 0.471    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.142   |
| eval/normalized_episode_reward_std | 5.08     |
| loss/actor                         | -140     |
| loss/alpha                         | -2.4     |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 24.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 9000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7593
----------------------------------------------------------------------------------
| alpha                              | 0.437    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.74     |
| eval/normalized_episode_reward_std | 7.09     |
| loss/actor                         | -156     |
| loss/alpha                         | -1.91    |
| loss/critic1                       | 27.9     |
| loss/critic2                       | 28.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 10000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7564
----------------------------------------------------------------------------------
| alpha                              | 0.409    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.0311   |
| eval/normalized_episode_reward_std | 8.36     |
| loss/actor                         | -170     |
| loss/alpha                         | -1.41    |
| loss/critic1                       | 31       |
| loss/critic2                       | 32       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 11000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7626
----------------------------------------------------------------------------------
| alpha                              | 0.386    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.14     |
| eval/normalized_episode_reward_std | 5.25     |
| loss/actor                         | -183     |
| loss/alpha                         | -0.838   |
| loss/critic1                       | 34.2     |
| loss/critic2                       | 35       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 12000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7214
----------------------------------------------------------------------------------
| alpha                              | 0.372    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.56     |
| eval/normalized_episode_reward_std | 6.53     |
| loss/actor                         | -196     |
| loss/alpha                         | -0.322   |
| loss/critic1                       | 38       |
| loss/critic2                       | 38.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.72     |
| timestep                           | 13000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7575
-----------------------------------------------------------------------------------
| alpha                              | 0.367     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 3.6       |
| eval/normalized_episode_reward_std | 10.6      |
| loss/actor                         | -208      |
| loss/alpha                         | -0.000292 |
| loss/critic1                       | 43.2      |
| loss/critic2                       | 44        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.76      |
| timestep                           | 14000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7350
----------------------------------------------------------------------------------
| alpha                              | 0.371    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 3.47     |
| eval/normalized_episode_reward_std | 9.1      |
| loss/actor                         | -219     |
| loss/alpha                         | 0.0895   |
| loss/critic1                       | 45.9     |
| loss/critic2                       | 46.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.73     |
| timestep                           | 15000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7637
----------------------------------------------------------------------------------
| alpha                              | 0.381    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 10.7     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -228     |
| loss/alpha                         | 0.12     |
| loss/critic1                       | 49.3     |
| loss/critic2                       | 49.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 16000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7525
----------------------------------------------------------------------------------
| alpha                              | 0.39     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 6.29     |
| eval/normalized_episode_reward_std | 8.24     |
| loss/actor                         | -238     |
| loss/alpha                         | 0.0642   |
| loss/critic1                       | 48.4     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 17000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7755
----------------------------------------------------------------------------------
| alpha                              | 0.401    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.83     |
| eval/normalized_episode_reward_std | 6.11     |
| loss/actor                         | -247     |
| loss/alpha                         | 0.11     |
| loss/critic1                       | 48.5     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 18000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7965
----------------------------------------------------------------------------------
| alpha                              | 0.417    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 28.8     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -255     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 52.9     |
| loss/critic2                       | 53       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 19000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7573
----------------------------------------------------------------------------------
| alpha                              | 0.43     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31.6     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -262     |
| loss/alpha                         | 0.0788   |
| loss/critic1                       | 52.9     |
| loss/critic2                       | 53.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 20000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7796
----------------------------------------------------------------------------------
| alpha                              | 0.44     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 32.3     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -269     |
| loss/alpha                         | 0.0429   |
| loss/critic1                       | 50.6     |
| loss/critic2                       | 50.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 21000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7688
----------------------------------------------------------------------------------
| alpha                              | 0.447    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31       |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -276     |
| loss/alpha                         | 0.0344   |
| loss/critic1                       | 51       |
| loss/critic2                       | 51.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 22000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7823
----------------------------------------------------------------------------------
| alpha                              | 0.453    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 30.1     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -283     |
| loss/alpha                         | 0.0185   |
| loss/critic1                       | 51.7     |
| loss/critic2                       | 51.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 23000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7746
-----------------------------------------------------------------------------------
| alpha                              | 0.453     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 29.6      |
| eval/normalized_episode_reward_std | 20.8      |
| loss/actor                         | -291      |
| loss/alpha                         | -0.000843 |
| loss/critic1                       | 46.1      |
| loss/critic2                       | 46.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.77      |
| timestep                           | 24000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8257
----------------------------------------------------------------------------------
| alpha                              | 0.457    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 21.4     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -298     |
| loss/alpha                         | 0.0202   |
| loss/critic1                       | 49.3     |
| loss/critic2                       | 49.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 25000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8209
-----------------------------------------------------------------------------------
| alpha                              | 0.458     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 49.8      |
| eval/normalized_episode_reward_std | 4.1       |
| loss/actor                         | -305      |
| loss/alpha                         | -0.000262 |
| loss/critic1                       | 49.4      |
| loss/critic2                       | 49.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.82      |
| timestep                           | 26000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7932
----------------------------------------------------------------------------------
| alpha                              | 0.458    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 35.7     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -313     |
| loss/alpha                         | -0.00838 |
| loss/critic1                       | 49.1     |
| loss/critic2                       | 49.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8232
----------------------------------------------------------------------------------
| alpha                              | 0.457    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.1     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -320     |
| loss/alpha                         | 0.00271  |
| loss/critic1                       | 46.8     |
| loss/critic2                       | 47.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 28000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8353
----------------------------------------------------------------------------------
| alpha                              | 0.458    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.4     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -328     |
| loss/alpha                         | 0.00703  |
| loss/critic1                       | 45.6     |
| loss/critic2                       | 46.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 29000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8266
----------------------------------------------------------------------------------
| alpha                              | 0.459    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.8     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -336     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 44.6     |
| loss/critic2                       | 44.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 30000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7985
----------------------------------------------------------------------------------
| alpha                              | 0.456    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 11.6     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -344     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 41.8     |
| loss/critic2                       | 42.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 31000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8544
----------------------------------------------------------------------------------
| alpha                              | 0.456    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.7     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -351     |
| loss/alpha                         | 0.0154   |
| loss/critic1                       | 48.2     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 32000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8386
----------------------------------------------------------------------------------
| alpha                              | 0.458    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.2     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -358     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 47.1     |
| loss/critic2                       | 47.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 33000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8285
----------------------------------------------------------------------------------
| alpha                              | 0.46     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.8     |
| eval/normalized_episode_reward_std | 7.68     |
| loss/actor                         | -366     |
| loss/alpha                         | -0.00251 |
| loss/critic1                       | 45.6     |
| loss/critic2                       | 46.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 34000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8052
----------------------------------------------------------------------------------
| alpha                              | 0.456    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.4     |
| eval/normalized_episode_reward_std | 4.37     |
| loss/actor                         | -373     |
| loss/alpha                         | -0.0241  |
| loss/critic1                       | 43.8     |
| loss/critic2                       | 44.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 35000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8123
----------------------------------------------------------------------------------
| alpha                              | 0.451    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.8     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -380     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 41.3     |
| loss/critic2                       | 41.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 36000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8176
----------------------------------------------------------------------------------
| alpha                              | 0.446    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.2     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -387     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 36.1     |
| loss/critic2                       | 36.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 37000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8341
----------------------------------------------------------------------------------
| alpha                              | 0.438    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.4     |
| eval/normalized_episode_reward_std | 11.9     |
| loss/actor                         | -394     |
| loss/alpha                         | -0.0378  |
| loss/critic1                       | 36       |
| loss/critic2                       | 35.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 38000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8398
----------------------------------------------------------------------------------
| alpha                              | 0.434    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31       |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -402     |
| loss/alpha                         | -0.00622 |
| loss/critic1                       | 35.5     |
| loss/critic2                       | 35.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 39000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8673
----------------------------------------------------------------------------------
| alpha                              | 0.432    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 28       |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -409     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 38.3     |
| loss/critic2                       | 38       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 40000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8682
----------------------------------------------------------------------------------
| alpha                              | 0.431    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.6     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -416     |
| loss/alpha                         | 0.00517  |
| loss/critic1                       | 41.3     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 41000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8092
----------------------------------------------------------------------------------
| alpha                              | 0.432    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45       |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -422     |
| loss/alpha                         | 0.00222  |
| loss/critic1                       | 40.4     |
| loss/critic2                       | 40.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 42000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8268
----------------------------------------------------------------------------------
| alpha                              | 0.429    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.8     |
| eval/normalized_episode_reward_std | 5.61     |
| loss/actor                         | -429     |
| loss/alpha                         | -0.0403  |
| loss/critic1                       | 40.6     |
| loss/critic2                       | 41.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 43000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8291
----------------------------------------------------------------------------------
| alpha                              | 0.422    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.5     |
| eval/normalized_episode_reward_std | 4.1      |
| loss/actor                         | -435     |
| loss/alpha                         | -0.00949 |
| loss/critic1                       | 39.8     |
| loss/critic2                       | 39.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 44000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8365
----------------------------------------------------------------------------------
| alpha                              | 0.424    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.9     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -440     |
| loss/alpha                         | -0.00187 |
| loss/critic1                       | 36.8     |
| loss/critic2                       | 36.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 45000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8627
----------------------------------------------------------------------------------
| alpha                              | 0.419    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.3     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -446     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 33.4     |
| loss/critic2                       | 33.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 46000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8241
----------------------------------------------------------------------------------
| alpha                              | 0.415    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -452     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 33.8     |
| loss/critic2                       | 33.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 47000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8610
----------------------------------------------------------------------------------
| alpha                              | 0.411    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 9.06     |
| loss/actor                         | -458     |
| loss/alpha                         | -0.0457  |
| loss/critic1                       | 33.9     |
| loss/critic2                       | 33.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 48000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8538
----------------------------------------------------------------------------------
| alpha                              | 0.409    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.1     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -463     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 33       |
| loss/critic2                       | 33       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 49000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8394
----------------------------------------------------------------------------------
| alpha                              | 0.41     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.6     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -468     |
| loss/alpha                         | -0.00556 |
| loss/critic1                       | 32.9     |
| loss/critic2                       | 32.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 50000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8236
----------------------------------------------------------------------------------
| alpha                              | 0.406    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.4     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -474     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 32.2     |
| loss/critic2                       | 31.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 51000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8585
----------------------------------------------------------------------------------
| alpha                              | 0.407    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.1     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -480     |
| loss/alpha                         | 0.0235   |
| loss/critic1                       | 33.5     |
| loss/critic2                       | 33.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 52000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8691
----------------------------------------------------------------------------------
| alpha                              | 0.403    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -486     |
| loss/alpha                         | -0.0522  |
| loss/critic1                       | 32.9     |
| loss/critic2                       | 32.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8631
----------------------------------------------------------------------------------
| alpha                              | 0.399    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -491     |
| loss/alpha                         | -0.00175 |
| loss/critic1                       | 33.5     |
| loss/critic2                       | 33.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8333
----------------------------------------------------------------------------------
| alpha                              | 0.397    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -496     |
| loss/alpha                         | -0.0264  |
| loss/critic1                       | 32.1     |
| loss/critic2                       | 32.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 55000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8630
----------------------------------------------------------------------------------
| alpha                              | 0.395    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -501     |
| loss/alpha                         | -0.0092  |
| loss/critic1                       | 32       |
| loss/critic2                       | 31.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 56000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8729
----------------------------------------------------------------------------------
| alpha                              | 0.391    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 23.4     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -506     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 32.1     |
| loss/critic2                       | 31.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 57000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8685
----------------------------------------------------------------------------------
| alpha                              | 0.388    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -510     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 31.4     |
| loss/critic2                       | 31.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 58000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8449
----------------------------------------------------------------------------------
| alpha                              | 0.385    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.3     |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -515     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 29.9     |
| loss/critic2                       | 30.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 59000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8899
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.3     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -519     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 31.3     |
| loss/critic2                       | 31.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 60000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8865
----------------------------------------------------------------------------------
| alpha                              | 0.379    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.3     |
| eval/normalized_episode_reward_std | 5.86     |
| loss/actor                         | -524     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 31.7     |
| loss/critic2                       | 31.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 61000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8210
----------------------------------------------------------------------------------
| alpha                              | 0.377    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -528     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 31.4     |
| loss/critic2                       | 31.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 62000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8653
----------------------------------------------------------------------------------
| alpha                              | 0.376    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 3.67     |
| loss/actor                         | -532     |
| loss/alpha                         | -0.00977 |
| loss/critic1                       | 30.4     |
| loss/critic2                       | 30.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 63000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8778
----------------------------------------------------------------------------------
| alpha                              | 0.375    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -536     |
| loss/alpha                         | -0.00647 |
| loss/critic1                       | 27.3     |
| loss/critic2                       | 27.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 64000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8471
----------------------------------------------------------------------------------
| alpha                              | 0.371    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.6     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -540     |
| loss/alpha                         | -0.0337  |
| loss/critic1                       | 27.5     |
| loss/critic2                       | 27.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 65000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8606
----------------------------------------------------------------------------------
| alpha                              | 0.367    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.8     |
| eval/normalized_episode_reward_std | 12.6     |
| loss/actor                         | -543     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 27       |
| loss/critic2                       | 26.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 66000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8850
----------------------------------------------------------------------------------
| alpha                              | 0.362    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58       |
| eval/normalized_episode_reward_std | 8.06     |
| loss/actor                         | -547     |
| loss/alpha                         | -0.0248  |
| loss/critic1                       | 26.5     |
| loss/critic2                       | 26       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 67000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8695
----------------------------------------------------------------------------------
| alpha                              | 0.363    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.7     |
| eval/normalized_episode_reward_std | 6.07     |
| loss/actor                         | -551     |
| loss/alpha                         | 0.00268  |
| loss/critic1                       | 27.6     |
| loss/critic2                       | 27.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8542
----------------------------------------------------------------------------------
| alpha                              | 0.361    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -554     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 26       |
| loss/critic2                       | 25.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 69000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8464
----------------------------------------------------------------------------------
| alpha                              | 0.361    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -558     |
| loss/alpha                         | 0.000337 |
| loss/critic1                       | 25.6     |
| loss/critic2                       | 25.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 70000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8352
----------------------------------------------------------------------------------
| alpha                              | 0.357    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -562     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 24.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8652
----------------------------------------------------------------------------------
| alpha                              | 0.357    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -565     |
| loss/alpha                         | -0.00711 |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 72000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8717
----------------------------------------------------------------------------------
| alpha                              | 0.353    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.1     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -568     |
| loss/alpha                         | -0.0407  |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 73000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8623
----------------------------------------------------------------------------------
| alpha                              | 0.351    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.3     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -571     |
| loss/alpha                         | 0.00682  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8660
----------------------------------------------------------------------------------
| alpha                              | 0.35     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -574     |
| loss/alpha                         | -0.0412  |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 75000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8798
----------------------------------------------------------------------------------
| alpha                              | 0.346    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 6.57     |
| loss/actor                         | -577     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 76000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8723
----------------------------------------------------------------------------------
| alpha                              | 0.345    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -579     |
| loss/alpha                         | -0.00368 |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 77000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8645
----------------------------------------------------------------------------------
| alpha                              | 0.343    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -582     |
| loss/alpha                         | -0.0274  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 24.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 78000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8808
----------------------------------------------------------------------------------
| alpha                              | 0.342    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -585     |
| loss/alpha                         | 0.00619  |
| loss/critic1                       | 24.4     |
| loss/critic2                       | 24.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 79000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8747
----------------------------------------------------------------------------------
| alpha                              | 0.342    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -588     |
| loss/alpha                         | 0.0153   |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 80000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8478
----------------------------------------------------------------------------------
| alpha                              | 0.342    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 9.28     |
| loss/actor                         | -590     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 81000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8969
----------------------------------------------------------------------------------
| alpha                              | 0.34     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -592     |
| loss/alpha                         | -0.0263  |
| loss/critic1                       | 26       |
| loss/critic2                       | 25.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 82000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9105
----------------------------------------------------------------------------------
| alpha                              | 0.338    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -595     |
| loss/alpha                         | 0.00655  |
| loss/critic1                       | 24.4     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 83000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8594
----------------------------------------------------------------------------------
| alpha                              | 0.339    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -597     |
| loss/alpha                         | 0.0116   |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 24.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 84000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8750
----------------------------------------------------------------------------------
| alpha                              | 0.337    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.2     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -599     |
| loss/alpha                         | -0.0476  |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 23.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 85000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8463
----------------------------------------------------------------------------------
| alpha                              | 0.334    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -601     |
| loss/alpha                         | -0.00999 |
| loss/critic1                       | 24.1     |
| loss/critic2                       | 24.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 86000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8698
----------------------------------------------------------------------------------
| alpha                              | 0.331    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -603     |
| loss/alpha                         | -0.0256  |
| loss/critic1                       | 22.6     |
| loss/critic2                       | 22.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 87000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8772
----------------------------------------------------------------------------------
| alpha                              | 0.328    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.7     |
| eval/normalized_episode_reward_std | 8.31     |
| loss/actor                         | -605     |
| loss/alpha                         | -0.0219  |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 88000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8387
----------------------------------------------------------------------------------
| alpha                              | 0.327    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -606     |
| loss/alpha                         | -0.0047  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 89000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8902
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -609     |
| loss/alpha                         | -0.0538  |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 90000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8942
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -611     |
| loss/alpha                         | 0.0249   |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 91000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8852
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -614     |
| loss/alpha                         | -0.00962 |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 92000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8892
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -616     |
| loss/alpha                         | 0.0184   |
| loss/critic1                       | 22.4     |
| loss/critic2                       | 22.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 93000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8966
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -618     |
| loss/alpha                         | 0.00583  |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 94000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8787
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -620     |
| loss/alpha                         | -0.00265 |
| loss/critic1                       | 21       |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 95000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8733
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.7     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -622     |
| loss/alpha                         | 0.00802  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8973
----------------------------------------------------------------------------------
| alpha                              | 0.326    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -623     |
| loss/alpha                         | 0.00293  |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 97000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8909
----------------------------------------------------------------------------------
| alpha                              | 0.327    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 4.62     |
| loss/actor                         | -625     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 98000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9088
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -626     |
| loss/alpha                         | -0.0519  |
| loss/critic1                       | 21.9     |
| loss/critic2                       | 22.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 99000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8716
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -628     |
| loss/alpha                         | -0.0217  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 100000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8892
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 101000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8759
----------------------------------------------------------------------------------
| alpha                              | 0.318    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -630     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 102000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8742
----------------------------------------------------------------------------------
| alpha                              | 0.316    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -631     |
| loss/alpha                         | 0.00587  |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 103000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9055
----------------------------------------------------------------------------------
| alpha                              | 0.315    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0274  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 104000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8753
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.00418 |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 105000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9101
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.7     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -634     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8997
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -635     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 107000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8892
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 5.45     |
| loss/actor                         | -636     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 108000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9179
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -637     |
| loss/alpha                         | 0.000138 |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 109000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9027
----------------------------------------------------------------------------------
| alpha                              | 0.307    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -638     |
| loss/alpha                         | -0.0152  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 110000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8976
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.6     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -639     |
| loss/alpha                         | 0.00618  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9170
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -640     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 112000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8806
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -641     |
| loss/alpha                         | -0.00649 |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 113000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8861
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -642     |
| loss/alpha                         | -0.0317  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 114000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9049
-----------------------------------------------------------------------------------
| alpha                              | 0.302     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -644      |
| loss/alpha                         | -7.53e-05 |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 115000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8946
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.00424  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 116000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9097
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -646     |
| loss/alpha                         | 0.00743  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 117000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8854
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -647     |
| loss/alpha                         | -0.0369  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 118000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9071
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 4.91     |
| loss/actor                         | -648     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 119000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9110
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -650     |
| loss/alpha                         | -0.00763 |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 120000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8826
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -651     |
| loss/alpha                         | -0.0003  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 121000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9027
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 3.92     |
| loss/actor                         | -652     |
| loss/alpha                         | -0.00794 |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 122000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9144
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 5.29     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.00598  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 123000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9074
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.3     |
| eval/normalized_episode_reward_std | 8.62     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0314  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 124000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8991
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -656     |
| loss/alpha                         | -0.0425  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 125000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9285
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -657     |
| loss/alpha                         | 0.0472   |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 126000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8995
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.00407 |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 127000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9042
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0211  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 128000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9066
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.00955  |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 129000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9056
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0268   |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9245
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9326
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.00116  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 132000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9284
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.000549 |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 133000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9004
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.00306 |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 134000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9167
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 135000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9145
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.7     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.00296 |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9246
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -669     |
| loss/alpha                         | 0.00853  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 137000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9166
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.00121  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 138000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9242
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 139000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9302
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 140000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9149
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 141000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9259
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0422  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 142000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9169
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.00307 |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8973
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 144000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9359
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9065
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 4.49     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.00682  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 146000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9147
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.9     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -674     |
| loss/alpha                         | -0.00278 |
| loss/critic1                       | 17       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 147000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9421
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.00178 |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 148000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9212
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 9.7      |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9105
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0252  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 150000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9100
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0314  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 151000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9314
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 152000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9500
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0188   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 153000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8992
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.00802  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 154000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9189
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 4.62     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 155000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9168
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.000264 |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 156000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9298
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 157000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9164
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.00978  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 158000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9231
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0292  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 159000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9202
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 160000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9372
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0085   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 161000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9055
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 162000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9273
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 163000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9179
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00968  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 164000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9200
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 165000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9128
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.00827  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 166000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9271
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 167000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9289
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.00322 |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 168000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9212
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0366   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 169000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9219
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0454  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 170000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9300
-----------------------------------------------------------------------------------
| alpha                              | 0.279     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.000215 |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 171000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9057
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 172000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9311
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.00501  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 173000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9328
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 174000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9389
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.00932  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 175000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9342
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00978  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 176000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9123
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00997  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 177000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9263
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0307   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 178000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9335
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0449  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 179000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9188
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 6.66     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 180000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9350
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0019  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 181000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9304
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.8     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0333   |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 182000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9322
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0159   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9443
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0515  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9338
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0262   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 185000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9378
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0383  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 186000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9415
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0244   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 187000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9143
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0363  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 188000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9340
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.000335 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 189000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9280
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0137   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9287
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 191000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9285
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0232   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9189
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 193000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9215
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 194000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8994
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0229   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 195000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9300
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0349   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9187
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 197000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9287
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.00964  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 198000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9332
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -704     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 199000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9447
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 200000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9320
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0053  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 201000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 202000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9202
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.00558  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 203000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9385
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0157   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 204000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9393
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 205000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9308
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00573  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 206000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9322
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.8     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.00621 |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 207000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9474
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00282  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 208000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9433
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00299  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9251
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 210000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9221
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00683  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 211000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9551
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00583  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 212000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9156
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0123   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 213000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9333
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.00248  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 214000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9372
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 215000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9325
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0389  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 216000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9417
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -709     |
| loss/alpha                         | 0.00881  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 217000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9346
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -710     |
| loss/alpha                         | -0.00525 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 218000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9230
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0238  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 219000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9332
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.011   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 220000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9539
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -710     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 221000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9256
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 222000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9258
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.00425  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 223000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9245
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.00207 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9423
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9481
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 226000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9158
----------------------------------------------------------------------------------
| alpha                              | 0.27     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 227000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9373
----------------------------------------------------------------------------------
| alpha                              | 0.27     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 5.8      |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00977  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 228000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9338
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9246
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 230000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9350
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.00519 |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 231000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9374
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 3.67     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.018   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 232000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9197
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0453  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 233000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9369
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 7.92     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9338
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 8.01     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00528  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 235000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0194   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 236000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9394
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 237000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9337
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0164  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9452
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 239000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9313
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 240000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9342
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00128  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 241000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9262
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 5.52     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0204  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 242000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9069
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 243000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9548
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 244000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9184
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 245000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9383
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00351 |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9276
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 9.82     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 247000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9485
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0422   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 248000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9490
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 249000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9350
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0597  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 250000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9375
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 7.42     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00263 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 251000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9440
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 252000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9455
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00488 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 253000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9353
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.93     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0258  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 254000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9474
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0242   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9309
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 4.4      |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0019   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 256000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.026   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 257000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9621
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0245   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9410
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00462 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 259000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9551
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 9.14     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0066  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 260000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9062
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.057    |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 261000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9192
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0255   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 262000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9416
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0114   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9351
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0501  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 264000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9389
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0374   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 265000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9355
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0084   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 266000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9312
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 267000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9399
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9628
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 269000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 270000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9215
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0298  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 271000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9410
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.00473  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 272000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9315
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.8     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0438   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 273000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9312
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0445  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 274000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9494
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 275000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9409
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 5.18     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.0564   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 276000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9482
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00026 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9545
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0437  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 278000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9320
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.0182   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9455
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.029    |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 280000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9357
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.005   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 281000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9329
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 282000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9472
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00193 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 283000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9365
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0254  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 284000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9364
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 285000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9505
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 286000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9428
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0245  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 287000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9359
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 288000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9388
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0453   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 289000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9289
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -724     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 290000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9481
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.00403 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 291000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9462
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.00873 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 292000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9396
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.00471 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 293000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9441
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 294000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9222
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 295000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9572
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0081   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 296000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9399
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.00985 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 297000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9468
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 7.91     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0267   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 298000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9296
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.00921 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9582
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 300000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9434
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0312   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 301000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9313
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 4.33     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 302000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9280
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 8.95     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00447 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 303000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9467
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00575 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 304000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9235
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 305000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9307
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0357   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 306000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9541
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.00128  |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 307000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9393
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.00316  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 308000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9323
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -724     |
| loss/alpha                         | -0.0536  |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 309000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9488
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 310000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9446
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -724     |
| loss/alpha                         | -0.00934 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 311000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9398
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -724     |
| loss/alpha                         | -0.036   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 312000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9439
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.00244  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 313000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9399
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 5.11     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0211   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 314000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9487
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 315000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9615
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0222  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 316000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9357
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.00278  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 317000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9477
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0061  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 318000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9375
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9471
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 9.89     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 320000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9294
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 321000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9444
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 7.03     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.00789  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 322000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9393
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 323000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9478
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.00181  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 324000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9367
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -725     |
| loss/alpha                         | 0.00132  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 325000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9537
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 8.13     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 326000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.0245   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 327000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9220
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0048  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9243
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.0476   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 329000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9399
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -725     |
| loss/alpha                         | -0.058   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9489
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 331000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9547
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -725     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 332000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9440
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 333000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9401
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -726     |
| loss/alpha                         | -0.04    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9298
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -726     |
| loss/alpha                         | -0.0562  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 335000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9469
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -726     |
| loss/alpha                         | 0.0312   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 336000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9656
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -726     |
| loss/alpha                         | -0.00408 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 337000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9451
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.0184   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 338000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9291
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -726     |
| loss/alpha                         | -0.00265 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 339000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9505
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.00188  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 340000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9437
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.0362   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 341000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9456
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -726     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 342000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9578
-----------------------------------------------------------------------------------
| alpha                              | 0.253     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -726      |
| loss/alpha                         | -0.000102 |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 343000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9376
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 10.5     |
| loss/actor                         | -726     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 344000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9439
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9457
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -727     |
| loss/alpha                         | 0.00662  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 346000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9321
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 4.82     |
| loss/actor                         | -727     |
| loss/alpha                         | 0.0232   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 347000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9243
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -727     |
| loss/alpha                         | 0.000961 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 348000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9537
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -727     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 349000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9396
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -728     |
| loss/alpha                         | -0.0295  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 350000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9389
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -728     |
| loss/alpha                         | -0.00239 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 351000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9573
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 4.67     |
| loss/actor                         | -728     |
| loss/alpha                         | 0.0193   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 352000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9376
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -728     |
| loss/alpha                         | 0.000365 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 353000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9436
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 354000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9242
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0522   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 355000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9333
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.00313 |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 356000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9386
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.00243 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 357000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9371
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -731     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 358000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9458
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.00233  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 359000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9391
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0533  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 360000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9410
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.00418  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 361000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9447
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0421  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 362000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9506
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0169   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 363000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9624
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0137   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 364000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9334
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.76     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 365000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9731
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.00532 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9407
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 367000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9436
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 368000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9588
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 369000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9444
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0352  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 370000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9434
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0568   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 371000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9384
-----------------------------------------------------------------------------------
| alpha                              | 0.251     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 4.16      |
| loss/actor                         | -729      |
| loss/alpha                         | -0.000508 |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 372000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9713
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0017  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 373000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9439
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0413  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 374000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9427
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 375000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9434
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 12.2     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 376000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.0237  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 377000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9465
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0044   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 378000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9537
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0066   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 379000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9199
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 380000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9427
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.00274 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9454
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.049    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 382000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9370
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0528   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 383000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9508
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.00389  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 384000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9286
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 8.15     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.00581 |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 385000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9316
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9642
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 8.98     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9437
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.00491 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 388000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9445
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 389000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9408
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 390000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9550
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -730     |
| loss/alpha                         | -0.024   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 391000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9439
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0228   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 392000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9388
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 393000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9513
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 394000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9489
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0446   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 395000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9545
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.75     |
| loss/actor                         | -732     |
| loss/alpha                         | -0.00734 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 396000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9571
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -732     |
| loss/alpha                         | 0.0267   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 397000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -732     |
| loss/alpha                         | -0.00425 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 398000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9416
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 9.55     |
| loss/actor                         | -732     |
| loss/alpha                         | -0.00528 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9460
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0522  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 400000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9529
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9415
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 8.27     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 402000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9549
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 403000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9321
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0336   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 404000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9322
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0366  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9390
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 406000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9507
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0264   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 407000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9508
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.00993  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 408000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9552
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9534
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 410000   |
----------------------------------------------------------------------------------
num rollout transitions: 249999, reward mean: 4.9283
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 4.06     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.043    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 411000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0369  |
| loss/critic1                       | 23       |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 412000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9426
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -730     |
| loss/alpha                         | 0.0524   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9683
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 7.37     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.00483  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 414000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9565
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0165  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 415000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9542
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.00486  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 416000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9455
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0323  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 417000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9519
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 418000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9708
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -730     |
| loss/alpha                         | 0.00267  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 419000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9557
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 420000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9558
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9570
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -732     |
| loss/alpha                         | 0.0387   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 422000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9461
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -732     |
| loss/alpha                         | -0.043   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 423000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9523
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -733     |
| loss/alpha                         | 0.0079   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 424000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9712
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -733     |
| loss/alpha                         | 0.0285   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 425000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9397
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0438  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 426000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9512
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0146  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9355
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 9.2      |
| loss/actor                         | -734     |
| loss/alpha                         | -0.00566 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9710
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 429000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9545
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.0452   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 430000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9466
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 431000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0169  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 432000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9546
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 433000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9651
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.00457  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 434000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9374
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 435000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9510
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9670
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.00842 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 437000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9554
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.00619  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 438000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0228   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9614
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0425   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 440000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9554
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.00702 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 441000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9611
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.00124 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 442000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9550
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.00172 |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 443000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9651
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0179   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 444000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9578
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 5.11     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 445000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9492
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 4.28     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0297  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 446000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9489
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 5.68     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0093  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 447000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9411
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.0278   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 448000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9384
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0487  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 449000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9511
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 450000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9558
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.00595 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 451000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0288   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 452000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9711
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 453000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9407
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0217  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 454000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9638
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 455000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9637
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -735     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 456000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9521
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 457000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 458000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9527
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -736     |
| loss/alpha                         | 0.0451   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.0114  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 460000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9586
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.0528  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 461000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -736     |
| loss/alpha                         | 0.00831  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 462000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9353
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -736     |
| loss/alpha                         | 0.00518  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 463000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9488
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 464000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9639
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 8.86     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.00461 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 465000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9448
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -736     |
| loss/alpha                         | 0.0345   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9647
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -737     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 467000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9559
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0209   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 468000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9552
-----------------------------------------------------------------------------------
| alpha                              | 0.242     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 22.3      |
| loss/actor                         | -737      |
| loss/alpha                         | -0.000696 |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 469000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9564
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 6.21     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0633   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 470000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9630
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -737     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 471000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9686
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -737     |
| loss/alpha                         | -0.0487  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 472000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -737     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 473000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9461
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -737     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 474000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9496
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 6.29     |
| loss/actor                         | -737     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 475000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9541
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0222   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 476000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 5.62     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.00498  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 477000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9535
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -737     |
| loss/alpha                         | -0.00295 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 478000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9711
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0408   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 479000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9598
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0292   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 480000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9585
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 9.72     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.00763 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 481000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9628
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0094  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 482000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9476
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0165  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 483000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9580
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00552  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 484000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0112   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 485000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9471
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00354  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 486000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9573
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.00283 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 487000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9638
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0323  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 488000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9604
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.00387 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 489000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61       |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.00792 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 490000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9423
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0153   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 491000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9618
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 493000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9608
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 7.44     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0297  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 495000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 9.04     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00537  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 496000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9601
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 4.77     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0441   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 497000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9678
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00627 |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 498000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9518
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0291   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 499000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9467
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 500000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9549
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00322 |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 501000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9627
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00024  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 503000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9620
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 7.32     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0213   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 504000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9531
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 11.2     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9617
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00225 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 506000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00806 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 507000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9530
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 508000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9548
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 509000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9446
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0215  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 510000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9509
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 7.04     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0219  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 511000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9554
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.98     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0465  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 512000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9669
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9496
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00333  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 514000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9436
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.053   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 515000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9494
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0322   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 516000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9438
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 517000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9696
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00363  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 518000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9480
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00588  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 519000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9770
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 520000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9495
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.021    |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 521000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9537
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 9.48     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0111   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 522000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9553
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0448   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 523000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9612
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9486
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0226  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 525000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.000892 |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 526000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9485
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0325  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 527000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9677
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 528000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9568
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.00586 |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9346
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0216   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 530000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9560
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.00123 |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 531000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9553
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.00745  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 532000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9647
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0605   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 533000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00541  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 534000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9577
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0424  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9553
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 536000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9638
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0253  |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 537000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00743  |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 538000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9521
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0444   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 539000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9665
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 540000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9563
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.039    |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 541000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9567
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 8.78     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0385  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 542000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9514
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00857 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 543000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9687
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.000388 |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 544000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0376  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 545000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9604
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00485 |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 546000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9540
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 547000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9752
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 4.32     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.049    |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 548000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9483
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.016    |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 549000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9571
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 5.59     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0246   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 550000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 551000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9613
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 552000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9592
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0358  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 553000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9603
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0241  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 554000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9515
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 555000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 556000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9618
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0198   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 557000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9667
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0308   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9526
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.96     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0115   |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9600
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00432 |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 560000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9518
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 561000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9586
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 6.19     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 562000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9690
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 563000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9630
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.019    |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 564000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9642
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0237  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 565000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9466
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0085   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 566000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00205  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 567000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9579
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0534  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 568000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9690
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0023   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9593
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 570000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0318   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 571000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9524
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0578   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9610
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 5.12     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 573000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9585
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 4.01     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00498  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 574000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9461
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0167   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9595
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00226  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 576000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9551
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 577000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9500
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0482  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 578000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0545  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 579000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9683
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0324  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 9.17     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0305   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 581000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -741     |
| loss/alpha                         | 0.00152  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 582000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9641
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.00083  |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 583000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9583
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0442  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 584000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.018    |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 585000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9752
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.00981  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 586000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9709
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 5.22     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 587000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9668
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.00621  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 588000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9530
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.00321 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 589000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9714
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0313   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 590000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9655
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 591000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -742     |
| loss/alpha                         | -0.0436  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 592000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -742     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 593000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9711
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 594000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9582
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0194  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9683
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 6.54     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0482   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 596000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 597000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9694
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0345  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 598000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9725
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 599000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9656
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 5.56     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 600000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9726
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0101   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 601000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9752
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 5.5      |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0357   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 602000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9645
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -741     |
| loss/alpha                         | 0.007    |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 603000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9635
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.84     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00662  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 604000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0411  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 605000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00361  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 606000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00969  |
| loss/critic1                       | 11       |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 607000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9567
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0264   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 608000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00207  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 609000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9575
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0304   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 610000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9555
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 4.51     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 611000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9635
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.00188 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9659
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.3     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0609   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 613000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9670
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.82     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.0527  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 614000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9746
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 8.36     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.0129  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 615000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9622
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 616000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9677
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -742     |
| loss/alpha                         | 0.026    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 617000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9785
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.00694 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 618000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9738
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -743     |
| loss/alpha                         | 0.004    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 619000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9586
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -743     |
| loss/alpha                         | -0.0591  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 620000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -743     |
| loss/alpha                         | 0.0399   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 621000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9577
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -744     |
| loss/alpha                         | 0.0146   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 622000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0199  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 623000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9793
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9602
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -743     |
| loss/alpha                         | 0.00364  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 625000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -743     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 626000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -744     |
| loss/alpha                         | 0.005    |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 627000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9634
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 628000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9586
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.00648  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 629000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0465  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 630000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9724
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0415  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 631000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9734
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.0113   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 632000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9548
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 633000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9646
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.0029   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9647
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -744     |
| loss/alpha                         | 0.00954  |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9725
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 636000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9573
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.0122   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 638000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9645
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.052    |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 639000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9673
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -745     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 10.6     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 640000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9642
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 5.82     |
| loss/actor                         | -745     |
| loss/alpha                         | 0.0328   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 641000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9784
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 4.99     |
| loss/actor                         | -745     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 642000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9591
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.00279 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9740
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0792   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 644000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9591
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 10.9     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0557  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9610
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 5.58     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0298  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 647000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00088  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 648000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 6.66     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0558  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9617
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.000317 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 650000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9645
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.83     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.017    |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 651000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9770
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 5.07     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 652000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9639
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0256  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 653000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00577  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 654000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9682
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.95     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.064    |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 655000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9727
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00154 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 656000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9714
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.027   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 657000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9632
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9687
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 6.31     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00298 |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 659000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00758  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 660000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0416  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0372  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 662000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0181   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 663000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00571  |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 664000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00602  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 665000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 666000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00523 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 667000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9610
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.038   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 668000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 669000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9737
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0667   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 670000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.00318 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 671000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9664
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 672000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0193  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 673000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 674000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9675
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00252  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 675000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0257   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 676000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0483   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 677000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9796
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 678000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9677
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0695  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 679000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9738
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.033   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 680000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00239  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 681000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00247  |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 682000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00112 |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 683000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 684000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 685000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9726
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00635 |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 686000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0532   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 687000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 11       |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 688000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 6.59     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.025    |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 689000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9640
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 8.85     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00585 |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 691000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0319   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 692000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.047    |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 693000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9693
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00411 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 694000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9610
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 695000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9813
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 6.05     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0345  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 696000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9763
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 697000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9801
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0557  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 698000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9740
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.67     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0504  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 699000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 700000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9628
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 701000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9720
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 702000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9747
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 703000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 704000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.00541  |
| loss/critic1                       | 10.4     |
| loss/critic2                       | 10.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 705000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9730
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0136   |
| loss/critic1                       | 11       |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 706000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 707000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.00352  |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 708000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0327   |
| loss/critic1                       | 10.3     |
| loss/critic2                       | 10.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 709000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0148  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 710000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0189   |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 711000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0193   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 712000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9644
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.00164  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 713000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9669
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0639   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 714000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9689
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.76     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0087   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 715000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0697  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 716000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00263  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 717000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9716
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00542 |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 718000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9737
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00487 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 719000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9797
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 8.97     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00614  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 720000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9521
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0318  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 721000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9723
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 722000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9829
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00745  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 723000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 4.83     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.00779  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 724000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00603 |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 725000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9720
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 10       |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 726000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9756
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0487   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 727000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 728000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00149 |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 729000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9819
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.82     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 730000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.014   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9776
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0114   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0284   |
| loss/critic1                       | 11       |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9802
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 734000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00483  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 735000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9842
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0226  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 736000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 8.19     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 737000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 4.05     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.018    |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 738000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9791
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0308   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 739000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9762
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00923 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 740000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9716
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 742000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9694
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0508  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 743000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9753
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 7.9      |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00344 |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 744000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -749     |
| loss/alpha                         | 0.014    |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 746000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 747000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9794
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.029   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00522 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0362  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9872
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00204  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 751000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9813
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 752000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -749     |
| loss/alpha                         | 0.045    |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 753000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9723
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.041    |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 754000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9722
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0164   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 755000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9801
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 756000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9874
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 757000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9768
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0094  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 759000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 12.2     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 760000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9721
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 761000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 4.01     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00268 |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 762000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 763000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00303 |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 764000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00602 |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 765000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9793
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 4.05     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.016    |
| loss/critic1                       | 10.2     |
| loss/critic2                       | 10.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 766000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0263   |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 767000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9827
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0037   |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 768000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0456  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 769000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0894   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 770000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0278  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 771000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 772000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00102 |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00204  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00262 |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 775000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00797  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 776000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00834  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 777000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 778000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00586 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 779000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 780000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9696
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00103 |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 4.04     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00149  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 782000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00744 |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9696
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 784000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 4        |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00957  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 785000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9842
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 786000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9777
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00111  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 787000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00209  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 788000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9774
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.016    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 790000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00409  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 791000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9817
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 4.45     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0415  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 792000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 793000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 5.43     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0243  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9822
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 795000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 796000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9633
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00811 |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 797000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 4.05     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00598 |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 799000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9735
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00858 |
| loss/critic1                       | 10.6     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 800000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9802
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 5.23     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0279   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 801000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0457   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 802000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9702
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 804000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00259 |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 805000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0243   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 806000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9764
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 7.46     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0372   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 807000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0131   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
----------------------------------------------------------------------------------
| alpha                              | 0.225    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00479  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 809000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0336   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 810000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9822
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0359  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 812000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0412  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 813000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 814000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 815000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.00902 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 816000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.00549 |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 817000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0556  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 818000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9823
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 5.77     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0182   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 820000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 821000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.77     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00828  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 822000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0101  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0587  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00148  |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00632  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9770
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.043   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 827000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.86     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0065   |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9805
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0405   |
| loss/critic1                       | 10.1     |
| loss/critic2                       | 10.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 10.4     |
| loss/critic2                       | 10.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 830000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 9.2      |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00153 |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 831000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00493 |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 832000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 833000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 834000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00486  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 835000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0268  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 836000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0378   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9772
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 9.86     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00674 |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 838000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0399  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 839000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9710
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 4.15     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0193   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0464   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 841000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 7.16     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00511 |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 842000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00995  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 843000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0612  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 844000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 845000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0273  |
| loss/critic1                       | 10.6     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 846000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00179 |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 847000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9805
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0106  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 848000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9823
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 10.2     |
| loss/critic2                       | 10.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 849000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 6.4      |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0346   |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 850000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 5.04     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0195   |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00141 |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 852000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9736
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 853000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 854000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 4.79     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00347  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 855000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 8.2      |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00275  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 856000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9863
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.81     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0475   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 857000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.046   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 858000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0533  |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 859000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0127  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 860000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 5.64     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0519   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 861000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9750
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 8.39     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0256   |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 862000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.044   |
| loss/critic1                       | 10.7     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 863000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00617  |
| loss/critic1                       | 10.6     |
| loss/critic2                       | 10.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 864000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9801
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 865000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00535 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 866000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0071   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 867000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0437   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 868000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 869000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0558  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9871
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 871000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0506   |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 872000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.00336  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 873000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 874000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9796
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 875000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 876000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 877000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 878000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.99     |
| loss/actor                         | -748     |
| loss/alpha                         | 0.0345   |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00745 |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 880000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9817
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 881000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9784
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0356   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 882000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 883000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 884000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0079   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 885000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 8.08     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0639   |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 886000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9873
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0649  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 7.11     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.00958 |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 888000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0547  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 889000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00775  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 890000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0334   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 891000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9851
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 12.2     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.00385  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 892000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0394   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9791
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.000306 |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 894000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9776
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0254  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 895000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.76     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0486   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 896000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9814
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0476  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 897000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9772
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 898000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0254  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 899000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 900000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0091   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 901000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9829
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.84     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00116  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 902000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9771
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0562   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 8.35     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0356   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 905000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0346   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 906000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.96     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0552  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 907000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9863
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0633  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 908000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.81     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0452   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 909000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00635 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.93     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0531  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 911000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 912000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
----------------------------------------------------------------------------------
| alpha                              | 0.212    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 913000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| alpha                              | 0.213    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0555   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 914000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9768
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0727   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9842
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.019    |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 916000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 5.25     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 5.26     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 919000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0381   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 920000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0261  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 921000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0425  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 922000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 923000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0114   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 925000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0381  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 926000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 927000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00461 |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 928000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0182  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 929000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9851
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 930000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.00571  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 931000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0237   |
| loss/critic1                       | 10.6     |
| loss/critic2                       | 10.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 932000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00282 |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 933000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0466   |
| loss/critic1                       | 11       |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0224   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9842
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0428   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 936000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0248  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 937000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -752     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 938000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -752     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 939000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -752     |
| loss/alpha                         | -0.0386  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 940000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 4.79     |
| loss/actor                         | -752     |
| loss/alpha                         | -0.0812  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 941000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0038  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 942000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 943000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0254  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 944000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0262  |
| loss/critic1                       | 10.5     |
| loss/critic2                       | 10.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| alpha                              | 0.212    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 946000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| alpha                              | 0.213    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 947000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9840
----------------------------------------------------------------------------------
| alpha                              | 0.213    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00375 |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 948000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9840
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0956   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 949000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 950000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9773
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 12.2     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0349   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 951000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
----------------------------------------------------------------------------------
| alpha                              | 0.224    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0311   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 952000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| alpha                              | 0.223    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 953000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 8.84     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 954000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 955000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 6.06     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00844 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 956000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 957000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00972 |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9827
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0256   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 959000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 960000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9829
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0462   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0413   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 962000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0326  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 963000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0442  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 964000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0371  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 965000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9763
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00707 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 966000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9882
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 967000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0485   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0278   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 969000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9883
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 4.04     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 970000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 971000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0402  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 972000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.75     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0539  |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 973000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9848
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 974000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.214    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00857  |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 975000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0318   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 977000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9783
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 978000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9813
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0249   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9823
----------------------------------------------------------------------------------
| alpha                              | 0.219    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00432  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 980000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 4.11     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 10.9     |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.221    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.0044   |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 982000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00288  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 983000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
----------------------------------------------------------------------------------
| alpha                              | 0.222    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 7.13     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0024  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 984000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
----------------------------------------------------------------------------------
| alpha                              | 0.22     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 985000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 986000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
----------------------------------------------------------------------------------
| alpha                              | 0.218    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 987000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9883
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.83     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00522 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 988000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
----------------------------------------------------------------------------------
| alpha                              | 0.217    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00812 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 989000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| alpha                              | 0.216    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 990000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
----------------------------------------------------------------------------------
| alpha                              | 0.215    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 991000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| alpha                              | 0.213    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -751     |
| loss/alpha                         | -0.031   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| alpha                              | 0.211    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 993000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9879
----------------------------------------------------------------------------------
| alpha                              | 0.208    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0375  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.207    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00607 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 995000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
----------------------------------------------------------------------------------
| alpha                              | 0.207    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00327 |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 996000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.207    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00425 |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 997000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| alpha                              | 0.206    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00799 |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 998000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.206    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0101  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 999000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| alpha                              | 0.206    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00331 |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1000000  |
----------------------------------------------------------------------------------
total time: 57807.86s
