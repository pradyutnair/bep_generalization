Training dynamics:
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.71214545 |
| loss/dynamics_train_loss   | -8.45      |
| timestep                   | 1          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.59803087 |
| loss/dynamics_train_loss   | -27        |
| timestep                   | 2          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.59410596 |
| loss/dynamics_train_loss   | -31.1      |
| timestep                   | 3          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.5713009 |
| loss/dynamics_train_loss   | -33.1     |
| timestep                   | 4         |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.5470908 |
| loss/dynamics_train_loss   | -34.4     |
| timestep                   | 5         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.51504266 |
| loss/dynamics_train_loss   | -35.3      |
| timestep                   | 6          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4924578 |
| loss/dynamics_train_loss   | -36       |
| timestep                   | 7         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.47644624 |
| loss/dynamics_train_loss   | -36.6      |
| timestep                   | 8          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4520926 |
| loss/dynamics_train_loss   | -37.1     |
| timestep                   | 9         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.43703598 |
| loss/dynamics_train_loss   | -37.6      |
| timestep                   | 10         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4226548 |
| loss/dynamics_train_loss   | -38       |
| timestep                   | 11        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.41020918 |
| loss/dynamics_train_loss   | -38.3      |
| timestep                   | 12         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.39550307 |
| loss/dynamics_train_loss   | -38.6      |
| timestep                   | 13         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3826364 |
| loss/dynamics_train_loss   | -38.9     |
| timestep                   | 14        |
---------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.376975 |
| loss/dynamics_train_loss   | -39.1    |
| timestep                   | 15       |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.36872843 |
| loss/dynamics_train_loss   | -39.4      |
| timestep                   | 16         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.35595497 |
| loss/dynamics_train_loss   | -39.7      |
| timestep                   | 17         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.35578114 |
| loss/dynamics_train_loss   | -39.9      |
| timestep                   | 18         |
----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.338633 |
| loss/dynamics_train_loss   | -40      |
| timestep                   | 19       |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.32508016 |
| loss/dynamics_train_loss   | -40.2      |
| timestep                   | 20         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.34031057 |
| loss/dynamics_train_loss   | -40.4      |
| timestep                   | 21         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.31537104 |
| loss/dynamics_train_loss   | -40.5      |
| timestep                   | 22         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3070236 |
| loss/dynamics_train_loss   | -40.6     |
| timestep                   | 23        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.30803287 |
| loss/dynamics_train_loss   | -40.8      |
| timestep                   | 24         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2924617 |
| loss/dynamics_train_loss   | -40.9     |
| timestep                   | 25        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.27705857 |
| loss/dynamics_train_loss   | -41        |
| timestep                   | 26         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.28039417 |
| loss/dynamics_train_loss   | -41.2      |
| timestep                   | 27         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.28454798 |
| loss/dynamics_train_loss   | -41.3      |
| timestep                   | 28         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.25826257 |
| loss/dynamics_train_loss   | -41.4      |
| timestep                   | 29         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2668979 |
| loss/dynamics_train_loss   | -41.5     |
| timestep                   | 30        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24894229 |
| loss/dynamics_train_loss   | -41.6      |
| timestep                   | 31         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24984172 |
| loss/dynamics_train_loss   | -41.7      |
| timestep                   | 32         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24678926 |
| loss/dynamics_train_loss   | -41.7      |
| timestep                   | 33         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24775943 |
| loss/dynamics_train_loss   | -41.8      |
| timestep                   | 34         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.22992714 |
| loss/dynamics_train_loss   | -41.9      |
| timestep                   | 35         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2325345 |
| loss/dynamics_train_loss   | -42       |
| timestep                   | 36        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2358448 |
| loss/dynamics_train_loss   | -42.1     |
| timestep                   | 37        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.2261697 |
| loss/dynamics_train_loss   | -42.2     |
| timestep                   | 38        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21904472 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 39         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21833143 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 40         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21135528 |
| loss/dynamics_train_loss   | -42.4      |
| timestep                   | 41         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21098228 |
| loss/dynamics_train_loss   | -42.5      |
| timestep                   | 42         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20451541 |
| loss/dynamics_train_loss   | -42.5      |
| timestep                   | 43         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21158504 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 44         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19818376 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 45         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19687596 |
| loss/dynamics_train_loss   | -42.7      |
| timestep                   | 46         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19410709 |
| loss/dynamics_train_loss   | -42.8      |
| timestep                   | 47         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19573092 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 48         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19381618 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 49         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18616398 |
| loss/dynamics_train_loss   | -43        |
| timestep                   | 50         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18279281 |
| loss/dynamics_train_loss   | -43        |
| timestep                   | 51         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18216562 |
| loss/dynamics_train_loss   | -43.1      |
| timestep                   | 52         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18493871 |
| loss/dynamics_train_loss   | -43.1      |
| timestep                   | 53         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1781228 |
| loss/dynamics_train_loss   | -43.2     |
| timestep                   | 54        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17680742 |
| loss/dynamics_train_loss   | -43.2      |
| timestep                   | 55         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17224555 |
| loss/dynamics_train_loss   | -43.3      |
| timestep                   | 56         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17852262 |
| loss/dynamics_train_loss   | -43.3      |
| timestep                   | 57         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1785741 |
| loss/dynamics_train_loss   | -43.4     |
| timestep                   | 58        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17740533 |
| loss/dynamics_train_loss   | -43.4      |
| timestep                   | 59         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17144671 |
| loss/dynamics_train_loss   | -43.5      |
| timestep                   | 60         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18269561 |
| loss/dynamics_train_loss   | -43.5      |
| timestep                   | 61         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1624519 |
| loss/dynamics_train_loss   | -43.6     |
| timestep                   | 62        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16371226 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 63         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1671004 |
| loss/dynamics_train_loss   | -43.6     |
| timestep                   | 64        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16372627 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 65         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16146325 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 66         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15526915 |
| loss/dynamics_train_loss   | -43.8      |
| timestep                   | 67         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15854956 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 68         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16031991 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 69         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1651111 |
| loss/dynamics_train_loss   | -44       |
| timestep                   | 70        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15061037 |
| loss/dynamics_train_loss   | -44        |
| timestep                   | 71         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14980312 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 72         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15172605 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 73         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15314314 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 74         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15391766 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 75         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15404737 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 76         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15568507 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 77         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1525908 |
| loss/dynamics_train_loss   | -44.3     |
| timestep                   | 78        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14033931 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 79         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14978683 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 80         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14348581 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 81         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14240769 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 82         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1426063 |
| loss/dynamics_train_loss   | -44.5     |
| timestep                   | 83        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1484936 |
| loss/dynamics_train_loss   | -44.5     |
| timestep                   | 84        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16704449 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 85         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14965317 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 86         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14450388 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 87         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14094226 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 88         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13703683 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 89         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13390018 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 90         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14686713 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 91         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14274777 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 92         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13693987 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 93         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13794331 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 94         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13421538 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 95         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13658495 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 96         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16206695 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 97         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1335381 |
| loss/dynamics_train_loss   | -45       |
| timestep                   | 98        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14102413 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 99         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14103137 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 100        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14247906 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 101        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14285645 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 102        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13313769 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 103        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14006327 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 104        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12961017 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 105        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13275594 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 106        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12816243 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 107        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13478582 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 108        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1329371 |
| loss/dynamics_train_loss   | -45.3     |
| timestep                   | 109       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12794283 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 110        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13308077 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 111        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13119648 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 112        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13390228 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 113        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13321818 |
| loss/dynamics_train_loss   | -45.5      |
| timestep                   | 114        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14796105 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 115        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12927243 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 116        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12796775 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 117        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12655015 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 118        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1277601 |
| loss/dynamics_train_loss   | -45.6     |
| timestep                   | 119       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13013795 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 120        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12247612 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 121        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12471934 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 122        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12147544 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 123        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12783554 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 124        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13959348 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 125        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12625399 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 126        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1272693 |
| loss/dynamics_train_loss   | -45.7     |
| timestep                   | 127       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12267057 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 128        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.122840405 |
| loss/dynamics_train_loss   | -45.8       |
| timestep                   | 129         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12148714 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 130        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12840563 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 131        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12913279 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 132        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12598439 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 133        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1262944 |
| loss/dynamics_train_loss   | -45.9     |
| timestep                   | 134       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.121185854 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 135         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12557587 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 136        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1254065 |
| loss/dynamics_train_loss   | -46       |
| timestep                   | 137       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13265306 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 138        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11694697 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 139        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12015309 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 140        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115497634 |
| loss/dynamics_train_loss   | -46.1       |
| timestep                   | 141         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11957774 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 142        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12803844 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 143        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11746682 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 144        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115259334 |
| loss/dynamics_train_loss   | -46.1       |
| timestep                   | 145         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12011057 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 146        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11967075 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 147        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12244816 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 148        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12103148 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 149        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11598543 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 150        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11677426 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 151        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11753398 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 152        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11861837 |
| loss/dynamics_train_loss   | -46.3      |
| timestep                   | 153        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12277172 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 154        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12685418 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 155        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12946002 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 156        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115593076 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 157         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11663175 |
| loss/dynamics_train_loss   | -46.4      |
| timestep                   | 158        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.112557136 |
| loss/dynamics_train_loss   | -46.6       |
| timestep                   | 159         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.120738566 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 160         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11296356 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 161        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11480067 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 162        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12102344 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 163        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.10886015 |
| loss/dynamics_train_loss   | -46.6      |
| timestep                   | 164        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11575768 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 165        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1206913 |
| loss/dynamics_train_loss   | -46.5     |
| timestep                   | 166       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.114249624 |
| loss/dynamics_train_loss   | -46.5       |
| timestep                   | 167         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11476555 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 168        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11264704 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 169        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11478503 |
| loss/dynamics_train_loss   | -46.5      |
| timestep                   | 170        |
----------------------------------------------------------------------------
elites:[6, 5, 3, 2, 0] , holdout loss: 0.106898233294487
num rollout transitions: 250000, reward mean: 4.8696
----------------------------------------------------------------------------------
| alpha                              | 0.952    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.25     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -16      |
| loss/alpha                         | -0.501   |
| loss/critic1                       | 3.23     |
| loss/critic2                       | 3.2      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8905
----------------------------------------------------------------------------------
| alpha                              | 0.861    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.25     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -32.1    |
| loss/alpha                         | -1.49    |
| loss/critic1                       | 3.5      |
| loss/critic2                       | 3.49     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8992
----------------------------------------------------------------------------------
| alpha                              | 0.78     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -46.3    |
| loss/alpha                         | -2.39    |
| loss/critic1                       | 5        |
| loss/critic2                       | 4.96     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9148
----------------------------------------------------------------------------------
| alpha                              | 0.709    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.22     |
| eval/normalized_episode_reward_std | 2.27     |
| loss/actor                         | -59.3    |
| loss/alpha                         | -3.01    |
| loss/critic1                       | 7.03     |
| loss/critic2                       | 6.88     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 4000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8717
----------------------------------------------------------------------------------
| alpha                              | 0.648    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.071    |
| eval/normalized_episode_reward_std | 4.82     |
| loss/actor                         | -73.3    |
| loss/alpha                         | -3.15    |
| loss/critic1                       | 9.69     |
| loss/critic2                       | 9.38     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 5000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8475
----------------------------------------------------------------------------------
| alpha                              | 0.597    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.698   |
| eval/normalized_episode_reward_std | 4.09     |
| loss/actor                         | -89.9    |
| loss/alpha                         | -2.94    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 6000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8086
----------------------------------------------------------------------------------
| alpha                              | 0.552    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.23    |
| eval/normalized_episode_reward_std | 5.02     |
| loss/actor                         | -108     |
| loss/alpha                         | -2.79    |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7753
----------------------------------------------------------------------------------
| alpha                              | 0.51     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.4     |
| eval/normalized_episode_reward_std | 4.76     |
| loss/actor                         | -125     |
| loss/alpha                         | -2.68    |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 22.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 8000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7313
----------------------------------------------------------------------------------
| alpha                              | 0.471    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.61    |
| eval/normalized_episode_reward_std | 4.38     |
| loss/actor                         | -141     |
| loss/alpha                         | -2.4     |
| loss/critic1                       | 26.5     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.73     |
| timestep                           | 9000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7352
----------------------------------------------------------------------------------
| alpha                              | 0.437    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.21    |
| eval/normalized_episode_reward_std | 4.79     |
| loss/actor                         | -156     |
| loss/alpha                         | -1.89    |
| loss/critic1                       | 29       |
| loss/critic2                       | 28.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.74     |
| timestep                           | 10000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7589
----------------------------------------------------------------------------------
| alpha                              | 0.41     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.834    |
| eval/normalized_episode_reward_std | 6.53     |
| loss/actor                         | -171     |
| loss/alpha                         | -1.33    |
| loss/critic1                       | 31.4     |
| loss/critic2                       | 30.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 11000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7823
----------------------------------------------------------------------------------
| alpha                              | 0.388    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.0641  |
| eval/normalized_episode_reward_std | 4.72     |
| loss/actor                         | -185     |
| loss/alpha                         | -0.781   |
| loss/critic1                       | 34.5     |
| loss/critic2                       | 33.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 12000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7204
----------------------------------------------------------------------------------
| alpha                              | 0.374    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.885   |
| eval/normalized_episode_reward_std | 3.81     |
| loss/actor                         | -198     |
| loss/alpha                         | -0.338   |
| loss/critic1                       | 33.9     |
| loss/critic2                       | 33       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.72     |
| timestep                           | 13000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7848
----------------------------------------------------------------------------------
| alpha                              | 0.367    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.98     |
| eval/normalized_episode_reward_std | 5.84     |
| loss/actor                         | -210     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 36.8     |
| loss/critic2                       | 36.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 14000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7703
----------------------------------------------------------------------------------
| alpha                              | 0.372    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.848    |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -222     |
| loss/alpha                         | 0.144    |
| loss/critic1                       | 38.7     |
| loss/critic2                       | 38       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 15000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7948
----------------------------------------------------------------------------------
| alpha                              | 0.384    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 6.55     |
| eval/normalized_episode_reward_std | 9.91     |
| loss/actor                         | -233     |
| loss/alpha                         | 0.136    |
| loss/critic1                       | 39.8     |
| loss/critic2                       | 39.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 16000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7524
----------------------------------------------------------------------------------
| alpha                              | 0.397    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 7.61     |
| eval/normalized_episode_reward_std | 6.87     |
| loss/actor                         | -243     |
| loss/alpha                         | 0.0781   |
| loss/critic1                       | 42.6     |
| loss/critic2                       | 42.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 17000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7595
----------------------------------------------------------------------------------
| alpha                              | 0.406    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 12       |
| eval/normalized_episode_reward_std | 9.67     |
| loss/actor                         | -252     |
| loss/alpha                         | 0.0834   |
| loss/critic1                       | 45.8     |
| loss/critic2                       | 44.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 18000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7512
----------------------------------------------------------------------------------
| alpha                              | 0.418    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 5.88     |
| eval/normalized_episode_reward_std | 7.58     |
| loss/actor                         | -261     |
| loss/alpha                         | 0.0577   |
| loss/critic1                       | 46.8     |
| loss/critic2                       | 45.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 19000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7944
----------------------------------------------------------------------------------
| alpha                              | 0.425    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 12.4     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -270     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 45.8     |
| loss/critic2                       | 45.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 20000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7980
----------------------------------------------------------------------------------
| alpha                              | 0.428    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 32.7     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -278     |
| loss/alpha                         | 0.0359   |
| loss/critic1                       | 45       |
| loss/critic2                       | 44.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 21000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7900
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 14.7     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -286     |
| loss/alpha                         | 0.0517   |
| loss/critic1                       | 71.6     |
| loss/critic2                       | 70.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 22000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8149
----------------------------------------------------------------------------------
| alpha                              | 0.444    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 29.6     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -293     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 50.4     |
| loss/critic2                       | 49.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 23000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8211
----------------------------------------------------------------------------------
| alpha                              | 0.448    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 38       |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -301     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 50.8     |
| loss/critic2                       | 49.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 24000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7758
----------------------------------------------------------------------------------
| alpha                              | 0.45     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.6     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -309     |
| loss/alpha                         | 0.0018   |
| loss/critic1                       | 48.5     |
| loss/critic2                       | 47.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 25000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8130
----------------------------------------------------------------------------------
| alpha                              | 0.451    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31.6     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -317     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 47.9     |
| loss/critic2                       | 47.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 26000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8124
----------------------------------------------------------------------------------
| alpha                              | 0.454    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 24.5     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -325     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 48.4     |
| loss/critic2                       | 48       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8322
----------------------------------------------------------------------------------
| alpha                              | 0.454    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 9.42     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -332     |
| loss/alpha                         | 0.00497  |
| loss/critic1                       | 47.9     |
| loss/critic2                       | 47.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 28000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8581
----------------------------------------------------------------------------------
| alpha                              | 0.457    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.1     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -340     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 51.3     |
| loss/critic2                       | 51.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 29000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8376
----------------------------------------------------------------------------------
| alpha                              | 0.463    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42.6     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -348     |
| loss/alpha                         | 0.0274   |
| loss/critic1                       | 50.3     |
| loss/critic2                       | 50.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 30000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7974
----------------------------------------------------------------------------------
| alpha                              | 0.465    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.3     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -355     |
| loss/alpha                         | -0.00334 |
| loss/critic1                       | 49.1     |
| loss/critic2                       | 49.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 31000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8007
----------------------------------------------------------------------------------
| alpha                              | 0.465    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.7     |
| eval/normalized_episode_reward_std | 6.27     |
| loss/actor                         | -363     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 49.8     |
| loss/critic2                       | 50.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 32000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8048
----------------------------------------------------------------------------------
| alpha                              | 0.468    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.6     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -370     |
| loss/alpha                         | 0.00224  |
| loss/critic1                       | 46.8     |
| loss/critic2                       | 46.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 33000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8285
----------------------------------------------------------------------------------
| alpha                              | 0.468    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.8     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -378     |
| loss/alpha                         | 0.00342  |
| loss/critic1                       | 40.2     |
| loss/critic2                       | 40.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 34000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8436
----------------------------------------------------------------------------------
| alpha                              | 0.465    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.5     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -385     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 41.2     |
| loss/critic2                       | 41.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 35000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8524
----------------------------------------------------------------------------------
| alpha                              | 0.462    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27.9     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -393     |
| loss/alpha                         | -0.00745 |
| loss/critic1                       | 41.2     |
| loss/critic2                       | 41.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 36000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8601
----------------------------------------------------------------------------------
| alpha                              | 0.461    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54       |
| eval/normalized_episode_reward_std | 4.36     |
| loss/actor                         | -400     |
| loss/alpha                         | -0.00226 |
| loss/critic1                       | 44.2     |
| loss/critic2                       | 44.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 37000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8323
----------------------------------------------------------------------------------
| alpha                              | 0.462    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.1     |
| eval/normalized_episode_reward_std | 9.14     |
| loss/actor                         | -408     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 42.1     |
| loss/critic2                       | 41.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 38000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8303
----------------------------------------------------------------------------------
| alpha                              | 0.465    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.2     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -415     |
| loss/alpha                         | -0.00405 |
| loss/critic1                       | 41.4     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 39000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8425
----------------------------------------------------------------------------------
| alpha                              | 0.461    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.5     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -422     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 41.7     |
| loss/critic2                       | 41.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 40000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8529
----------------------------------------------------------------------------------
| alpha                              | 0.459    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.7     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -430     |
| loss/alpha                         | -0.00886 |
| loss/critic1                       | 41.1     |
| loss/critic2                       | 40.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 41000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8145
----------------------------------------------------------------------------------
| alpha                              | 0.455    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 36.7     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -437     |
| loss/alpha                         | -0.00829 |
| loss/critic1                       | 38.7     |
| loss/critic2                       | 39       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 42000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8480
----------------------------------------------------------------------------------
| alpha                              | 0.453    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.6     |
| eval/normalized_episode_reward_std | 7.87     |
| loss/actor                         | -444     |
| loss/alpha                         | -0.017   |
| loss/critic1                       | 40.3     |
| loss/critic2                       | 39.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 43000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8345
----------------------------------------------------------------------------------
| alpha                              | 0.448    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.7     |
| eval/normalized_episode_reward_std | 8.92     |
| loss/actor                         | -451     |
| loss/alpha                         | -0.0248  |
| loss/critic1                       | 39.9     |
| loss/critic2                       | 39.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 44000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8756
-----------------------------------------------------------------------------------
| alpha                              | 0.447     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 15        |
| eval/normalized_episode_reward_std | 12.3      |
| loss/actor                         | -457      |
| loss/alpha                         | -0.000816 |
| loss/critic1                       | 40.5      |
| loss/critic2                       | 40.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 45000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8950
----------------------------------------------------------------------------------
| alpha                              | 0.445    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.7     |
| eval/normalized_episode_reward_std | 16.4     |
| loss/actor                         | -464     |
| loss/alpha                         | 0.00722  |
| loss/critic1                       | 41.9     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 46000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8523
----------------------------------------------------------------------------------
| alpha                              | 0.444    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.6     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -470     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 41.2     |
| loss/critic2                       | 40.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 47000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8443
----------------------------------------------------------------------------------
| alpha                              | 0.442    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.7     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -476     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 39.3     |
| loss/critic2                       | 38.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 48000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8659
----------------------------------------------------------------------------------
| alpha                              | 0.438    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.9     |
| eval/normalized_episode_reward_std | 7.31     |
| loss/actor                         | -482     |
| loss/alpha                         | -0.00892 |
| loss/critic1                       | 38.2     |
| loss/critic2                       | 37.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 49000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8486
----------------------------------------------------------------------------------
| alpha                              | 0.434    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.2     |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -488     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 37.4     |
| loss/critic2                       | 36.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 50000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8623
----------------------------------------------------------------------------------
| alpha                              | 0.429    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.7     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -494     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 34.5     |
| loss/critic2                       | 34.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 51000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8613
----------------------------------------------------------------------------------
| alpha                              | 0.423    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.4     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -499     |
| loss/alpha                         | -0.04    |
| loss/critic1                       | 34       |
| loss/critic2                       | 33.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 52000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8492
----------------------------------------------------------------------------------
| alpha                              | 0.419    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.5     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -504     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 34.2     |
| loss/critic2                       | 33.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8712
----------------------------------------------------------------------------------
| alpha                              | 0.415    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.5     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -510     |
| loss/alpha                         | -0.00685 |
| loss/critic1                       | 33.8     |
| loss/critic2                       | 33.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8745
----------------------------------------------------------------------------------
| alpha                              | 0.415    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.2     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -515     |
| loss/alpha                         | -0.00494 |
| loss/critic1                       | 33.3     |
| loss/critic2                       | 32.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 55000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8930
----------------------------------------------------------------------------------
| alpha                              | 0.416    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 14.1     |
| eval/normalized_episode_reward_std | 9.85     |
| loss/actor                         | -520     |
| loss/alpha                         | -0.0072  |
| loss/critic1                       | 34.5     |
| loss/critic2                       | 34.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 56000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8963
----------------------------------------------------------------------------------
| alpha                              | 0.413    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.1     |
| eval/normalized_episode_reward_std | 6.31     |
| loss/actor                         | -525     |
| loss/alpha                         | 0.00678  |
| loss/critic1                       | 39       |
| loss/critic2                       | 38.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 57000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8668
----------------------------------------------------------------------------------
| alpha                              | 0.414    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.9     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -529     |
| loss/alpha                         | -0.00627 |
| loss/critic1                       | 37.4     |
| loss/critic2                       | 36.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 58000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8567
----------------------------------------------------------------------------------
| alpha                              | 0.413    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.1     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -533     |
| loss/alpha                         | -0.00431 |
| loss/critic1                       | 37.7     |
| loss/critic2                       | 37       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 59000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8486
----------------------------------------------------------------------------------
| alpha                              | 0.409    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31.6     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -537     |
| loss/alpha                         | -0.0583  |
| loss/critic1                       | 36.6     |
| loss/critic2                       | 36.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 60000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9033
----------------------------------------------------------------------------------
| alpha                              | 0.4      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.4     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -541     |
| loss/alpha                         | -0.0172  |
| loss/critic1                       | 35.9     |
| loss/critic2                       | 35.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 61000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8874
----------------------------------------------------------------------------------
| alpha                              | 0.399    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.1     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -545     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 30.6     |
| loss/critic2                       | 29.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 62000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8539
----------------------------------------------------------------------------------
| alpha                              | 0.396    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 32.1     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -549     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 30.9     |
| loss/critic2                       | 30.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 63000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9003
----------------------------------------------------------------------------------
| alpha                              | 0.393    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -553     |
| loss/alpha                         | -0.0465  |
| loss/critic1                       | 32.5     |
| loss/critic2                       | 32.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 64000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8830
----------------------------------------------------------------------------------
| alpha                              | 0.386    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.3     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -556     |
| loss/alpha                         | -0.0158  |
| loss/critic1                       | 30.3     |
| loss/critic2                       | 29.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 65000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9119
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -560     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 31       |
| loss/critic2                       | 30.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 66000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8817
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -563     |
| loss/alpha                         | 0.0131   |
| loss/critic1                       | 31.3     |
| loss/critic2                       | 30.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 67000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8772
----------------------------------------------------------------------------------
| alpha                              | 0.384    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61       |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -567     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 31       |
| loss/critic2                       | 30.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8990
----------------------------------------------------------------------------------
| alpha                              | 0.379    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27       |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -570     |
| loss/alpha                         | -0.0409  |
| loss/critic1                       | 29.9     |
| loss/critic2                       | 29.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 69000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9180
----------------------------------------------------------------------------------
| alpha                              | 0.376    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -574     |
| loss/alpha                         | 0.00787  |
| loss/critic1                       | 29.4     |
| loss/critic2                       | 29.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 70000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8719
----------------------------------------------------------------------------------
| alpha                              | 0.374    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -578     |
| loss/alpha                         | -0.0326  |
| loss/critic1                       | 27.8     |
| loss/critic2                       | 27.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8772
----------------------------------------------------------------------------------
| alpha                              | 0.37     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -581     |
| loss/alpha                         | -0.0193  |
| loss/critic1                       | 28       |
| loss/critic2                       | 27.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 72000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8787
----------------------------------------------------------------------------------
| alpha                              | 0.366    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -584     |
| loss/alpha                         | -0.044   |
| loss/critic1                       | 27.6     |
| loss/critic2                       | 27.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 73000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8946
----------------------------------------------------------------------------------
| alpha                              | 0.362    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.9     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -587     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 28.4     |
| loss/critic2                       | 27.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9090
----------------------------------------------------------------------------------
| alpha                              | 0.36     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 3.93     |
| loss/actor                         | -591     |
| loss/alpha                         | -0.00808 |
| loss/critic1                       | 29.6     |
| loss/critic2                       | 29       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 75000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9021
----------------------------------------------------------------------------------
| alpha                              | 0.357    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.1     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -594     |
| loss/alpha                         | -0.0438  |
| loss/critic1                       | 26.9     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 76000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9082
----------------------------------------------------------------------------------
| alpha                              | 0.357    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.2     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -598     |
| loss/alpha                         | 0.064    |
| loss/critic1                       | 35.1     |
| loss/critic2                       | 34.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 77000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8990
----------------------------------------------------------------------------------
| alpha                              | 0.364    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.3     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -601     |
| loss/alpha                         | -0.00427 |
| loss/critic1                       | 29       |
| loss/critic2                       | 28.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 78000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8936
----------------------------------------------------------------------------------
| alpha                              | 0.359    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.5     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -604     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 28.2     |
| loss/critic2                       | 27.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 79000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9036
----------------------------------------------------------------------------------
| alpha                              | 0.358    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.2     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -607     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 27.2     |
| loss/critic2                       | 26.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 80000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8987
----------------------------------------------------------------------------------
| alpha                              | 0.353    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -609     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 81000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9126
----------------------------------------------------------------------------------
| alpha                              | 0.349    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -612     |
| loss/alpha                         | -0.0293  |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 25.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 82000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9036
----------------------------------------------------------------------------------
| alpha                              | 0.345    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -615     |
| loss/alpha                         | -0.0111  |
| loss/critic1                       | 26.8     |
| loss/critic2                       | 26.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 83000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9034
----------------------------------------------------------------------------------
| alpha                              | 0.347    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 11.2     |
| loss/actor                         | -618     |
| loss/alpha                         | 0.00922  |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 25.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 84000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8995
----------------------------------------------------------------------------------
| alpha                              | 0.344    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.4     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -621     |
| loss/alpha                         | -0.0392  |
| loss/critic1                       | 25.5     |
| loss/critic2                       | 25       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 85000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9200
----------------------------------------------------------------------------------
| alpha                              | 0.341    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -623     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 27.6     |
| loss/critic2                       | 27.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 86000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9115
----------------------------------------------------------------------------------
| alpha                              | 0.339    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -626     |
| loss/alpha                         | 0.0112   |
| loss/critic1                       | 27.9     |
| loss/critic2                       | 27.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 87000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8915
----------------------------------------------------------------------------------
| alpha                              | 0.34     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -628     |
| loss/alpha                         | -0.0106  |
| loss/critic1                       | 26.7     |
| loss/critic2                       | 26.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 88000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9023
----------------------------------------------------------------------------------
| alpha                              | 0.341    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -631     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 26       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 89000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8853
----------------------------------------------------------------------------------
| alpha                              | 0.341    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -633     |
| loss/alpha                         | -0.0135  |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 24.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 90000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8960
----------------------------------------------------------------------------------
| alpha                              | 0.338    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56       |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 91000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9169
----------------------------------------------------------------------------------
| alpha                              | 0.336    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -637     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 92000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8937
----------------------------------------------------------------------------------
| alpha                              | 0.334    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55       |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -639     |
| loss/alpha                         | -0.00545 |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 93000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9331
----------------------------------------------------------------------------------
| alpha                              | 0.333    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -642     |
| loss/alpha                         | -0.0191  |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 94000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8603
----------------------------------------------------------------------------------
| alpha                              | 0.333    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.00337 |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 22.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 95000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9018
----------------------------------------------------------------------------------
| alpha                              | 0.332    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.00488  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9038
----------------------------------------------------------------------------------
| alpha                              | 0.332    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -647     |
| loss/alpha                         | 0.00437  |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 97000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8974
----------------------------------------------------------------------------------
| alpha                              | 0.331    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.2     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -649     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 23.3     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 98000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9188
----------------------------------------------------------------------------------
| alpha                              | 0.329    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 14       |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -651     |
| loss/alpha                         | 0.00731  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 99000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| alpha                              | 0.331    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.00805  |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 24.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 100000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8790
----------------------------------------------------------------------------------
| alpha                              | 0.33     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -654     |
| loss/alpha                         | -0.0253  |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 101000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9107
----------------------------------------------------------------------------------
| alpha                              | 0.327    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.00775  |
| loss/critic1                       | 24.2     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 102000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9049
----------------------------------------------------------------------------------
| alpha                              | 0.328    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 103000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9359
----------------------------------------------------------------------------------
| alpha                              | 0.327    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -660     |
| loss/alpha                         | -0.0234  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 104000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9222
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0429  |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 105000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9305
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.5     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0299   |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9047
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 8.05     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 22.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 107000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9095
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 5.96     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 22.3     |
| loss/critic2                       | 22.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 108000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9333
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 10.8     |
| eval/normalized_episode_reward_std | 9.41     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0178   |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 22.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 109000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9614
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 9.1      |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0305   |
| loss/critic1                       | 27.4     |
| loss/critic2                       | 27       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 110000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9285
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.00573  |
| loss/critic1                       | 27.3     |
| loss/critic2                       | 26.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9408
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 4.55     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0067  |
| loss/critic1                       | 27.8     |
| loss/critic2                       | 27.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 112000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9509
----------------------------------------------------------------------------------
| alpha                              | 0.326    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0494   |
| loss/critic1                       | 26.9     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 113000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9003
----------------------------------------------------------------------------------
| alpha                              | 0.326    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56       |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0663  |
| loss/critic1                       | 24.5     |
| loss/critic2                       | 24.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 114000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8996
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 115000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| alpha                              | 0.317    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.00771 |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 116000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9378
----------------------------------------------------------------------------------
| alpha                              | 0.317    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 8.37     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.00924  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 117000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9433
----------------------------------------------------------------------------------
| alpha                              | 0.316    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 4.04     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.048   |
| loss/critic1                       | 22       |
| loss/critic2                       | 22.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 118000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9382
----------------------------------------------------------------------------------
| alpha                              | 0.315    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0362   |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 119000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9509
----------------------------------------------------------------------------------
| alpha                              | 0.318    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 10.3     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00621  |
| loss/critic1                       | 22.8     |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 120000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9465
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -689     |
| loss/alpha                         | 0.048    |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 121000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9368
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0388  |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 122000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9463
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0385   |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 123000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9352
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -696     |
| loss/alpha                         | -0.00393 |
| loss/critic1                       | 22.9     |
| loss/critic2                       | 22.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 124000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9385
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 43.3     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -698     |
| loss/alpha                         | -0.0304  |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 22.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 125000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9756
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.024    |
| loss/critic1                       | 25.3     |
| loss/critic2                       | 25.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 126000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9250
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.4     |
| eval/normalized_episode_reward_std | 32.4     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0213   |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 25.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 127000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9574
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -704     |
| loss/alpha                         | -0.016   |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 25       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 128000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9472
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0245  |
| loss/critic1                       | 27.4     |
| loss/critic2                       | 27.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 129000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9573
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0192   |
| loss/critic1                       | 28.5     |
| loss/critic2                       | 29.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9568
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0217  |
| loss/critic1                       | 28.1     |
| loss/critic2                       | 28.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9427
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00261  |
| loss/critic1                       | 25.4     |
| loss/critic2                       | 25.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 132000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9520
----------------------------------------------------------------------------------
| alpha                              | 0.319    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 25.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 133000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9671
----------------------------------------------------------------------------------
| alpha                              | 0.318    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00602 |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 134000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9686
----------------------------------------------------------------------------------
| alpha                              | 0.317    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 25.2     |
| loss/critic2                       | 24.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 135000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9521
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.7     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9744
----------------------------------------------------------------------------------
| alpha                              | 0.321    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.8     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0188  |
| loss/critic1                       | 25.9     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 137000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9866
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0274   |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 26.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 138000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 26       |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 139000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9527
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 26.2     |
| loss/critic2                       | 26       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 140000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9506
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -727     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 25.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 141000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9421
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 8.94     |
| loss/actor                         | -729     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 24.4     |
| loss/critic2                       | 24       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 142000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9673
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.3     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -729     |
| loss/alpha                         | -0.00146 |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9749
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -731     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 144000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -732     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 25.3     |
| loss/critic2                       | 25.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0182  |
| loss/critic1                       | 27.2     |
| loss/critic2                       | 27.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 146000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9518
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.00303  |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 147000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9556
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0239   |
| loss/critic1                       | 25.1     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 148000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 25.2     |
| loss/critic2                       | 25.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9624
----------------------------------------------------------------------------------
| alpha                              | 0.324    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 9.95     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.00242  |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 150000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9612
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -741     |
| loss/alpha                         | -0.00741 |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 26.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 151000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9672
----------------------------------------------------------------------------------
| alpha                              | 0.323    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -743     |
| loss/alpha                         | -0.00485 |
| loss/critic1                       | 26.6     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 152000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 24.8     |
| loss/critic2                       | 24.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 153000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9427
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 23.4     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -747     |
| loss/alpha                         | -0.00949 |
| loss/critic1                       | 24.8     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 154000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00251 |
| loss/critic1                       | 26.2     |
| loss/critic2                       | 26       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 155000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9712
----------------------------------------------------------------------------------
| alpha                              | 0.319    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -749     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 24.8     |
| loss/critic2                       | 24.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 156000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9632
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 4.65     |
| loss/actor                         | -750     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 24.4     |
| loss/critic2                       | 24.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 157000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -751     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 158000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.315    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 159000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9600
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -752     |
| loss/alpha                         | -0.00972 |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 21.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 160000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9852
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 8.5      |
| loss/actor                         | -753     |
| loss/alpha                         | -0.0111  |
| loss/critic1                       | 23       |
| loss/critic2                       | 23.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 161000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9572
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 5.86     |
| loss/actor                         | -754     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 162000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
----------------------------------------------------------------------------------
| alpha                              | 0.315    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -755     |
| loss/alpha                         | 0.033    |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 163000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| alpha                              | 0.317    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55       |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.008   |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 164000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.8     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -758     |
| loss/alpha                         | -0.0219  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 165000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.1     |
| eval/normalized_episode_reward_std | 27.1     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.006   |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 166000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -760     |
| loss/alpha                         | 0.0198   |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 24.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 167000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| alpha                              | 0.315    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -761     |
| loss/alpha                         | 0.00972  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 168000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| alpha                              | 0.316    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 169000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.7     |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 23.2     |
| loss/critic2                       | 22.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 170000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9678
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -763     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 171000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.3     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -764     |
| loss/alpha                         | 0.0289   |
| loss/critic1                       | 22.3     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 172000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9608
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -764     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 21.9     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 173000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9794
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 7.49     |
| loss/actor                         | -765     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 23.3     |
| loss/critic2                       | 22.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 174000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| alpha                              | 0.31     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.6     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -766     |
| loss/alpha                         | 0.00628  |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 23       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 175000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.00147  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 176000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9614
----------------------------------------------------------------------------------
| alpha                              | 0.31     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.2     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -768     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 22       |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 177000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
-----------------------------------------------------------------------------------
| alpha                              | 0.308     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 23.2      |
| loss/actor                         | -769      |
| loss/alpha                         | -0.000499 |
| loss/critic1                       | 22.6      |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 178000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0385   |
| loss/critic1                       | 23       |
| loss/critic2                       | 23       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 179000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0368   |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 180000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.317    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.3     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -773     |
| loss/alpha                         | 0.00661  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 181000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| alpha                              | 0.318    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -774     |
| loss/alpha                         | 0.016    |
| loss/critic1                       | 24.6     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 182000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
----------------------------------------------------------------------------------
| alpha                              | 0.316    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -775     |
| loss/alpha                         | -0.0506  |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9764
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -776     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 23.3     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9480
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -776     |
| loss/alpha                         | -0.0217  |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 23.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 185000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| alpha                              | 0.312    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.4     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -777     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 23       |
| loss/critic2                       | 22.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 186000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -777     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 187000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 10.6     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -778     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 188000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54       |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -778     |
| loss/alpha                         | 0.00771  |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 23.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 189000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.9     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -778     |
| loss/alpha                         | -0.00125 |
| loss/critic1                       | 26       |
| loss/critic2                       | 25.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.7     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -778     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 24.6     |
| loss/critic2                       | 24.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 191000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.311    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -779     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9805
----------------------------------------------------------------------------------
| alpha                              | 0.309    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -779     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 193000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -779     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 194000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| alpha                              | 0.307    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49       |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 195000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0285   |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| alpha                              | 0.309    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.00651 |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 197000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
-----------------------------------------------------------------------------------
| alpha                              | 0.308     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 49.9      |
| eval/normalized_episode_reward_std | 22.6      |
| loss/actor                         | -781      |
| loss/alpha                         | -0.000955 |
| loss/critic1                       | 22.8      |
| loss/critic2                       | 22.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 198000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.307    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0223  |
| loss/critic1                       | 22.4     |
| loss/critic2                       | 22.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 199000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -781     |
| loss/alpha                         | 0.00128  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 200000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 21       |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 201000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0342  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 202000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 11.9     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 203000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 204000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -783     |
| loss/alpha                         | 0.0071   |
| loss/critic1                       | 22.1     |
| loss/critic2                       | 21.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 205000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.7     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -783     |
| loss/alpha                         | 0.0288   |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 206000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -783     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 207000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42.6     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -784     |
| loss/alpha                         | 0.0123   |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 208000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.5     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.00133  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9730
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.4     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 210000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.7     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -786     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 211000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -786     |
| loss/alpha                         | 0.0511   |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 212000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| alpha                              | 0.307    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.6     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -786     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 213000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -787     |
| loss/alpha                         | 0.0127   |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 214000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -788     |
| loss/alpha                         | 0.0279   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 215000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.5     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -789     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 216000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -789     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 217000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.9     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 218000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42       |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 219000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.00854 |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 220000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.0309   |
| loss/critic1                       | 21.8     |
| loss/critic2                       | 21.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 221000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 7.27     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0452  |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 222000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.00332  |
| loss/critic1                       | 21.9     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 223000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 7.17     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.00317  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9525
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 31.7     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.0282  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 226000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.000164 |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 227000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.00379 |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 228000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.0295  |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
-----------------------------------------------------------------------------------
| alpha                              | 0.299     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.1      |
| eval/normalized_episode_reward_std | 19.9      |
| loss/actor                         | -792      |
| loss/alpha                         | -0.000483 |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 19.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 230000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 231000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9879
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -792     |
| loss/alpha                         | 0.0141   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 232000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.9     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -793     |
| loss/alpha                         | -0.0135  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 233000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -793     |
| loss/alpha                         | 0.0252   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.4     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -793     |
| loss/alpha                         | 0.00528  |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 235000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9716
-----------------------------------------------------------------------------------
| alpha                              | 0.3       |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 9         |
| loss/actor                         | -793      |
| loss/alpha                         | -0.000673 |
| loss/critic1                       | 20.5      |
| loss/critic2                       | 20.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 236000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 5.09     |
| loss/actor                         | -794     |
| loss/alpha                         | 0.00484  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 237000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 9.42     |
| loss/actor                         | -794     |
| loss/alpha                         | -0.00671 |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.2     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -794     |
| loss/alpha                         | 0.0123   |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 239000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -795     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 240000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -795     |
| loss/alpha                         | -0.00811 |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 241000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -795     |
| loss/alpha                         | 0.00757  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 242000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0218  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 243000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 9.13     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 244000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0222   |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 245000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00791 |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00113 |
| loss/critic1                       | 21       |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 247000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.96     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.00244  |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 248000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0361   |
| loss/critic1                       | 28.2     |
| loss/critic2                       | 27.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 249000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| alpha                              | 0.305    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.017   |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 250000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00697  |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 251000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 5.63     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0419  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 252000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 253000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 254000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.3     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00806 |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0164   |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 256000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0321  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 257000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0159   |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 259000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.000419 |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 260000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 261000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0188   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 262000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.1     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.0375  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0249   |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 264000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 265000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00987  |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 266000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00553 |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 267000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 9.55     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0173   |
| loss/critic1                       | 21       |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 269000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 270000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| alpha                              | 0.297    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0165  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 271000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 7.71     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00221 |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 272000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0212  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 273000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.00364  |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 274000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 275000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0327   |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 276000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0367  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 278000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 4.41     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.00787 |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 280000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0299   |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 281000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.00633 |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 282000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 283000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.0132   |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 284000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -805     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 285000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 7.03     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.000625 |
| loss/critic1                       | 19       |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 286000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.9     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0111  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 287000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 26.8     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.00587 |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 288000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0408
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0113   |
| loss/critic1                       | 21.8     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 289000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 6.17     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0351   |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 290000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.00356 |
| loss/critic1                       | 21       |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 291000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 292000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 293000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 294000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0275  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 295000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 296000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.00535  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 297000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.00207  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 298000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0258   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.5     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 300000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.7     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0186   |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 301000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
----------------------------------------------------------------------------------
| alpha                              | 0.296    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.000836 |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 302000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 7.07     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0222  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 303000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 5.6      |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 304000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0032   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 305000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 306000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -806     |
| loss/alpha                         | 0.000101 |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 307000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34       |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -806     |
| loss/alpha                         | -0.00775 |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 308000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0586   |
| loss/critic1                       | 21       |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 309000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 9.25     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 21.2     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 310000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.00463 |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 311000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.8     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 312000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9763
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 313000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 314000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.00819  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 315000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 316000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.051   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 317000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -805     |
| loss/alpha                         | -0.00538 |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 318000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.00862 |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0415  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 320000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 321000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.00408  |
| loss/critic1                       | 19       |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 322000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.9     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 323000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.00726  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 324000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 9.66     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0302   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 325000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0353  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 326000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 327000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 4.21     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 6.19     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0312  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 329000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 32       |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0172  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 331000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 8.01     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0456  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 332000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.6     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0504   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 333000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.017    |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 4.02     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 335000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.00889  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 336000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -805     |
| loss/alpha                         | -0.0152  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 337000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.024    |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 338000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 339000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.00794 |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 340000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 341000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 342000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -806     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 343000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.00698  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 344000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.00313 |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 346000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0373   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 347000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 348000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| alpha                              | 0.287    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 349000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0528  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 350000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00209  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 351000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0179   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 352000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.00467 |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 353000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 354000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 4.34     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 355000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 356000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0358  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 357000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0323   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 358000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 359000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.92     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00511  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 360000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 361000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0289   |
| loss/critic1                       | 18       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 362000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.000421 |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 363000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 364000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0388  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 365000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00603  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0029  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 367000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| alpha                              | 0.284     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 16.2      |
| loss/actor                         | -808      |
| loss/alpha                         | -0.000215 |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 368000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45       |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 369000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0288  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 370000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0393   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 371000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.00419 |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 372000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.00586 |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 373000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
-----------------------------------------------------------------------------------
| alpha                              | 0.281     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -808      |
| loss/alpha                         | -0.000924 |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 374000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00577  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 375000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 8.51     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0217  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 376000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 7.93     |
| loss/actor                         | -809     |
| loss/alpha                         | -0.00316 |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 377000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -809     |
| loss/alpha                         | 0.019    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 378000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -809     |
| loss/alpha                         | -0.00958 |
| loss/critic1                       | 24       |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 379000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.7     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 380000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.9     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00179  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.8     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 382000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0375   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 383000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 384000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0337  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 385000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 8.21     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0528  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -808     |
| loss/alpha                         | -0.0111  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 388000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.00577 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 389000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 4.09     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.00144  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 390000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 8.01     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.00259 |
| loss/critic1                       | 16       |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 391000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0336  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 392000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0303  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 393000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0341   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 394000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.97     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 395000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0206   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 396000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0392   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 397000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0462  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 398000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -807     |
| loss/alpha                         | -0.004   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 400000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 402000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 6.37     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.00441  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 403000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.9     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0058  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 404000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.00896  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 4.37     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0296   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 406000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 407000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -807     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 408000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -807     |
| loss/alpha                         | 0.03     |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 410000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -807     |
| loss/alpha                         | 0.0236   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 411000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -808     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 412000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -808     |
| loss/alpha                         | 0.00477  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -809     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 414000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -809     |
| loss/alpha                         | -0.00474 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 415000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 4.75     |
| loss/actor                         | -809     |
| loss/alpha                         | -0.00718 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 416000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50       |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -809     |
| loss/alpha                         | -0.0032  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 417000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -809     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 418000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -810     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 419000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 11.2     |
| loss/actor                         | -810     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 420000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 7.07     |
| loss/actor                         | -811     |
| loss/alpha                         | -0.0264  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -811     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 422000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
-----------------------------------------------------------------------------------
| alpha                              | 0.275     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 22.3      |
| loss/actor                         | -811      |
| loss/alpha                         | -0.000808 |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 423000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0270
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -811     |
| loss/alpha                         | 0.00602  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 424000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -812     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 425000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 7.32     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 426000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00537 |
| loss/critic1                       | 16       |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -813     |
| loss/alpha                         | 0.0173   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 429000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00794 |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 430000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 431000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 432000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.00504  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 433000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.00664 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 434000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 435000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.00769 |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -815     |
| loss/alpha                         | 0.00959  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 437000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.00745  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 438000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0379
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 4.3      |
| loss/actor                         | -815     |
| loss/alpha                         | 0.0261   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.00692  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 440000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 5.13     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.000471 |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 441000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 442000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 443000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 444000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0337
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.4     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.00737 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 445000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
----------------------------------------------------------------------------------
| alpha                              | 0.27     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0175  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 446000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 447000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.27     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 448000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0435
----------------------------------------------------------------------------------
| alpha                              | 0.27     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0417  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 449000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.00345  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 450000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 451000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.0111   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 452000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.00793  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 453000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 454000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| alpha                              | 0.264     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.78      |
| loss/actor                         | -813      |
| loss/alpha                         | -0.000492 |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 455000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 5.94     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.0058   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 456000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00227 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 457000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.0038   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 458000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 16.4     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00534 |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.6     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00859 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 460000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 461000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 462000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 8.05     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.0286   |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 463000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.0127   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 464000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -812     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 465000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.69     |
| loss/actor                         | -811     |
| loss/alpha                         | -0.0604  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -811     |
| loss/alpha                         | 0.000163 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 467000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -810     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 468000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -810     |
| loss/alpha                         | 0.0322   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 469000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -811     |
| loss/alpha                         | 0.0101   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 470000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -811     |
| loss/alpha                         | -0.0303  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 471000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -810     |
| loss/alpha                         | 0.0244   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 472000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -811     |
| loss/alpha                         | 0.00624  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 473000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -811     |
| loss/alpha                         | 0.00918  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 474000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -811     |
| loss/alpha                         | 0.00611  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 475000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0389
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 476000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0428
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -812     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 477000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -812     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 478000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 27.1     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 479000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 8.58     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.00518  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 480000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.00635  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 481000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 4.06     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0386  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 482000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 6.58     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.0707   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 483000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 484000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00361 |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 24       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 485000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 4.96     |
| loss/actor                         | -812     |
| loss/alpha                         | 0.00605  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 486000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 487000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 4.14     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.00218 |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 488000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 4.15     |
| loss/actor                         | -813     |
| loss/alpha                         | -0.0115  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 489000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -813     |
| loss/alpha                         | 0.0268   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 490000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.00427  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 491000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0275  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 493000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -814     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 495000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -814     |
| loss/alpha                         | -0.00779 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 496000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -815     |
| loss/alpha                         | 0.0156   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 497000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -815     |
| loss/alpha                         | 0.0399   |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 498000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -815     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 499000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.00074  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 500000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0421
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 501000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -816     |
| loss/alpha                         | 0.0261   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -816     |
| loss/alpha                         | 0.0216   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 503000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0241  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 504000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.00374 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 4.21     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.00398  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 506000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.00589 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 507000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.0115   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 508000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.00207  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 509000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.1     |
| eval/normalized_episode_reward_std | 5.18     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 510000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0425
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 8.44     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 511000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.00334 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 512000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.7     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.00432 |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 514000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0152   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 515000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 516000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 517000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.9     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 518000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 519000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00551 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 520000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0112   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 521000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 522000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.81     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0155   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 523000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.00474 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.00389  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 525000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0147   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 526000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0367
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.00224 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 527000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 528000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 530000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0084   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 531000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.98     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0175  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 532000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 7.3      |
| loss/actor                         | -822     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 533000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.00855 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 534000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.00987 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0386
-----------------------------------------------------------------------------------
| alpha                              | 0.257     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 11.2      |
| loss/actor                         | -822      |
| loss/alpha                         | -0.000792 |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 536000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0525   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 537000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -822     |
| loss/alpha                         | 0.00224  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 538000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.9     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.00711 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 539000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.00646 |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 540000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 32.2     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 541000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 9.32     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 542000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.00136 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 543000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00113  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 544000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.0398  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 545000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 5.59     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 546000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.75     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00042  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 547000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 548000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
-----------------------------------------------------------------------------------
| alpha                              | 0.257     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.3      |
| eval/normalized_episode_reward_std | 24.9      |
| loss/actor                         | -822      |
| loss/alpha                         | -0.000364 |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 549000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0115  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 550000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0450
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0283   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 551000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0428
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0251   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 552000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 553000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00955  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 554000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 555000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 556000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0437
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 557000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0169   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.9     |
| eval/normalized_episode_reward_std | 6.5      |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0237  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0451
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0115  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 560000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00816 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 561000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0431
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0237  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 562000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0489
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0265   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 563000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| alpha                              | 0.256     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.3      |
| eval/normalized_episode_reward_std | 23.4      |
| loss/actor                         | -820      |
| loss/alpha                         | -0.000729 |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 564000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.8     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 565000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00246  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 566000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00231 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 567000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0427
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00848 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 568000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0400
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.9     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 4.16     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00764  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 570000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0419  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 571000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.4     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0392   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 573000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00587 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 574000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0424
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0125   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00965 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 576000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 577000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0431
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00508  |
| loss/critic1                       | 39.5     |
| loss/critic2                       | 31.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 578000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0215  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 579000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0367
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.011    |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.000248 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 581000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 582000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 583000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 10.3     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 584000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0419
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.2     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 585000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0152   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 586000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 4.27     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 587000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.00475 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 588000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 589000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0324   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 590000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0353  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 591000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0489
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.00622  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 592000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0403
-----------------------------------------------------------------------------------
| alpha                              | 0.255     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 25.8      |
| loss/actor                         | -819      |
| loss/alpha                         | -0.000223 |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 593000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00695  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 594000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00623  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
-----------------------------------------------------------------------------------
| alpha                              | 0.255     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 23.1      |
| loss/actor                         | -818      |
| loss/alpha                         | -0.000792 |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 596000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0438
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00228  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 597000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 598000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0443
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.00648  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 599000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 600000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0430
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 601000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 602000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.00186 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 603000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.0426  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 604000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 605000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.6     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 606000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0474
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 607000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0424
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.00488 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 608000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0370
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0043  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 609000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 610000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 611000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0335
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -815     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.00247  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 613000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0450
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 27.1     |
| loss/actor                         | -815     |
| loss/alpha                         | 0.0334   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 614000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53       |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -816     |
| loss/alpha                         | 0.0322   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 615000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -816     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 616000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 36.3     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 617000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60       |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.00315  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 618000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.7     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.00586 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 619000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0454
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -817     |
| loss/alpha                         | -0.0146  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 620000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0444
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 30.7     |
| loss/actor                         | -817     |
| loss/alpha                         | 0.000936 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 621000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
-----------------------------------------------------------------------------------
| alpha                              | 0.25      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.3      |
| eval/normalized_episode_reward_std | 29.8      |
| loss/actor                         | -818      |
| loss/alpha                         | -0.000494 |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 622000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.5     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0346   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 623000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52       |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0147   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 30.7     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 625000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 626000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 627000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 628000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00201  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 629000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00341  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 630000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.2     |
| eval/normalized_episode_reward_std | 3.83     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 631000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 4.76     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0618   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 632000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.5     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00197  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 633000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.00892 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.0421  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82       |
| eval/normalized_episode_reward_std | 9.98     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00102  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 636000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00176  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.00885  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 638000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0286   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 639000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -818     |
| loss/alpha                         | -0.035   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 640000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 641000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0482
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -818     |
| loss/alpha                         | 0.0059   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 642000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0456
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.00692  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.1     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -819     |
| loss/alpha                         | 0.0286   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 644000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.00224 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 647000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -819     |
| loss/alpha                         | 0.00551  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 648000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 33       |
| loss/actor                         | -819     |
| loss/alpha                         | -0.0351  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0424
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.1     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -819     |
| loss/alpha                         | -0.00137 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 650000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00819  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 651000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0442
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00294  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 652000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.4     |
| eval/normalized_episode_reward_std | 35       |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0138   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 653000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0410
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 654000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0378
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0198   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 655000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 656000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0493
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00722  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 657000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00111 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00603 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 659000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 29       |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 660000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0415
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.022    |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00281 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 662000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 33       |
| loss/actor                         | -820     |
| loss/alpha                         | -0.0431  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 663000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -820     |
| loss/alpha                         | -0.00091 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 664000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 8.96     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00357  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 665000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.00329  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 666000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0045   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 667000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0419
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.1     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -820     |
| loss/alpha                         | 0.0225   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 668000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0094   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 669000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0559  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 670000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00397  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 671000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0405
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00316  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 672000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0345   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 673000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0167  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 674000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.2     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 675000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
-----------------------------------------------------------------------------------
| alpha                              | 0.25      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 26.1      |
| loss/actor                         | -822      |
| loss/alpha                         | -0.000465 |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 676000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0158  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 677000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0520
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00208  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 678000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.000304 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 679000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 32.3     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 680000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0433
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.7     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00798  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 681000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0480
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 8.17     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.00861 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 682000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0448
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0297  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 683000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -821     |
| loss/alpha                         | 0.00986  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 684000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -821     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 685000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 5.22     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 686000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.00828  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 687000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.00998  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 688000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.5     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -822     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 689000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0428
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.8     |
| eval/normalized_episode_reward_std | 32.7     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0442
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0336  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 691000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -822     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 692000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 693000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0395
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0272   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 694000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0379
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 34.3     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.011   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 695000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0154   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 696000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0483
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.7     |
| eval/normalized_episode_reward_std | 33.9     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 697000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0398
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 8.48     |
| loss/actor                         | -824     |
| loss/alpha                         | -0.0233  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 698000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 32.7     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.0518   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 699000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -824     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 700000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0449
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.00165  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 701000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0423
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -825     |
| loss/alpha                         | -0.00224 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 702000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0452
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 703000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0437
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 704000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0443
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -825     |
| loss/alpha                         | -0.00468 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 705000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0413
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.00204 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 706000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.03     |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 707000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 708000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0431
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.000221 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 709000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0379
-----------------------------------------------------------------------------------
| alpha                              | 0.249     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 16.2      |
| loss/actor                         | -826      |
| loss/alpha                         | -0.000196 |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 710000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.1     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 711000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0344
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 712000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0542
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.047   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 713000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0390
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0208   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 714000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.00282 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 715000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0411
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.5     |
| eval/normalized_episode_reward_std | 29       |
| loss/actor                         | -826     |
| loss/alpha                         | 0.00302  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 716000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 4.57     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0445   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 717000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.00224 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 718000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0476
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 28.1     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.025    |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 719000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.00736  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 720000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0253  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 721000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0204  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 722000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 30.3     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0075   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 723000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0474
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 8.73     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0256   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 724000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00274 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 725000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.000147 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 726000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 727000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0364
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 728000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0464
-----------------------------------------------------------------------------------
| alpha                              | 0.246     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.6      |
| eval/normalized_episode_reward_std | 31.9      |
| loss/actor                         | -827      |
| loss/alpha                         | -0.000701 |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.05      |
| timestep                           | 729000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0416
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 730000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0483
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.3     |
| eval/normalized_episode_reward_std | 7.75     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0288   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0465
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.000251 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 734000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.00295  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 735000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.00993  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 736000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0427
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0238  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 737000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.3     |
| eval/normalized_episode_reward_std | 31.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00548 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 738000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0504
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 7.09     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 739000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 33.3     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 740000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.3     |
| eval/normalized_episode_reward_std | 35.5     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.00786  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0504
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.00714  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 742000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0433  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 743000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0172  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 744000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0319  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0370
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 32.2     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.00857  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 746000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0334   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 747000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.00522 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0061  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0515
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0135   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0677
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.6     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0662   |
| loss/critic1                       | 68.6     |
| loss/critic2                       | 34.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.07     |
| timestep                           | 751000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0428
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -824     |
| loss/alpha                         | -0.00635 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 752000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0590
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -824     |
| loss/alpha                         | -0.0695  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 753000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0455
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.0132   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 754000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0535
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -824     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 755000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0523
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.00524  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 756000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00729  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 757000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0398
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00864  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0405
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0355   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 759000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00879  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 760000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0499
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.00226 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 761000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0397
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -823     |
| loss/alpha                         | -0.0246  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 762000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0409
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.00182  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 763000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0445
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.7     |
| eval/normalized_episode_reward_std | 8.6      |
| loss/actor                         | -823     |
| loss/alpha                         | -0.0369  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 764000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0484
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -823     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 765000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -824     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 766000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.000319 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 767000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -824     |
| loss/alpha                         | 0.0195   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 768000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0151  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 769000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.3     |
| eval/normalized_episode_reward_std | 34.2     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 770000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0409
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0158  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 771000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 772000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 6.8      |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0397
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -825     |
| loss/alpha                         | -0.0567  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0335   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 775000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -825     |
| loss/alpha                         | 0.0482   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 776000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -825     |
| loss/alpha                         | -0.00696 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 777000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0484
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0141   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 778000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
-----------------------------------------------------------------------------------
| alpha                              | 0.244     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 28.7      |
| loss/actor                         | -826      |
| loss/alpha                         | -0.000288 |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 779000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
-----------------------------------------------------------------------------------
| alpha                              | 0.243     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 12.9      |
| loss/actor                         | -826      |
| loss/alpha                         | -0.000494 |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 780000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0423
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0376
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.014    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 782000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0450
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 784000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -826     |
| loss/alpha                         | 0.000696 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 785000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0572
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -826     |
| loss/alpha                         | -0.00196 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 786000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0412
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.1     |
| eval/normalized_episode_reward_std | 9.57     |
| loss/actor                         | -826     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 787000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0461
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 788000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 6.57     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0543   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 790000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0430
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 32.9     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 791000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0512
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0132   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 792000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0506
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.3     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 793000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0489
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0068   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 795000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.00623 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 796000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.00186  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 797000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0505
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0164  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0500
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 799000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0234  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 800000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0403
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 8.35     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 801000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0464
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 84.3     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0264   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 802000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0564
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.9     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0371
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0243   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 804000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0501
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 805000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 806000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 807000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0472
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0235   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0410
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00215 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 809000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.01     |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 810000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0289   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0435
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 30.3     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0373  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 812000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0479
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.00938 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 813000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0385
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.000124 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 814000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0463
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 815000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0456
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0318   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 816000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0402
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.000141 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 817000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0487
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0106  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 818000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0505
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0571   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 820000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0251  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 821000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 822000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.00144  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0432
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0035  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0385
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0284   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0524
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0203  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 827000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0449
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.5     |
| eval/normalized_episode_reward_std | 32.8     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.00835  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0376
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.0095   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 830000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -829     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 831000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 34.8     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.000851 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 832000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.5     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 833000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 834000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0512
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0148  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 835000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 25.3     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.00457  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 836000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0430
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.6     |
| eval/normalized_episode_reward_std | 32.2     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0539
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 29       |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 838000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0465
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.2     |
| eval/normalized_episode_reward_std | 6.96     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 839000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0551
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.00373  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 841000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 842000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0461
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 84.5     |
| eval/normalized_episode_reward_std | 5.96     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0215  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 843000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0497
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 31.5     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.0224   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 844000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -829     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 845000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0443
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0142  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 846000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0563
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0146   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 847000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.5     |
| eval/normalized_episode_reward_std | 32.1     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 848000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0429
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58       |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0237  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 849000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0488
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.00752 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 850000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0478
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 35.2     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0211   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 852000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0452
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -828     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 853000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -828     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 854000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0453
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.1     |
| eval/normalized_episode_reward_std | 28.1     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 855000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0496
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 856000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0429
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.045    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 857000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 858000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0385
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.00558  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 859000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0451
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 860000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0499
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0596   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 861000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0455
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.00805  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 862000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0542
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0068  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 863000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0540
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00394 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 864000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0496
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00217 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 865000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0445
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0061  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 866000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0453
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.00944  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 867000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0510
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 84.3     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 868000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0417  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 869000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00257 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0476
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 871000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0433
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -827     |
| loss/alpha                         | -0.00557 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 872000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 873000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0499
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -827     |
| loss/alpha                         | 0.0195   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 874000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0400
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.8     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -828     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 875000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -828     |
| loss/alpha                         | 0.00738  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 876000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0504
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.047    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 877000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -829     |
| loss/alpha                         | 0.00794  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 878000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0393
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.00985 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0413
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.000732 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 880000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0483
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0373  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 881000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0495
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0403   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 882000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0529
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 883000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0431
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00243  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 884000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0386
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.5     |
| eval/normalized_episode_reward_std | 30.3     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0284   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 885000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 886000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0464
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 34.1     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 888000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0485
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 12.5     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 889000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.00937 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 890000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0502
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 29.9     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 891000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0585
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.00712  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 892000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0439
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0518
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 84.1     |
| eval/normalized_episode_reward_std | 4.31     |
| loss/actor                         | -833     |
| loss/alpha                         | 0.00114  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 894000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0507
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -833     |
| loss/alpha                         | -0.00379 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 895000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0444
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -833     |
| loss/alpha                         | -0.0191  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 896000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0536
-----------------------------------------------------------------------------------
| alpha                              | 0.235     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 25.3      |
| loss/actor                         | -833      |
| loss/alpha                         | -0.000936 |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.05      |
| timestep                           | 897000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0524
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -833     |
| loss/alpha                         | -0.00508 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 898000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0448
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -833     |
| loss/alpha                         | 0.0185   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 899000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0557
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -833     |
| loss/alpha                         | -0.0263  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 900000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0513
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0115   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 901000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0511
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 84.9     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -832     |
| loss/alpha                         | -0.00254 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 902000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0421
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0416
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 8.47     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0488
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 9.83     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.00345  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 905000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0458
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 906000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0493
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 25.3     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0026  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 907000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0438
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 908000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.058    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 909000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0580
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 85.4     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -832     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 911000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 912000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0395
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0601  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 913000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0492
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 31.8     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 914000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.0302  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0466
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0406   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 916000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0484
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -832     |
| loss/alpha                         | 0.0418   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0550
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -832     |
| loss/alpha                         | -0.036   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0578
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 34       |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 919000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0465
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.00952 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 920000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0498
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.2     |
| eval/normalized_episode_reward_std | 31.1     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0506  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 921000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0598
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 922000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0559
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 923000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0401
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 33.4     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0186   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 33.4     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0202  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 925000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0558
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 32.5     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 926000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0410
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.2     |
| eval/normalized_episode_reward_std | 36.3     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.00792 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 927000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0606
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -830     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 928000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0358   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 929000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0517
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0396  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 930000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0537
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0618   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 931000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0494
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00472  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 932000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0558
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0468  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 933000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0513
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0482
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0131   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0476
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 936000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0473
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 937000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0511
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0224   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 938000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0317  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 939000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0309   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 940000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0424
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0312   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 941000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0456
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.3     |
| eval/normalized_episode_reward_std | 7.08     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00171  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 942000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0497
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 943000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0167  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 944000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0435
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0254  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0405
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 31.6     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0018  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 946000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0486
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0135   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 947000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0253  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 948000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0481
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.2     |
| eval/normalized_episode_reward_std | 32.8     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0045   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 949000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0530
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 27.5     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0304  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 950000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0468
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.1     |
| eval/normalized_episode_reward_std | 30.2     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00602  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 951000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0409
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0246   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 952000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0501
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00406  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 953000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.005    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 954000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 955000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0568
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.4     |
| eval/normalized_episode_reward_std | 4.58     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0173   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 956000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0535
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00218  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 957000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0544
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0501
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.7     |
| eval/normalized_episode_reward_std | 6.05     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 959000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.4     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.011    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 960000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0429
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0352   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0572
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00612  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 962000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.6     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.00303 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 963000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0431
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0164  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 964000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0537
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00275  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 965000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.0351  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 966000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0512
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 31.6     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.00388  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 967000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0474
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0501
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0082   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 969000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0486
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.9     |
| eval/normalized_episode_reward_std | 4.14     |
| loss/actor                         | -831     |
| loss/alpha                         | 0.0127   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 970000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0426
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -831     |
| loss/alpha                         | -0.00232 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 971000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0462
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0313  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 972000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 973000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0534
-----------------------------------------------------------------------------------
| alpha                              | 0.228     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 30.4      |
| loss/actor                         | -830      |
| loss/alpha                         | -0.000825 |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.05      |
| timestep                           | 974000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0481
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0151  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 975000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0460
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00316  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0515
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 977000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0465
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0123   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 978000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0497
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.7     |
| eval/normalized_episode_reward_std | 33       |
| loss/actor                         | -830     |
| loss/alpha                         | 0.038    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0517
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 32.1     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 980000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0475
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.00921 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0490
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 982000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0567
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0252  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.06     |
| timestep                           | 983000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0454
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 30.9     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 984000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0521
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 10.9     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 985000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0469
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -830     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 986000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.2     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 987000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0482
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00346  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 988000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0479
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0135   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 989000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0438
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00488  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 990000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0503
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 991000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0499
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0339   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0462
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0241   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 993000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0463
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00226  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 995000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00549  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 996000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0454
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00521  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 997000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0530
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0155   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 998000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0515
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 999000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0543
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 30.9     |
| loss/actor                         | -830     |
| loss/alpha                         | 0.00193  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.05     |
| timestep                           | 1000000  |
----------------------------------------------------------------------------------
total time: 58458.75s
