Training dynamics:
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.63821614 |
| loss/dynamics_train_loss   | -8.82      |
| timestep                   | 1          |
----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.528565 |
| loss/dynamics_train_loss   | -27.2    |
| timestep                   | 2        |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.52965564 |
| loss/dynamics_train_loss   | -31.2      |
| timestep                   | 3          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.5177375 |
| loss/dynamics_train_loss   | -33.2     |
| timestep                   | 4         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.51619357 |
| loss/dynamics_train_loss   | -34.5      |
| timestep                   | 5          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.49374136 |
| loss/dynamics_train_loss   | -35.4      |
| timestep                   | 6          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4810864 |
| loss/dynamics_train_loss   | -36.1     |
| timestep                   | 7         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.47009087 |
| loss/dynamics_train_loss   | -36.7      |
| timestep                   | 8          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.45232287 |
| loss/dynamics_train_loss   | -37.2      |
| timestep                   | 9          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.44027147 |
| loss/dynamics_train_loss   | -37.7      |
| timestep                   | 10         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.4366486 |
| loss/dynamics_train_loss   | -38       |
| timestep                   | 11        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.41829032 |
| loss/dynamics_train_loss   | -38.4      |
| timestep                   | 12         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.40848392 |
| loss/dynamics_train_loss   | -38.7      |
| timestep                   | 13         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.39958963 |
| loss/dynamics_train_loss   | -39        |
| timestep                   | 14         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.38276392 |
| loss/dynamics_train_loss   | -39.3      |
| timestep                   | 15         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.37848347 |
| loss/dynamics_train_loss   | -39.5      |
| timestep                   | 16         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.36529773 |
| loss/dynamics_train_loss   | -39.7      |
| timestep                   | 17         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.36062747 |
| loss/dynamics_train_loss   | -39.9      |
| timestep                   | 18         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3613099 |
| loss/dynamics_train_loss   | -40.1     |
| timestep                   | 19        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.35264814 |
| loss/dynamics_train_loss   | -40.3      |
| timestep                   | 20         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.33758253 |
| loss/dynamics_train_loss   | -40.5      |
| timestep                   | 21         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.34402084 |
| loss/dynamics_train_loss   | -40.7      |
| timestep                   | 22         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.32638416 |
| loss/dynamics_train_loss   | -40.8      |
| timestep                   | 23         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.32235023 |
| loss/dynamics_train_loss   | -41        |
| timestep                   | 24         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.31424722 |
| loss/dynamics_train_loss   | -41.1      |
| timestep                   | 25         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.3040911 |
| loss/dynamics_train_loss   | -41.2     |
| timestep                   | 26        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.30183747 |
| loss/dynamics_train_loss   | -41.3      |
| timestep                   | 27         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.29255417 |
| loss/dynamics_train_loss   | -41.5      |
| timestep                   | 28         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.28799564 |
| loss/dynamics_train_loss   | -41.6      |
| timestep                   | 29         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.27635854 |
| loss/dynamics_train_loss   | -41.6      |
| timestep                   | 30         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.28367752 |
| loss/dynamics_train_loss   | -41.7      |
| timestep                   | 31         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.26480603 |
| loss/dynamics_train_loss   | -41.9      |
| timestep                   | 32         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.26345563 |
| loss/dynamics_train_loss   | -41.9      |
| timestep                   | 33         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.25840813 |
| loss/dynamics_train_loss   | -42        |
| timestep                   | 34         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24825191 |
| loss/dynamics_train_loss   | -42.1      |
| timestep                   | 35         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24463305 |
| loss/dynamics_train_loss   | -42.1      |
| timestep                   | 36         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.24586578 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 37         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.23218763 |
| loss/dynamics_train_loss   | -42.4      |
| timestep                   | 38         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.23711295 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 39         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.23128684 |
| loss/dynamics_train_loss   | -42.3      |
| timestep                   | 40         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21759208 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 41         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.21285501 |
| loss/dynamics_train_loss   | -42.6      |
| timestep                   | 42         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20952407 |
| loss/dynamics_train_loss   | -42.7      |
| timestep                   | 43         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.20103231 |
| loss/dynamics_train_loss   | -42.7      |
| timestep                   | 44         |
----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.200966 |
| loss/dynamics_train_loss   | -42.8    |
| timestep                   | 45       |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19945724 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 46         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19768365 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 47         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18799186 |
| loss/dynamics_train_loss   | -42.9      |
| timestep                   | 48         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1859465 |
| loss/dynamics_train_loss   | -42.9     |
| timestep                   | 49        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.19109242 |
| loss/dynamics_train_loss   | -43        |
| timestep                   | 50         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.18263797 |
| loss/dynamics_train_loss   | -43.2      |
| timestep                   | 51         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17878172 |
| loss/dynamics_train_loss   | -43.2      |
| timestep                   | 52         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17183231 |
| loss/dynamics_train_loss   | -43.3      |
| timestep                   | 53         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1698046 |
| loss/dynamics_train_loss   | -43.3     |
| timestep                   | 54        |
---------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1672157 |
| loss/dynamics_train_loss   | -43.4     |
| timestep                   | 55        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.17066368 |
| loss/dynamics_train_loss   | -43.5      |
| timestep                   | 56         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16221416 |
| loss/dynamics_train_loss   | -43.4      |
| timestep                   | 57         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16271904 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 58         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15640904 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 59         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15273091 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 60         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15792343 |
| loss/dynamics_train_loss   | -43.7      |
| timestep                   | 61         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.16430002 |
| loss/dynamics_train_loss   | -43.7      |
| timestep                   | 62         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1538957 |
| loss/dynamics_train_loss   | -43.8     |
| timestep                   | 63        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15003133 |
| loss/dynamics_train_loss   | -43.8      |
| timestep                   | 64         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15033853 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 65         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15101777 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 66         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14952734 |
| loss/dynamics_train_loss   | -43.9      |
| timestep                   | 67         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15056875 |
| loss/dynamics_train_loss   | -44        |
| timestep                   | 68         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14780825 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 69         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14706726 |
| loss/dynamics_train_loss   | -44.1      |
| timestep                   | 70         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14445345 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 71         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14911762 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 72         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15213622 |
| loss/dynamics_train_loss   | -44.2      |
| timestep                   | 73         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14890437 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 74         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13995583 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 75         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14844498 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 76         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.15027632 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 77         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1431723 |
| loss/dynamics_train_loss   | -44.4     |
| timestep                   | 78        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14649525 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 79         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14120686 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 80         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13973838 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 81         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1352396 |
| loss/dynamics_train_loss   | -44.6     |
| timestep                   | 82        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13872914 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 83         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14313236 |
| loss/dynamics_train_loss   | -44.6      |
| timestep                   | 84         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1373134 |
| loss/dynamics_train_loss   | -44.7     |
| timestep                   | 85        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13674684 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 86         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13711724 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 87         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13295026 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 88         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13791768 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 89         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1338083 |
| loss/dynamics_train_loss   | -44.9     |
| timestep                   | 90        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13365373 |
| loss/dynamics_train_loss   | -44.8      |
| timestep                   | 91         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13364947 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 92         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13226709 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 93         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13275453 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 94         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13283823 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 95         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13526182 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 96         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13619295 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 97         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12938328 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 98         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13218349 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 99         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12990485 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 100        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.14064494 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 101        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13100596 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 102        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12905622 |
| loss/dynamics_train_loss   | -45.2      |
| timestep                   | 103        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12619248 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 104        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.124798976 |
| loss/dynamics_train_loss   | -45.2       |
| timestep                   | 105         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12537983 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 106        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13123204 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 107        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12730785 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 108        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12426398 |
| loss/dynamics_train_loss   | -45.3      |
| timestep                   | 109        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1311336 |
| loss/dynamics_train_loss   | -45.4     |
| timestep                   | 110       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13721634 |
| loss/dynamics_train_loss   | -45.4      |
| timestep                   | 111        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1214851 |
| loss/dynamics_train_loss   | -45.4     |
| timestep                   | 112       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12626025 |
| loss/dynamics_train_loss   | -45.5      |
| timestep                   | 113        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12021153 |
| loss/dynamics_train_loss   | -45.5      |
| timestep                   | 114        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.119822025 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 115         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12364298 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 116        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.123914815 |
| loss/dynamics_train_loss   | -45.5       |
| timestep                   | 117         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12939969 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 118        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.123503886 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 119         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12594922 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 120        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1191922 |
| loss/dynamics_train_loss   | -45.7     |
| timestep                   | 121       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12432785 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 122        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11410682 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 123        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11562474 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 124        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12116704 |
| loss/dynamics_train_loss   | -45.8      |
| timestep                   | 125        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.121725634 |
| loss/dynamics_train_loss   | -45.8       |
| timestep                   | 126         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.13030794 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 127        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11754887 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 128        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12426554 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 129        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11969616 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 130        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11745529 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 131        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12105508 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 132        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11901693 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 133        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12112878 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 134        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12130022 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 135        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11777947 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 136        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11746547 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 137        |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.1154262 |
| loss/dynamics_train_loss   | -46.1     |
| timestep                   | 138       |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11143935 |
| loss/dynamics_train_loss   | -46.1      |
| timestep                   | 139        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.116874374 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 140         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.115173005 |
| loss/dynamics_train_loss   | -46.2       |
| timestep                   | 141         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.116029836 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 142         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.12023331 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 143        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11376488 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 144        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11471684 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 145        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.11500114 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 146        |
----------------------------------------------------------------------------
elites:[5, 6, 0, 1, 2] , holdout loss: 0.10661239922046661
num rollout transitions: 250000, reward mean: 4.8595
----------------------------------------------------------------------------------
| alpha                              | 0.952    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.25     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -16.2    |
| loss/alpha                         | -0.502   |
| loss/critic1                       | 3.23     |
| loss/critic2                       | 3.24     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9016
----------------------------------------------------------------------------------
| alpha                              | 0.861    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.25     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -32.4    |
| loss/alpha                         | -1.5     |
| loss/critic1                       | 3.54     |
| loss/critic2                       | 3.51     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9125
----------------------------------------------------------------------------------
| alpha                              | 0.78     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.24     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -46.7    |
| loss/alpha                         | -2.41    |
| loss/critic1                       | 4.96     |
| loss/critic2                       | 4.97     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9303
----------------------------------------------------------------------------------
| alpha                              | 0.709    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.25     |
| eval/normalized_episode_reward_std | 2.35     |
| loss/actor                         | -59.7    |
| loss/alpha                         | -3.07    |
| loss/critic1                       | 6.89     |
| loss/critic2                       | 6.96     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 4000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8874
----------------------------------------------------------------------------------
| alpha                              | 0.647    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.456    |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -73.4    |
| loss/alpha                         | -3.29    |
| loss/critic1                       | 9.14     |
| loss/critic2                       | 9.21     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 5000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8852
----------------------------------------------------------------------------------
| alpha                              | 0.595    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.0868  |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -89.6    |
| loss/alpha                         | -3.12    |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 6000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8333
----------------------------------------------------------------------------------
| alpha                              | 0.55     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.41    |
| eval/normalized_episode_reward_std | 3.94     |
| loss/actor                         | -108     |
| loss/alpha                         | -2.87    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8074
----------------------------------------------------------------------------------
| alpha                              | 0.509    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.14    |
| eval/normalized_episode_reward_std | 4.53     |
| loss/actor                         | -126     |
| loss/alpha                         | -2.66    |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 8000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7659
----------------------------------------------------------------------------------
| alpha                              | 0.471    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -1.8     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -144     |
| loss/alpha                         | -2.35    |
| loss/critic1                       | 23       |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 9000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7416
----------------------------------------------------------------------------------
| alpha                              | 0.438    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -2.3     |
| eval/normalized_episode_reward_std | 3.95     |
| loss/actor                         | -160     |
| loss/alpha                         | -1.95    |
| loss/critic1                       | 26.2     |
| loss/critic2                       | 26.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.74     |
| timestep                           | 10000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7333
----------------------------------------------------------------------------------
| alpha                              | 0.408    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -0.814   |
| eval/normalized_episode_reward_std | 3.97     |
| loss/actor                         | -174     |
| loss/alpha                         | -1.5     |
| loss/critic1                       | 27.8     |
| loss/critic2                       | 28.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.73     |
| timestep                           | 11000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7476
----------------------------------------------------------------------------------
| alpha                              | 0.384    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | -2.82    |
| eval/normalized_episode_reward_std | 5.44     |
| loss/actor                         | -187     |
| loss/alpha                         | -1.02    |
| loss/critic1                       | 29.8     |
| loss/critic2                       | 30.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 12000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7714
----------------------------------------------------------------------------------
| alpha                              | 0.366    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.71     |
| eval/normalized_episode_reward_std | 3.97     |
| loss/actor                         | -199     |
| loss/alpha                         | -0.464   |
| loss/critic1                       | 32.5     |
| loss/critic2                       | 32.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 13000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7497
----------------------------------------------------------------------------------
| alpha                              | 0.358    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.663    |
| eval/normalized_episode_reward_std | 4.59     |
| loss/actor                         | -211     |
| loss/alpha                         | -0.0915  |
| loss/critic1                       | 34.8     |
| loss/critic2                       | 35.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 14000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7243
----------------------------------------------------------------------------------
| alpha                              | 0.359    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.962    |
| eval/normalized_episode_reward_std | 5.32     |
| loss/actor                         | -222     |
| loss/alpha                         | 0.0838   |
| loss/critic1                       | 35       |
| loss/critic2                       | 35.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.72     |
| timestep                           | 15000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7640
----------------------------------------------------------------------------------
| alpha                              | 0.37     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.17     |
| eval/normalized_episode_reward_std | 6.33     |
| loss/actor                         | -232     |
| loss/alpha                         | 0.177    |
| loss/critic1                       | 37.3     |
| loss/critic2                       | 37.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 16000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7498
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 9.52     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -242     |
| loss/alpha                         | 0.0923   |
| loss/critic1                       | 39.8     |
| loss/critic2                       | 39.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 17000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7618
----------------------------------------------------------------------------------
| alpha                              | 0.396    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 5.52     |
| eval/normalized_episode_reward_std | 6.32     |
| loss/actor                         | -250     |
| loss/alpha                         | 0.113    |
| loss/critic1                       | 41.7     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 18000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7791
----------------------------------------------------------------------------------
| alpha                              | 0.408    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.53     |
| eval/normalized_episode_reward_std | 7.22     |
| loss/actor                         | -258     |
| loss/alpha                         | 0.062    |
| loss/critic1                       | 44.9     |
| loss/critic2                       | 44.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 19000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7748
----------------------------------------------------------------------------------
| alpha                              | 0.416    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 8.14     |
| eval/normalized_episode_reward_std | 9.17     |
| loss/actor                         | -265     |
| loss/alpha                         | 0.0492   |
| loss/critic1                       | 49.4     |
| loss/critic2                       | 48.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 20000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7740
----------------------------------------------------------------------------------
| alpha                              | 0.423    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 28.2     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -271     |
| loss/alpha                         | 0.0514   |
| loss/critic1                       | 52.9     |
| loss/critic2                       | 52.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.77     |
| timestep                           | 21000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7614
----------------------------------------------------------------------------------
| alpha                              | 0.43     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 33.8     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -278     |
| loss/alpha                         | 0.0416   |
| loss/critic1                       | 54.9     |
| loss/critic2                       | 55       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 22000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7503
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.3     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -284     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 53.7     |
| loss/critic2                       | 54.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.75     |
| timestep                           | 23000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7555
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 33.6     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -290     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 49.9     |
| loss/critic2                       | 50.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.76     |
| timestep                           | 24000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7796
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.9     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -296     |
| loss/alpha                         | 0.0268   |
| loss/critic1                       | 47.8     |
| loss/critic2                       | 48.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 25000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8101
----------------------------------------------------------------------------------
| alpha                              | 0.441    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.9     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -302     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 48.7     |
| loss/critic2                       | 49.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 26000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8059
----------------------------------------------------------------------------------
| alpha                              | 0.445    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.3     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -308     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 48.2     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7901
----------------------------------------------------------------------------------
| alpha                              | 0.443    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 31.8     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -314     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 46.2     |
| loss/critic2                       | 46.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 28000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7798
----------------------------------------------------------------------------------
| alpha                              | 0.443    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.8     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -320     |
| loss/alpha                         | -0.00361 |
| loss/critic1                       | 46.1     |
| loss/critic2                       | 46.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 29000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8204
----------------------------------------------------------------------------------
| alpha                              | 0.438    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27.6     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -326     |
| loss/alpha                         | -0.024   |
| loss/critic1                       | 44.7     |
| loss/critic2                       | 45.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 30000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8191
----------------------------------------------------------------------------------
| alpha                              | 0.437    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.5     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -332     |
| loss/alpha                         | -0.00513 |
| loss/critic1                       | 42.3     |
| loss/critic2                       | 42.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 31000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7882
----------------------------------------------------------------------------------
| alpha                              | 0.438    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45       |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -338     |
| loss/alpha                         | 0.00676  |
| loss/critic1                       | 40.6     |
| loss/critic2                       | 41.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 32000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8135
----------------------------------------------------------------------------------
| alpha                              | 0.437    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.6     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -345     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 40.7     |
| loss/critic2                       | 41.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 33000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8097
----------------------------------------------------------------------------------
| alpha                              | 0.433    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.8     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -351     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 39.4     |
| loss/critic2                       | 39.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 34000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8210
----------------------------------------------------------------------------------
| alpha                              | 0.43     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42.2     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -357     |
| loss/alpha                         | 0.00595  |
| loss/critic1                       | 38.8     |
| loss/critic2                       | 39.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 35000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7970
----------------------------------------------------------------------------------
| alpha                              | 0.432    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 11.1     |
| eval/normalized_episode_reward_std | 10.9     |
| loss/actor                         | -364     |
| loss/alpha                         | -0.00255 |
| loss/critic1                       | 38       |
| loss/critic2                       | 38.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.8      |
| timestep                           | 36000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8750
----------------------------------------------------------------------------------
| alpha                              | 0.433    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.9     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -370     |
| loss/alpha                         | 0.0229   |
| loss/critic1                       | 50.3     |
| loss/critic2                       | 51.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 37000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8189
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.2     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -376     |
| loss/alpha                         | 0.000361 |
| loss/critic1                       | 49.7     |
| loss/critic2                       | 50.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 38000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7810
----------------------------------------------------------------------------------
| alpha                              | 0.436    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.1     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -381     |
| loss/alpha                         | -0.00836 |
| loss/critic1                       | 47.5     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.78     |
| timestep                           | 39000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7901
----------------------------------------------------------------------------------
| alpha                              | 0.431    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.8     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -387     |
| loss/alpha                         | -0.0278  |
| loss/critic1                       | 45       |
| loss/critic2                       | 45.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 40000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8246
----------------------------------------------------------------------------------
| alpha                              | 0.425    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -392     |
| loss/alpha                         | -0.0355  |
| loss/critic1                       | 44.8     |
| loss/critic2                       | 45.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 41000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7929
----------------------------------------------------------------------------------
| alpha                              | 0.421    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -398     |
| loss/alpha                         | -0.0255  |
| loss/critic1                       | 32.9     |
| loss/critic2                       | 33.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.79     |
| timestep                           | 42000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8318
----------------------------------------------------------------------------------
| alpha                              | 0.416    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.5     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -404     |
| loss/alpha                         | -0.0275  |
| loss/critic1                       | 32.3     |
| loss/critic2                       | 32.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 43000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8346
----------------------------------------------------------------------------------
| alpha                              | 0.414    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27.9     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -409     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 31.7     |
| loss/critic2                       | 32.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 44000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8719
----------------------------------------------------------------------------------
| alpha                              | 0.412    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.3     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -415     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 35.9     |
| loss/critic2                       | 36.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 45000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8757
----------------------------------------------------------------------------------
| alpha                              | 0.411    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.6     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -420     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 36.7     |
| loss/critic2                       | 37.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 46000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8479
----------------------------------------------------------------------------------
| alpha                              | 0.408    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.6     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -426     |
| loss/alpha                         | 0.00334  |
| loss/critic1                       | 38.7     |
| loss/critic2                       | 39.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 47000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8571
----------------------------------------------------------------------------------
| alpha                              | 0.41     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52       |
| eval/normalized_episode_reward_std | 4.6      |
| loss/actor                         | -431     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 37       |
| loss/critic2                       | 37.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 48000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8091
----------------------------------------------------------------------------------
| alpha                              | 0.409    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.2     |
| eval/normalized_episode_reward_std | 8.93     |
| loss/actor                         | -437     |
| loss/alpha                         | -0.0193  |
| loss/critic1                       | 35       |
| loss/critic2                       | 35.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 49000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8523
----------------------------------------------------------------------------------
| alpha                              | 0.404    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.4     |
| eval/normalized_episode_reward_std | 9.14     |
| loss/actor                         | -442     |
| loss/alpha                         | -0.0473  |
| loss/critic1                       | 33.3     |
| loss/critic2                       | 33.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 50000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8189
----------------------------------------------------------------------------------
| alpha                              | 0.398    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.2     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -448     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 29.1     |
| loss/critic2                       | 29.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 51000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8571
----------------------------------------------------------------------------------
| alpha                              | 0.393    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.9     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -453     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 29.2     |
| loss/critic2                       | 29.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 52000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8586
----------------------------------------------------------------------------------
| alpha                              | 0.386    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.2     |
| eval/normalized_episode_reward_std | 10.5     |
| loss/actor                         | -459     |
| loss/alpha                         | -0.032   |
| loss/critic1                       | 29.5     |
| loss/critic2                       | 29.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8767
----------------------------------------------------------------------------------
| alpha                              | 0.384    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.8     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -465     |
| loss/alpha                         | -0.00807 |
| loss/critic1                       | 31.4     |
| loss/critic2                       | 31.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8325
----------------------------------------------------------------------------------
| alpha                              | 0.385    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.9     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -470     |
| loss/alpha                         | 0.00931  |
| loss/critic1                       | 30.4     |
| loss/critic2                       | 30.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 55000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8890
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.9     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -475     |
| loss/alpha                         | -0.00897 |
| loss/critic1                       | 31.2     |
| loss/critic2                       | 32.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 56000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8295
----------------------------------------------------------------------------------
| alpha                              | 0.384    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 37.6     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -481     |
| loss/alpha                         | -0.00469 |
| loss/critic1                       | 31.8     |
| loss/critic2                       | 32.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.83     |
| timestep                           | 57000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8827
----------------------------------------------------------------------------------
| alpha                              | 0.383    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49       |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -486     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 33.3     |
| loss/critic2                       | 34.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 58000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8710
----------------------------------------------------------------------------------
| alpha                              | 0.378    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.6     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -491     |
| loss/alpha                         | -0.0411  |
| loss/critic1                       | 32.2     |
| loss/critic2                       | 32.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 59000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8862
----------------------------------------------------------------------------------
| alpha                              | 0.374    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 7.31     |
| loss/actor                         | -495     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 32.6     |
| loss/critic2                       | 33.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 60000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8808
----------------------------------------------------------------------------------
| alpha                              | 0.373    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.9     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -500     |
| loss/alpha                         | -0.0282  |
| loss/critic1                       | 31.4     |
| loss/critic2                       | 32       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 61000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8733
----------------------------------------------------------------------------------
| alpha                              | 0.371    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57       |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -505     |
| loss/alpha                         | 0.00236  |
| loss/critic1                       | 28.6     |
| loss/critic2                       | 29.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 62000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8634
-----------------------------------------------------------------------------------
| alpha                              | 0.369     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 54.9      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -509      |
| loss/alpha                         | -0.000912 |
| loss/critic1                       | 28.3      |
| loss/critic2                       | 28.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.86      |
| timestep                           | 63000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8155
----------------------------------------------------------------------------------
| alpha                              | 0.368    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -514     |
| loss/alpha                         | -0.0379  |
| loss/critic1                       | 27.7     |
| loss/critic2                       | 28.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 64000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8706
----------------------------------------------------------------------------------
| alpha                              | 0.364    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 5.26     |
| loss/actor                         | -519     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 26.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 65000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8704
----------------------------------------------------------------------------------
| alpha                              | 0.361    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.3     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -523     |
| loss/alpha                         | 0.00131  |
| loss/critic1                       | 24.8     |
| loss/critic2                       | 25.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 66000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8841
----------------------------------------------------------------------------------
| alpha                              | 0.36     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -528     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 26.2     |
| loss/critic2                       | 26.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 67000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8397
----------------------------------------------------------------------------------
| alpha                              | 0.358    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.1     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -532     |
| loss/alpha                         | -0.0335  |
| loss/critic1                       | 25.3     |
| loss/critic2                       | 25.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8947
----------------------------------------------------------------------------------
| alpha                              | 0.356    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50       |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -537     |
| loss/alpha                         | -0.011   |
| loss/critic1                       | 27.5     |
| loss/critic2                       | 28       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 69000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9044
----------------------------------------------------------------------------------
| alpha                              | 0.354    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -541     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 27.7     |
| loss/critic2                       | 28.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 70000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8852
----------------------------------------------------------------------------------
| alpha                              | 0.352    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.1     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -545     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 27.3     |
| loss/critic2                       | 27.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8837
----------------------------------------------------------------------------------
| alpha                              | 0.349    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -548     |
| loss/alpha                         | -0.0194  |
| loss/critic1                       | 26.7     |
| loss/critic2                       | 27.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 72000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8570
----------------------------------------------------------------------------------
| alpha                              | 0.349    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -552     |
| loss/alpha                         | 0.00333  |
| loss/critic1                       | 27.1     |
| loss/critic2                       | 27.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 73000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8479
----------------------------------------------------------------------------------
| alpha                              | 0.349    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -556     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 25.2     |
| loss/critic2                       | 25.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.85     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9048
----------------------------------------------------------------------------------
| alpha                              | 0.345    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.4     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -559     |
| loss/alpha                         | -0.0535  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 75000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8862
----------------------------------------------------------------------------------
| alpha                              | 0.339    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.4     |
| eval/normalized_episode_reward_std | 9.43     |
| loss/actor                         | -563     |
| loss/alpha                         | -0.0412  |
| loss/critic1                       | 23.2     |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 76000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8921
----------------------------------------------------------------------------------
| alpha                              | 0.337    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -566     |
| loss/alpha                         | 0.00851  |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 24.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 77000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8681
----------------------------------------------------------------------------------
| alpha                              | 0.336    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -570     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 23.8     |
| loss/critic2                       | 24.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 78000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9083
----------------------------------------------------------------------------------
| alpha                              | 0.333    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.8     |
| eval/normalized_episode_reward_std | 7.01     |
| loss/actor                         | -573     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 24       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 79000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8966
----------------------------------------------------------------------------------
| alpha                              | 0.332    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -576     |
| loss/alpha                         | -0.0245  |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 24       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 80000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8845
----------------------------------------------------------------------------------
| alpha                              | 0.331    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -579     |
| loss/alpha                         | 0.000799 |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 24.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 81000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8890
----------------------------------------------------------------------------------
| alpha                              | 0.329    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -582     |
| loss/alpha                         | -0.00962 |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 22.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 82000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9088
----------------------------------------------------------------------------------
| alpha                              | 0.331    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 4.46     |
| loss/actor                         | -585     |
| loss/alpha                         | 0.00549  |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 83000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9158
----------------------------------------------------------------------------------
| alpha                              | 0.33     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -588     |
| loss/alpha                         | 0.0046   |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 24.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 84000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8905
-----------------------------------------------------------------------------------
| alpha                              | 0.331     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -591      |
| loss/alpha                         | -0.000932 |
| loss/critic1                       | 21.9      |
| loss/critic2                       | 22.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 85000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8893
----------------------------------------------------------------------------------
| alpha                              | 0.328    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -594     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 23.2     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 86000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8729
----------------------------------------------------------------------------------
| alpha                              | 0.325    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -596     |
| loss/alpha                         | -0.0423  |
| loss/critic1                       | 23.5     |
| loss/critic2                       | 23.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 87000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8773
----------------------------------------------------------------------------------
| alpha                              | 0.322    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -598     |
| loss/alpha                         | -0.0223  |
| loss/critic1                       | 22.4     |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 88000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8946
----------------------------------------------------------------------------------
| alpha                              | 0.32     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53       |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -601     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 89000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9228
-----------------------------------------------------------------------------------
| alpha                              | 0.319     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.6      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -603      |
| loss/alpha                         | -0.000885 |
| loss/critic1                       | 21.1      |
| loss/critic2                       | 21.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 90000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8765
-----------------------------------------------------------------------------------
| alpha                              | 0.318     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52.3      |
| eval/normalized_episode_reward_std | 21.5      |
| loss/actor                         | -605      |
| loss/alpha                         | -0.000384 |
| loss/critic1                       | 20.1      |
| loss/critic2                       | 20.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 91000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9100
----------------------------------------------------------------------------------
| alpha                              | 0.316    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -607     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 92000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8935
----------------------------------------------------------------------------------
| alpha                              | 0.314    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -609     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 93000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9139
----------------------------------------------------------------------------------
| alpha                              | 0.31     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.1     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -611     |
| loss/alpha                         | -0.0403  |
| loss/critic1                       | 20       |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 94000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9051
----------------------------------------------------------------------------------
| alpha                              | 0.309    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -613     |
| loss/alpha                         | -0.0031  |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 95000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8936
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -615     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9042
----------------------------------------------------------------------------------
| alpha                              | 0.309    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -618     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 97000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9235
-----------------------------------------------------------------------------------
| alpha                              | 0.309     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 14.1      |
| loss/actor                         | -620      |
| loss/alpha                         | -0.000134 |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 20.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 98000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9314
----------------------------------------------------------------------------------
| alpha                              | 0.31     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.1     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -622     |
| loss/alpha                         | -0.004   |
| loss/critic1                       | 19       |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 99000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9145
----------------------------------------------------------------------------------
| alpha                              | 0.308    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 38.1     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -624     |
| loss/alpha                         | -0.00686 |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 100000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9343
----------------------------------------------------------------------------------
| alpha                              | 0.309    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 9.12     |
| loss/actor                         | -626     |
| loss/alpha                         | 0.0213   |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 21.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 101000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9356
----------------------------------------------------------------------------------
| alpha                              | 0.31     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 9.7      |
| loss/actor                         | -627     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 102000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9289
----------------------------------------------------------------------------------
| alpha                              | 0.306    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.0395  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 22.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 103000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8968
----------------------------------------------------------------------------------
| alpha                              | 0.304    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -630     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 104000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9111
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.2     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 105000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9218
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -633     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9288
----------------------------------------------------------------------------------
| alpha                              | 0.303    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0129  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 107000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9387
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -636     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 108000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8953
----------------------------------------------------------------------------------
| alpha                              | 0.302    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.0156   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 109000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.301    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 110000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9464
----------------------------------------------------------------------------------
| alpha                              | 0.3      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -641     |
| loss/alpha                         | -0.0319  |
| loss/critic1                       | 19       |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9140
----------------------------------------------------------------------------------
| alpha                              | 0.298    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -642     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 112000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9238
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -644     |
| loss/alpha                         | -0.0415  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 113000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9199
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 114000   |
----------------------------------------------------------------------------------
num rollout transitions: 249999, reward mean: 4.9075
----------------------------------------------------------------------------------
| alpha                              | 0.299    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.1     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -647     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 115000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9335
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 37.1     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -648     |
| loss/alpha                         | -0.038   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 116000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9495
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -650     |
| loss/alpha                         | -0.0431  |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 24.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 117000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9222
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -651     |
| loss/alpha                         | 0.0213   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 118000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9307
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.9     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.0357   |
| loss/critic1                       | 20       |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 119000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9183
----------------------------------------------------------------------------------
| alpha                              | 0.295    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 30.7     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -654     |
| loss/alpha                         | -0.00276 |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 120000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9431
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.00553 |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 121000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9455
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -657     |
| loss/alpha                         | 0.0022   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 122000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9057
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 123000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9337
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.00319  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 124000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9482
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 125000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9395
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.4     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 126000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9396
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.00913  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 127000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9428
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 3.82     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 128000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9375
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.9     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.00254  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 129000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9561
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 9.35     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.0381   |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9374
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.00661 |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9437
----------------------------------------------------------------------------------
| alpha                              | 0.29     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 5.65     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.003    |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 132000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9518
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.00353  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 20.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 133000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9375
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0127  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 134000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9395
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.2     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 135000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9345
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.8     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0244   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9497
----------------------------------------------------------------------------------
| alpha                              | 0.288    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.5     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0146  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 137000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9513
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 45.7     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.00511  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 138000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9520
----------------------------------------------------------------------------------
| alpha                              | 0.291    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.5     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0416   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 139000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9503
----------------------------------------------------------------------------------
| alpha                              | 0.293    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.00997  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 140000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9357
----------------------------------------------------------------------------------
| alpha                              | 0.294    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.8     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -682     |
| loss/alpha                         | 0.00273  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 141000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9671
----------------------------------------------------------------------------------
| alpha                              | 0.292    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0443  |
| loss/critic1                       | 20       |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 142000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9489
----------------------------------------------------------------------------------
| alpha                              | 0.289    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9324
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0381  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 144000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9377
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00172  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.00731 |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 146000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9409
----------------------------------------------------------------------------------
| alpha                              | 0.286    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0292   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 147000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9675
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0406  |
| loss/critic1                       | 19       |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 148000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9142
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.2     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0372   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 150000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9156
----------------------------------------------------------------------------------
| alpha                              | 0.284    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0141   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 151000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9239
-----------------------------------------------------------------------------------
| alpha                              | 0.285     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.9      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.000984 |
| loss/critic1                       | 18        |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 152000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9189
----------------------------------------------------------------------------------
| alpha                              | 0.285    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 153000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9423
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -693     |
| loss/alpha                         | -0.037   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 154000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9421
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 155000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9535
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.000945 |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 156000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9529
-----------------------------------------------------------------------------------
| alpha                              | 0.279     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.9      |
| eval/normalized_episode_reward_std | 25.7      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.000228 |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 157000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9553
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 28.1     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00874  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 158000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9789
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 4.24     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 159000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9449
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 160000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9511
----------------------------------------------------------------------------------
| alpha                              | 0.283    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 11.9     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 161000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9447
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -698     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 162000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9408
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 163000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9614
-----------------------------------------------------------------------------------
| alpha                              | 0.279     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 19.4      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.000911 |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 164000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9607
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0091   |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 165000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9423
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 166000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9257
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.7     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -701     |
| loss/alpha                         | 0.00127  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 167000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9616
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48       |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0333   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 168000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 3.93     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.00452  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 169000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9392
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 32.9     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0367  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 170000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
----------------------------------------------------------------------------------
| alpha                              | 0.281    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.3     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.0272   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 171000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9531
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.5     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -704     |
| loss/alpha                         | -0.0249  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 172000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9678
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.008    |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 173000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9591
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0232   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 174000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.1     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 175000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9677
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0246  |
| loss/critic1                       | 19       |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 176000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9460
----------------------------------------------------------------------------------
| alpha                              | 0.282    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.00011 |
| loss/critic1                       | 18       |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 177000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9582
----------------------------------------------------------------------------------
| alpha                              | 0.28     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.1     |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.00225 |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 178000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9526
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.5     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 179000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9569
----------------------------------------------------------------------------------
| alpha                              | 0.278    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.00419 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 180000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9457
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0364   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 181000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9548
----------------------------------------------------------------------------------
| alpha                              | 0.279    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.3     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.044   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 182000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9521
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9577
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 36.9     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0218   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.8     |
| eval/normalized_episode_reward_std | 27.5     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0297  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 185000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.8     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 186000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9632
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.3     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.00982  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 187000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9466
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0351  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 188000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9511
----------------------------------------------------------------------------------
| alpha                              | 0.277    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 8.1      |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 189000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9540
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9398
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 7.21     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00509 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 191000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9680
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0364  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9666
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0427   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 193000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9559
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.3     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00533 |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 194000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9665
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.000125 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 195000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9454
----------------------------------------------------------------------------------
| alpha                              | 0.274    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9564
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 8.71     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 197000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9554
----------------------------------------------------------------------------------
| alpha                              | 0.276    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 43.1     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -720     |
| loss/alpha                         | 9.35e-05 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 198000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.4     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00405 |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 199000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9842
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0173  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 200000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9469
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52       |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 201000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9708
----------------------------------------------------------------------------------
| alpha                              | 0.275    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0459  |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 202000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9574
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.9     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0127   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 203000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9760
----------------------------------------------------------------------------------
| alpha                              | 0.272    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 204000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9551
----------------------------------------------------------------------------------
| alpha                              | 0.273    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.000996 |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 205000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
----------------------------------------------------------------------------------
| alpha                              | 0.271    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.3     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.00422 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 206000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9776
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0724  |
| loss/critic1                       | 18       |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 207000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0537   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 208000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9510
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.00958 |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 210000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9605
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0026   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 211000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9640
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 7.82     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.034    |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 212000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9486
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00644 |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 213000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9693
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0472  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 214000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9729
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.4     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 215000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3.81     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0439   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 216000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9496
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.5     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 17       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 217000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0403   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 218000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -723     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 219000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9512
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -723     |
| loss/alpha                         | -0.0823  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 220000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.2     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -724     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 221000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -724     |
| loss/alpha                         | -0.0019  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 222000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -724     |
| loss/alpha                         | 0.00539  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 223000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9672
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.2     |
| eval/normalized_episode_reward_std | 26.5     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.033    |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.1     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -725     |
| loss/alpha                         | 0.00958  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -726     |
| loss/alpha                         | 0.00704  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 226000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -727     |
| loss/alpha                         | 0.000153 |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 227000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9657
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -727     |
| loss/alpha                         | -0.0226  |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 228000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9689
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -728     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9602
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -728     |
| loss/alpha                         | -0.00617 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 230000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9724
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 8.95     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 231000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9661
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -730     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 232000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -731     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 233000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 9.45     |
| loss/actor                         | -732     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -732     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 235000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.8     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -733     |
| loss/alpha                         | 0.00486  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 236000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 237000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.0111   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 7.23     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.016   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 239000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9721
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -734     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 240000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -734     |
| loss/alpha                         | -0.0498  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 241000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9719
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.97     |
| loss/actor                         | -735     |
| loss/alpha                         | 0.00706  |
| loss/critic1                       | 16       |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 242000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9599
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -736     |
| loss/alpha                         | 0.0487   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 243000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9879
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 244000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 7.34     |
| loss/actor                         | -736     |
| loss/alpha                         | -0.00472 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 245000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9446
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0125   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -737     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 247000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9694
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -737     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 248000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9728
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -737     |
| loss/alpha                         | 0.0157   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 249000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9763
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0221  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 250000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 16.4     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 251000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 252000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9734
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 253000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9809
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -738     |
| loss/alpha                         | 0.0235   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 254000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9724
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9601
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.00306  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 256000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 8.38     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0454   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 257000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9657
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 6.96     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0491  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0336   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 259000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9722
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 260000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9744
----------------------------------------------------------------------------------
| alpha                              | 0.268    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0296   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 261000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.267    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 16       |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 262000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9824
----------------------------------------------------------------------------------
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0262  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9675
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -738     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 264000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9822
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 265000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -739     |
| loss/alpha                         | 0.0167   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 266000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9688
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.00755 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 267000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9593
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 5.57     |
| loss/actor                         | -739     |
| loss/alpha                         | -0.0256  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.00404 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 269000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.014   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 270000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0167  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 271000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9624
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0315   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 272000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.000752 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 273000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9645
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0134   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 274000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9747
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -740     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 275000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.0454  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 276000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9557
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 6.97     |
| loss/actor                         | -740     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56       |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -741     |
| loss/alpha                         | 0.0378   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 278000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -741     |
| loss/alpha                         | -0.00367 |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -742     |
| loss/alpha                         | 0.00448  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 280000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9770
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -742     |
| loss/alpha                         | -0.008   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 281000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9753
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -742     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 282000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -743     |
| loss/alpha                         | -0.00606 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 283000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9734
----------------------------------------------------------------------------------
| alpha                              | 0.264    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.00646  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 284000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9773
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.1     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 285000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9897
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.0202  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 286000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -745     |
| loss/alpha                         | 0.0327   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 287000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 12.4     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 288000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| alpha                              | 0.261    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.0199  |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 289000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9601
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -744     |
| loss/alpha                         | -0.059   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 290000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 9.88     |
| loss/actor                         | -744     |
| loss/alpha                         | 0.0407   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 291000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 292000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -745     |
| loss/alpha                         | -0.0402  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 293000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -745     |
| loss/alpha                         | 0.00787  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 294000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0354   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 295000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9690
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0189   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 296000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 6.84     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 297000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9792
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 31.1     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 298000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.2     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0331  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0387   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 300000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 301000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0566  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 302000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 8.28     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.00611  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 303000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 7.49     |
| loss/actor                         | -746     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 304000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.00569  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 305000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -746     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 306000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9710
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -747     |
| loss/alpha                         | 0.0256   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 307000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
-----------------------------------------------------------------------------------
| alpha                              | 0.256     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -747      |
| loss/alpha                         | -3.93e-05 |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 308000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 30.9     |
| loss/actor                         | -748     |
| loss/alpha                         | -0.00894 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 309000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0317   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 310000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -749     |
| loss/alpha                         | 0.0304   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 311000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00345  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 312000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9738
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -750     |
| loss/alpha                         | 0.00405  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 313000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9829
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00781 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 314000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -751     |
| loss/alpha                         | -0.00709 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 315000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9705
-----------------------------------------------------------------------------------
| alpha                              | 0.26      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 6.73      |
| loss/actor                         | -752      |
| loss/alpha                         | -0.000639 |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 316000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9815
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -752     |
| loss/alpha                         | 0.00379  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 317000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 4.02     |
| loss/actor                         | -753     |
| loss/alpha                         | -0.00187 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 318000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -753     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9678
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 4.1      |
| loss/actor                         | -754     |
| loss/alpha                         | 0.0153   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 320000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| alpha                              | 0.26     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 20.4     |
| loss/actor                         | -754     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 321000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 31.6     |
| loss/actor                         | -755     |
| loss/alpha                         | -0.0148  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 322000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -755     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 323000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.78     |
| loss/actor                         | -755     |
| loss/alpha                         | -0.00436 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 324000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9652
----------------------------------------------------------------------------------
| alpha                              | 0.262    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -755     |
| loss/alpha                         | 0.0687   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 325000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
-----------------------------------------------------------------------------------
| alpha                              | 0.264     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 6.78      |
| loss/actor                         | -756      |
| loss/alpha                         | -0.000589 |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 326000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -756     |
| loss/alpha                         | 0.008    |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 327000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| alpha                              | 0.265    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -756     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0095  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 329000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9773
----------------------------------------------------------------------------------
| alpha                              | 0.263    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 5.94     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9666
----------------------------------------------------------------------------------
| alpha                              | 0.259    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 331000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -758     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 332000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9593
----------------------------------------------------------------------------------
| alpha                              | 0.258    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.00627 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 333000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -758     |
| loss/alpha                         | -0.0027  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9670
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 335000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9535
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 6.13     |
| loss/actor                         | -757     |
| loss/alpha                         | 0.0216   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 336000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.00362 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 337000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9604
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0263  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 338000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9580
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 339000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9462
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 340000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9728
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -757     |
| loss/alpha                         | 0.0184   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 341000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -757     |
| loss/alpha                         | 0.0239   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 342000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -757     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 343000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -758     |
| loss/alpha                         | 0.00661  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 344000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9744
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -758     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.00371 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 346000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.00137 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 347000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -757     |
| loss/alpha                         | -0.0457  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 348000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -758     |
| loss/alpha                         | 0.0368   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 349000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -758     |
| loss/alpha                         | 0.0169   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 350000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -758     |
| loss/alpha                         | 0.0073   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 351000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9774
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -758     |
| loss/alpha                         | 0.00882  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 352000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -758     |
| loss/alpha                         | -0.026   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 353000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9887
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.00938 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 354000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| alpha                              | 0.256    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.0257   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 355000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9824
----------------------------------------------------------------------------------
| alpha                              | 0.257    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.8     |
| eval/normalized_episode_reward_std | 28.1     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.0016   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 356000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -759     |
| loss/alpha                         | -0.0194  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 357000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.045   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 358000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9562
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 359000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.0244   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 360000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9817
----------------------------------------------------------------------------------
| alpha                              | 0.254    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.0246  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 361000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9778
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.00723 |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 362000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9811
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.00054 |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 363000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 364000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| alpha                              | 0.255    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -759     |
| loss/alpha                         | 0.00304  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 365000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| alpha                              | 0.253    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -759     |
| loss/alpha                         | -0.0221  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9873
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -760     |
| loss/alpha                         | -0.0219  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 367000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.5     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -760     |
| loss/alpha                         | -0.00553 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 368000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -760     |
| loss/alpha                         | -0.0388  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 369000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9679
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -760     |
| loss/alpha                         | -0.0334  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 370000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -761     |
| loss/alpha                         | 0.0261   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 371000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -761     |
| loss/alpha                         | -0.014   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 372000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -761     |
| loss/alpha                         | 0.00974  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 373000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 374000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.00936  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 375000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 376000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.8     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 377000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -762     |
| loss/alpha                         | 0.00633  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 378000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
-----------------------------------------------------------------------------------
| alpha                              | 0.25      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 6.94      |
| loss/actor                         | -763      |
| loss/alpha                         | -0.000328 |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 379000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -763     |
| loss/alpha                         | 0.00362  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 380000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.61     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.00804 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -763     |
| loss/alpha                         | -0.00487 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 382000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -763     |
| loss/alpha                         | -0.0259  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 383000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -763     |
| loss/alpha                         | 0.0435   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 384000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 4.41     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.01    |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 385000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9892
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 9.28     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0421  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9911
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 8.23     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.00418 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 10.2     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 388000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -761     |
| loss/alpha                         | 0.0328   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 389000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -761     |
| loss/alpha                         | -0.00909 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 390000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -761     |
| loss/alpha                         | 0.0281   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 391000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -761     |
| loss/alpha                         | -0.00643 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 392000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0552  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 393000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -761     |
| loss/alpha                         | 0.0138   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 394000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9720
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 395000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9762
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 396000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 397000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -762     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 398000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -763     |
| loss/alpha                         | -0.0164  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.00566 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 400000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9637
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -763     |
| loss/alpha                         | 0.00658  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 5.97     |
| loss/actor                         | -763     |
| loss/alpha                         | 0.00166  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 402000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 9.09     |
| loss/actor                         | -763     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 403000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 9.46     |
| loss/actor                         | -762     |
| loss/alpha                         | -0.00174 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 404000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -763     |
| loss/alpha                         | -0.0127  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9915
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -763     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 406000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9826
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -764     |
| loss/alpha                         | -0.0115  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 407000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -764     |
| loss/alpha                         | 0.00486  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 408000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 27.8     |
| loss/actor                         | -764     |
| loss/alpha                         | 0.00235  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -765     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 410000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -765     |
| loss/alpha                         | 0.00922  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 411000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 9.17     |
| loss/actor                         | -765     |
| loss/alpha                         | -0.00434 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 412000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9922
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -766     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.1     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -766     |
| loss/alpha                         | 0.0264   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 414000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -766     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 415000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -766     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 416000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -767     |
| loss/alpha                         | 0.0181   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 417000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.5     |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.0324   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 418000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -767     |
| loss/alpha                         | -0.0641  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 419000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9716
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.0113   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 420000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.0318   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -767     |
| loss/alpha                         | 0.00105  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 422000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 9.19     |
| loss/actor                         | -767     |
| loss/alpha                         | -0.00891 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 423000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 6.11     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.00102  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 424000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -767     |
| loss/alpha                         | -0.00937 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 425000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -767     |
| loss/alpha                         | 0.0274   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 426000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -768     |
| loss/alpha                         | -0.00819 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -768     |
| loss/alpha                         | -0.00122 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -768     |
| loss/alpha                         | -0.0475  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 429000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -769     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 430000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 9.26     |
| loss/actor                         | -769     |
| loss/alpha                         | -0.00859 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 431000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -770     |
| loss/alpha                         | 0.0016   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 432000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9824
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -770     |
| loss/alpha                         | 0.0312   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 433000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.3     |
| eval/normalized_episode_reward_std | 32.3     |
| loss/actor                         | -770     |
| loss/alpha                         | -0.00814 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 434000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
----------------------------------------------------------------------------------
| alpha                              | 0.251    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 435000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.00012 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 437000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9706
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 8.92     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.00604  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 438000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.00652 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 440000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0192   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 441000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.011    |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 442000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.00675  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 443000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.8     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.018    |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 444000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9772
----------------------------------------------------------------------------------
| alpha                              | 0.252    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.00855  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 445000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.25     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.2     |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0462  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 446000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| alpha                              | 0.249    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -772     |
| loss/alpha                         | 0.00851  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 447000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 30.7     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 448000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.86     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 449000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 450000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 451000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.00786 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 452000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 453000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 30.2     |
| loss/actor                         | -773     |
| loss/alpha                         | 0.0491   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 454000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 32       |
| loss/actor                         | -773     |
| loss/alpha                         | 0.00755  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 455000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -773     |
| loss/alpha                         | 0.00273  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 456000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
----------------------------------------------------------------------------------
| alpha                              | 0.248    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 9.05     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0152   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 457000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9853
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 9.98     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0204  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 458000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 8.86     |
| loss/actor                         | -772     |
| loss/alpha                         | -0.0642  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.9     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.00331  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 460000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 8.83     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0387   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 461000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 4.08     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0353  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 462000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9782
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 463000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -770     |
| loss/alpha                         | 0.0374   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 464000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 7.63     |
| loss/actor                         | -770     |
| loss/alpha                         | -0.00495 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 465000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 8.74     |
| loss/actor                         | -770     |
| loss/alpha                         | -0.00186 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -770     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 467000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -770     |
| loss/alpha                         | 0.045    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 468000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9824
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0101   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 469000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56       |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0141   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 470000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 27.8     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.00119 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 471000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0209   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 472000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 9.55     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0081  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 473000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 474000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.3     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.00212  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 475000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0148   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 476000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.7     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0434  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 477000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0174  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 478000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -771     |
| loss/alpha                         | 0.0224   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 479000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9892
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.031   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 480000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -771     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 481000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -772     |
| loss/alpha                         | -0.00803 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 482000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0459   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 483000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 29.9     |
| loss/actor                         | -772     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 484000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -773     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 485000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -773     |
| loss/alpha                         | -0.00922 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 486000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9826
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -774     |
| loss/alpha                         | -0.014   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 487000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -774     |
| loss/alpha                         | 0.0236   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 488000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -775     |
| loss/alpha                         | -0.0249  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 489000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 48.3     |
| eval/normalized_episode_reward_std | 33       |
| loss/actor                         | -775     |
| loss/alpha                         | -0.0179  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 490000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -775     |
| loss/alpha                         | 0.00211  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 491000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -775     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -776     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 493000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 15.7     |
| loss/actor                         | -776     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -776     |
| loss/alpha                         | 0.0194   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 495000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -777     |
| loss/alpha                         | 0.00426  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 496000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 6.47     |
| loss/actor                         | -777     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 497000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -777     |
| loss/alpha                         | -0.00663 |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 498000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9742
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -778     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 499000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 7.16     |
| loss/actor                         | -778     |
| loss/alpha                         | 0.0055   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 500000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 9.11     |
| loss/actor                         | -779     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 501000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -779     |
| loss/alpha                         | 0.00964  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0712   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 503000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.0453  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 504000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0425  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 506000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.00256  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 507000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.00624 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 508000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 9.67     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.00689 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 509000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.00372  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 510000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 511000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.0465   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 512000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.00461  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 514000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 515000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 516000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0215  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 517000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 518000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.0214   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 519000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0102   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 520000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 4.43     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0407  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 521000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 522000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0409   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 523000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0278  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 9.63     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0359  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 525000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 6.83     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0368   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 526000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 27.2     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 527000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.00122 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 528000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9784
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 530000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 22.3     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.00727  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 531000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9872
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.9     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -780     |
| loss/alpha                         | 0.00774  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 532000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.1     |
| eval/normalized_episode_reward_std | 30.9     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 533000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 28.3     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 534000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.00996 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.00467 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 536000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 537000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.0052  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 538000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.1     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.000729 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 539000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 9.7      |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 540000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.8     |
| eval/normalized_episode_reward_std | 33.6     |
| loss/actor                         | -780     |
| loss/alpha                         | -0.00398 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 541000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0171   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 542000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -780     |
| loss/alpha                         | 0.0231   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 543000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| alpha                              | 0.246    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0485   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 544000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| alpha                              | 0.247    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.4     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0523  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 545000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.0192   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 546000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -781     |
| loss/alpha                         | -0.0365  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 547000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -781     |
| loss/alpha                         | 0.00488  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 548000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.4     |
| eval/normalized_episode_reward_std | 29.1     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.0321  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 549000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 550000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -782     |
| loss/alpha                         | -0.00994 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 551000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.5     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -782     |
| loss/alpha                         | 0.00194  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 552000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -783     |
| loss/alpha                         | 0.0352   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 553000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -783     |
| loss/alpha                         | 0.00842  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 554000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58       |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -783     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 555000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -784     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 556000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.9     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -784     |
| loss/alpha                         | -0.0448  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 557000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55       |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -784     |
| loss/alpha                         | -0.00342 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 25       |
| loss/actor                         | -785     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -784     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 560000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 561000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.00404 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 562000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.0344   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 563000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9958
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.2     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 564000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.0135  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 565000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -784     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 566000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -784     |
| loss/alpha                         | -0.00599 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 567000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.0115  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 568000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -784     |
| loss/alpha                         | -0.00468 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.2     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.0116   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 570000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 5.15     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.00936  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 571000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.6     |
| eval/normalized_episode_reward_std | 34.5     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.00956 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -785     |
| loss/alpha                         | 0.0268   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 573000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.6     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -785     |
| loss/alpha                         | -0.0106  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 574000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -786     |
| loss/alpha                         | 0.043    |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.8     |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -786     |
| loss/alpha                         | -0.0575  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 576000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -787     |
| loss/alpha                         | 0.043    |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 577000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -787     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 578000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -787     |
| loss/alpha                         | -0.00601 |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 579000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.2     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -788     |
| loss/alpha                         | 0.00404  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -788     |
| loss/alpha                         | -0.056   |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 581000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -788     |
| loss/alpha                         | 0.0231   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 582000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -788     |
| loss/alpha                         | 0.000985 |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 583000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| alpha                              | 0.24      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 55.3      |
| eval/normalized_episode_reward_std | 30.6      |
| loss/actor                         | -788      |
| loss/alpha                         | -0.000598 |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 584000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -788     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 585000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -788     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 586000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -789     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 587000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 6.09     |
| loss/actor                         | -788     |
| loss/alpha                         | -0.00378 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 588000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -789     |
| loss/alpha                         | 0.00215  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 589000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -789     |
| loss/alpha                         | 0.0258   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 590000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -789     |
| loss/alpha                         | 0.0565   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 591000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 4.92     |
| loss/actor                         | -789     |
| loss/alpha                         | -0.0541  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 592000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9927
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 32.5     |
| loss/actor                         | -789     |
| loss/alpha                         | 0.00572  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 593000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 594000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.00734 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 596000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.00743  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 597000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 598000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 10.3     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.00429  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 599000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0056  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 600000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0274  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 601000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 602000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 12.2     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0122   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 603000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 604000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0503   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 605000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0243  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 606000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 607000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.9     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -790     |
| loss/alpha                         | -0.00832 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 608000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 609000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0054   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 610000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0452   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 611000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.000369 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.3     |
| eval/normalized_episode_reward_std | 32.3     |
| loss/actor                         | -790     |
| loss/alpha                         | 0.0417   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 613000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.00293 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 614000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.1     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.00639 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 615000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 616000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.4     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 617000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -790     |
| loss/alpha                         | -0.00163 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 618000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 619000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 620000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.00375 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 621000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9892
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.00396 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 622000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.00371 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 623000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 27.7     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 625000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 626000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.00138  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 627000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.0169   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 628000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -791     |
| loss/alpha                         | 0.0302   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 629000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 630000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.029    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 631000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -791     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 632000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.00752  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 633000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.00519  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.00156 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 636000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -792     |
| loss/alpha                         | -0.00225 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 638000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -792     |
| loss/alpha                         | 0.0455   |
| loss/critic1                       | 16       |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 639000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.7     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -793     |
| loss/alpha                         | -0.00823 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 640000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| alpha                              | 0.243    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -793     |
| loss/alpha                         | -0.0233  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 641000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.1     |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -794     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 642000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -794     |
| loss/alpha                         | 0.00295  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.3     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -794     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 644000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -794     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -795     |
| loss/alpha                         | -0.00489 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -795     |
| loss/alpha                         | 0.0491   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 647000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57       |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -795     |
| loss/alpha                         | -0.0193  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 648000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.2     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -795     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0088  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 650000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.044    |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 651000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 652000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| alpha                              | 0.241    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0362  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 653000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0278  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 654000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 27.8     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 655000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
----------------------------------------------------------------------------------
| alpha                              | 0.24     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00222  |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 656000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| alpha                              | 0.239    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 657000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 5.61     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 659000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0293  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 660000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00914 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 662000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0174  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 663000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00322  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 664000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| alpha                              | 0.236     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56        |
| eval/normalized_episode_reward_std | 29.9      |
| loss/actor                         | -796      |
| loss/alpha                         | -5.44e-05 |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 665000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
-----------------------------------------------------------------------------------
| alpha                              | 0.234     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.9      |
| eval/normalized_episode_reward_std | 33.8      |
| loss/actor                         | -796      |
| loss/alpha                         | -0.000753 |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 666000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00929 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 667000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0156   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 668000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.6     |
| eval/normalized_episode_reward_std | 6.11     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00618 |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 669000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00369  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 670000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 31.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 671000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.9     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0152   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 672000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 673000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0023   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 674000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0223  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 675000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0706   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 676000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9964
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0073  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 677000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00365 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 678000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00179 |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 679000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00091 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 680000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00301  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 681000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00545 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 682000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0253  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 683000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 684000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00596 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 685000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0243   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 686000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00294 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 687000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 32.8     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00509 |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 688000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0166  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 689000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0228   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 9.95     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00138 |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 691000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 34.2     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00467 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 692000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 25.3     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00513  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 693000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 694000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0323   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 695000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0325  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 696000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 697000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.8     |
| eval/normalized_episode_reward_std | 33       |
| loss/actor                         | -797     |
| loss/alpha                         | -0.01    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 698000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00744 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 699000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00702  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 700000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00176  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 701000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00503 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 702000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00943  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 703000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00955  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 704000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00898 |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 705000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
-----------------------------------------------------------------------------------
| alpha                              | 0.236     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 15.6      |
| loss/actor                         | -796      |
| loss/alpha                         | -0.000718 |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 706000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.7     |
| eval/normalized_episode_reward_std | 34.6     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00127  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 707000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 708000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| alpha                              | 0.236     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 24.6      |
| loss/actor                         | -796      |
| loss/alpha                         | -0.000801 |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 709000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.3     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 710000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9853
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00877 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 711000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00103  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 712000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -795     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 713000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.6     |
| eval/normalized_episode_reward_std | 32.2     |
| loss/actor                         | -795     |
| loss/alpha                         | -0.00555 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 714000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.8     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -795     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 715000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -795     |
| loss/alpha                         | -0.00899 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 716000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00615 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 717000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00665  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 718000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0326   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 719000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0533  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 720000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0353   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 721000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 29.2     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00769 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 722000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.00926 |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 723000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00222  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 724000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 725000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 31.6     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.0344   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 726000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -796     |
| loss/alpha                         | 0.00109  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 727000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.2     |
| eval/normalized_episode_reward_std | 32.3     |
| loss/actor                         | -796     |
| loss/alpha                         | -0.0445  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 728000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 729000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0505   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 730000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0342  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00973  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.1     |
| eval/normalized_episode_reward_std | 6.95     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.00412  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.3     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00417 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 734000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 26.8     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 735000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0407   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 736000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00873 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 737000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 738000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 739000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0024  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 740000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.5     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.00794  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.9     |
| eval/normalized_episode_reward_std | 4.7      |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00988 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 742000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00223 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 743000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0048  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 744000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 746000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -799     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 747000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0343   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.00385 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 751000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 752000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.00935 |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 753000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0686   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 754000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 31.5     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0215  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 755000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 756000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 757000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.8     |
| eval/normalized_episode_reward_std | 29.9     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.00844 |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0272   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 759000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.00109 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 760000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 9.83     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 761000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 762000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.00266  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 763000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 764000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0555  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 765000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0217   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 766000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.00242  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 767000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 29.8     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 768000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.5     |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0171   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 769000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.1     |
| eval/normalized_episode_reward_std | 3.87     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 770000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0314  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 771000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0272   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 772000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 34.8     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0136   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| alpha                              | 0.237    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0122   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 775000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 776000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 26.6     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 777000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 4.13     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 778000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 24.7     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 779000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 780000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 29.6     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0146  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 8.75     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.00522  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 782000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00499 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.025    |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 784000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0404  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 785000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0307   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 786000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 30.3     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0246   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 787000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 788000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0181  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00271  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 790000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00578 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 791000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 26.9     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00541  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 792000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.0334  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 793000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.0218   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -797     |
| loss/alpha                         | 0.00823  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 795000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 27.5     |
| loss/actor                         | -797     |
| loss/alpha                         | -0.00186 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 796000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00825 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 797000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 29.3     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59       |
| eval/normalized_episode_reward_std | 25.5     |
| loss/actor                         | -798     |
| loss/alpha                         | 0.0311   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 799000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -798     |
| loss/alpha                         | -0.00279 |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 800000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 30.2     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.0481  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 801000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0563   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 802000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -799     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 804000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00777  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 805000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.2     |
| eval/normalized_episode_reward_std | 7.61     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00256 |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 806000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0731  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 807000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0414   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0343  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 809000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00839  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 810000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00768 |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 28.9     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00205 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 812000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 813000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.019    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 814000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 815000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0357   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 816000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 28.6     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 817000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00894 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 818000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0115   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0165  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 820000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.86     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00708  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 821000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0173  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 822000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0445   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0206   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0514  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00382 |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 24.3     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0362   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 827000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00358  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 25.3     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0048  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0101   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 830000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0175  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 831000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0092   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 832000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00356  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 833000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.5     |
| eval/normalized_episode_reward_std | 26.3     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 834000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0188  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 835000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.2     |
| eval/normalized_episode_reward_std | 29.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.000466 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 836000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 29       |
| loss/actor                         | -801     |
| loss/alpha                         | 0.00111  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 838000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 25.6     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.01    |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 839000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 28.1     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0207   |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 31.4     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00167  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 841000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 842000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00557 |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 843000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 844000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 845000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 28       |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 846000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0585  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 847000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55       |
| eval/normalized_episode_reward_std | 28.7     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00256 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 848000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 32.2     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0398   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 849000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 850000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0473  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 852000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0417  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 853000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0614   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 854000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 27.1     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 855000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 29.7     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0006  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 856000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.5     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00539 |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 857000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 25.3     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0496  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 858000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 859000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 27.5     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0083   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 860000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 861000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 5.85     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0156   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 862000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 863000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 30.6     |
| loss/actor                         | -799     |
| loss/alpha                         | -0.00185 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 864000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.4     |
| eval/normalized_episode_reward_std | 30.7     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 865000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0478   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 866000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 23.9     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 867000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00747 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 868000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 29.9     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 869000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.4     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 31.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 871000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0481  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 872000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0267   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 873000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00253 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 874000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.8     |
| eval/normalized_episode_reward_std | 3.8      |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0148  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 875000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 32.8     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 876000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0329   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 877000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0595  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 878000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00977 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 880000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 4.7      |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00363 |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 881000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 30.9     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 882000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 883000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00799 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 884000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 31.5     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 885000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 31.5     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00457 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 886000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.2     |
| eval/normalized_episode_reward_std | 6.07     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00645 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 888000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 14.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 889000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.3     |
| eval/normalized_episode_reward_std | 5.69     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 890000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60       |
| eval/normalized_episode_reward_std | 30.2     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.00939  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 891000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 30.1     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00597 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 892000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00264 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.032    |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 894000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 895000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 30.7     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 896000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0616   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 897000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 33.6     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0505  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 898000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0371   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 899000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.035   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 900000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 901000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.00901 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 902000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0378   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 26.7     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 905000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 28.2     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0275  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 906000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.5     |
| eval/normalized_episode_reward_std | 27.9     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0829   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 907000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| alpha                              | 0.236    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 33.6     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 908000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 28.1     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0119  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 909000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 30       |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0672  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.3     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 911000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 5.88     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0743   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 912000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 9.22     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.000912 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 913000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00874 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 914000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00423 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 916000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00796 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 8.1      |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0295   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.9     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00884 |
| loss/critic1                       | 15       |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 919000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0286  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 920000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 921000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 922000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.6     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00662  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 923000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.0831  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 925000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 16.4     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 926000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0308   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 927000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.00185  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 928000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0455  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 929000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 7.26     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0202   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 930000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55       |
| eval/normalized_episode_reward_std | 32       |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 931000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 932000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 10.4     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0263   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 933000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 30.4     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.00275 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 23.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 936000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00119  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 937000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 9.65     |
| loss/actor                         | -800     |
| loss/alpha                         | 0.00466  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 938000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 12.6     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0404  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 939000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -800     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 940000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| alpha                              | 0.228    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.3     |
| eval/normalized_episode_reward_std | 6.2      |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 941000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -801     |
| loss/alpha                         | -0.0358  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 942000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83       |
| eval/normalized_episode_reward_std | 4.49     |
| loss/actor                         | -801     |
| loss/alpha                         | 0.0346   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 943000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 29.9     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0403   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 944000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 8.29     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0913   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
----------------------------------------------------------------------------------
| alpha                              | 0.242    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0616   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 946000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| alpha                              | 0.244    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 9.87     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0252  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 947000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| alpha                              | 0.238    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0912  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 948000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0222  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 949000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0018   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 950000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 25.9     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00448 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 951000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0455  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 952000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 953000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.00241 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 954000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0367   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 955000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0356   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 956000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0343  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 957000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 31       |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00883 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00749  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 959000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.6     |
| eval/normalized_episode_reward_std | 32.7     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.0264  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 960000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.0306   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| alpha                              | 0.235     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 21.9      |
| loss/actor                         | -803      |
| loss/alpha                         | -0.000284 |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 962000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0433  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 963000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 964000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -802     |
| loss/alpha                         | -0.00507 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 965000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00722  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 966000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00478  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 967000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.00412  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -802     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 969000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 28.4     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.00454 |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 970000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.00993  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 971000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -803     |
| loss/alpha                         | 0.00903  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 972000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.5     |
| eval/normalized_episode_reward_std | 34.6     |
| loss/actor                         | -803     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 973000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0151  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 974000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00344  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 975000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00632  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 9.29     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.0141   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 977000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00822  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 978000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 34.9     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00326  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 980000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00472  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.6     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.00328 |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 982000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
-----------------------------------------------------------------------------------
| alpha                              | 0.233     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61        |
| eval/normalized_episode_reward_std | 30.4      |
| loss/actor                         | -804      |
| loss/alpha                         | -0.000778 |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 983000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 984000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 985000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| alpha                              | 0.233    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.7     |
| eval/normalized_episode_reward_std | 6.03     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 986000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| alpha                              | 0.231    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 7.8      |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 987000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 19.9     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0212  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 988000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 12.6     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 989000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.1     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 990000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -804     |
| loss/alpha                         | -0.00705 |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 991000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| alpha                              | 0.226    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -804     |
| loss/alpha                         | 0.00252  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.4     |
| eval/normalized_episode_reward_std | 34.9     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 993000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| alpha                              | 0.227    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 28.8     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 995000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| alpha                              | 0.229    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 996000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| alpha                              | 0.23     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 31.2     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 997000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| alpha                              | 0.232    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0252   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 998000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| alpha                              | 0.234    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -805     |
| loss/alpha                         | 0.0159   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 999000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| alpha                              | 0.235    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -806     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1000000  |
----------------------------------------------------------------------------------
total time: 65763.97s
