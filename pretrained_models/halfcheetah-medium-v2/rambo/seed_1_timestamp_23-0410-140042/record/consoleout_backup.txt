Pretraining policy
Training dynamics:
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.06513826 |
| loss/dynamics_train_loss   | -18.4      |
| timestep                   | 1          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.050004978 |
| loss/dynamics_train_loss   | -28.5       |
| timestep                   | 2           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.04410062 |
| loss/dynamics_train_loss   | -31.6      |
| timestep                   | 3          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.04035858 |
| loss/dynamics_train_loss   | -33.4      |
| timestep                   | 4          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.037683982 |
| loss/dynamics_train_loss   | -34.7       |
| timestep                   | 5           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.035999782 |
| loss/dynamics_train_loss   | -35.8       |
| timestep                   | 6           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03449289 |
| loss/dynamics_train_loss   | -36.6      |
| timestep                   | 7          |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03364333 |
| loss/dynamics_train_loss   | -37.3      |
| timestep                   | 8          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.032630794 |
| loss/dynamics_train_loss   | -37.9       |
| timestep                   | 9           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.031897504 |
| loss/dynamics_train_loss   | -38.5       |
| timestep                   | 10          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03104366 |
| loss/dynamics_train_loss   | -38.9      |
| timestep                   | 11         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.030520666 |
| loss/dynamics_train_loss   | -39.4       |
| timestep                   | 12          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.030000964 |
| loss/dynamics_train_loss   | -39.7       |
| timestep                   | 13          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02977764 |
| loss/dynamics_train_loss   | -40.1      |
| timestep                   | 14         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.029151533 |
| loss/dynamics_train_loss   | -40.4       |
| timestep                   | 15          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028855812 |
| loss/dynamics_train_loss   | -40.7       |
| timestep                   | 16          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028416205 |
| loss/dynamics_train_loss   | -41         |
| timestep                   | 17          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028166434 |
| loss/dynamics_train_loss   | -41.2       |
| timestep                   | 18          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.027803332 |
| loss/dynamics_train_loss   | -41.5       |
| timestep                   | 19          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.027289633 |
| loss/dynamics_train_loss   | -41.7       |
| timestep                   | 20          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026937684 |
| loss/dynamics_train_loss   | -41.9       |
| timestep                   | 21          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026784366 |
| loss/dynamics_train_loss   | -42.1       |
| timestep                   | 22          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026333708 |
| loss/dynamics_train_loss   | -42.4       |
| timestep                   | 23          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026140979 |
| loss/dynamics_train_loss   | -42.5       |
| timestep                   | 24          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02594451 |
| loss/dynamics_train_loss   | -42.7      |
| timestep                   | 25         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025599206 |
| loss/dynamics_train_loss   | -42.9       |
| timestep                   | 26          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02534182 |
| loss/dynamics_train_loss   | -43.1      |
| timestep                   | 27         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025046358 |
| loss/dynamics_train_loss   | -43.2       |
| timestep                   | 28          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024844442 |
| loss/dynamics_train_loss   | -43.4       |
| timestep                   | 29          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024644902 |
| loss/dynamics_train_loss   | -43.5       |
| timestep                   | 30          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02448029 |
| loss/dynamics_train_loss   | -43.6      |
| timestep                   | 31         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024135942 |
| loss/dynamics_train_loss   | -43.8       |
| timestep                   | 32          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023891132 |
| loss/dynamics_train_loss   | -43.9       |
| timestep                   | 33          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023877697 |
| loss/dynamics_train_loss   | -44.1       |
| timestep                   | 34          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023584321 |
| loss/dynamics_train_loss   | -44.2       |
| timestep                   | 35          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02334713 |
| loss/dynamics_train_loss   | -44.3      |
| timestep                   | 36         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023199942 |
| loss/dynamics_train_loss   | -44.4       |
| timestep                   | 37          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02294897 |
| loss/dynamics_train_loss   | -44.5      |
| timestep                   | 38         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02280629 |
| loss/dynamics_train_loss   | -44.7      |
| timestep                   | 39         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022605665 |
| loss/dynamics_train_loss   | -44.8       |
| timestep                   | 40          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02235405 |
| loss/dynamics_train_loss   | -44.9      |
| timestep                   | 41         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022239055 |
| loss/dynamics_train_loss   | -45         |
| timestep                   | 42          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02207437 |
| loss/dynamics_train_loss   | -45.1      |
| timestep                   | 43         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021896152 |
| loss/dynamics_train_loss   | -45.2       |
| timestep                   | 44          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021791581 |
| loss/dynamics_train_loss   | -45.3       |
| timestep                   | 45          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021645037 |
| loss/dynamics_train_loss   | -45.4       |
| timestep                   | 46          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021655757 |
| loss/dynamics_train_loss   | -45.5       |
| timestep                   | 47          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021489497 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 48          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02124795 |
| loss/dynamics_train_loss   | -45.7      |
| timestep                   | 49         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021190908 |
| loss/dynamics_train_loss   | -45.8       |
| timestep                   | 50          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02146346 |
| loss/dynamics_train_loss   | -45.9      |
| timestep                   | 51         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020985087 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 52          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020765081 |
| loss/dynamics_train_loss   | -46.1       |
| timestep                   | 53          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02073143 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 54         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020603523 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 55          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020522531 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 56          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020502888 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 57          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020287901 |
| loss/dynamics_train_loss   | -46.5       |
| timestep                   | 58          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020187672 |
| loss/dynamics_train_loss   | -46.6       |
| timestep                   | 59          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020032745 |
| loss/dynamics_train_loss   | -46.6       |
| timestep                   | 60          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019967766 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 61          |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0197603 |
| loss/dynamics_train_loss   | -46.8     |
| timestep                   | 62        |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01962268 |
| loss/dynamics_train_loss   | -46.9      |
| timestep                   | 63         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019563716 |
| loss/dynamics_train_loss   | -46.9       |
| timestep                   | 64          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019378623 |
| loss/dynamics_train_loss   | -47         |
| timestep                   | 65          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019540926 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 66          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019241061 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 67          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01924496 |
| loss/dynamics_train_loss   | -47.2      |
| timestep                   | 68         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019244444 |
| loss/dynamics_train_loss   | -47.3       |
| timestep                   | 69          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019065661 |
| loss/dynamics_train_loss   | -47.3       |
| timestep                   | 70          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019102957 |
| loss/dynamics_train_loss   | -47.4       |
| timestep                   | 71          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018754248 |
| loss/dynamics_train_loss   | -47.4       |
| timestep                   | 72          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019053081 |
| loss/dynamics_train_loss   | -47.5       |
| timestep                   | 73          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018826501 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 74          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018811604 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 75          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018522382 |
| loss/dynamics_train_loss   | -47.7       |
| timestep                   | 76          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018505123 |
| loss/dynamics_train_loss   | -47.8       |
| timestep                   | 77          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018401533 |
| loss/dynamics_train_loss   | -47.8       |
| timestep                   | 78          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018368157 |
| loss/dynamics_train_loss   | -47.9       |
| timestep                   | 79          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018286485 |
| loss/dynamics_train_loss   | -47.9       |
| timestep                   | 80          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018273085 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 81          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018188383 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 82          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018152792 |
| loss/dynamics_train_loss   | -48.1       |
| timestep                   | 83          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018202033 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 84          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017943893 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 85          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017915677 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 86          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017788183 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 87          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017873688 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 88          |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0178503 |
| loss/dynamics_train_loss   | -48.4     |
| timestep                   | 89        |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017774552 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 90          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017693529 |
| loss/dynamics_train_loss   | -48.5       |
| timestep                   | 91          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01761758 |
| loss/dynamics_train_loss   | -48.5      |
| timestep                   | 92         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017534424 |
| loss/dynamics_train_loss   | -48.6       |
| timestep                   | 93          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01741894 |
| loss/dynamics_train_loss   | -48.6      |
| timestep                   | 94         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017441254 |
| loss/dynamics_train_loss   | -48.7       |
| timestep                   | 95          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017344328 |
| loss/dynamics_train_loss   | -48.7       |
| timestep                   | 96          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017379453 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 97          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017200539 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 98          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01723228 |
| loss/dynamics_train_loss   | -48.8      |
| timestep                   | 99         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017079871 |
| loss/dynamics_train_loss   | -48.9       |
| timestep                   | 100         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017125392 |
| loss/dynamics_train_loss   | -48.9       |
| timestep                   | 101         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016995996 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 102         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01703989 |
| loss/dynamics_train_loss   | -49        |
| timestep                   | 103        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016954811 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 104         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016982684 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 105         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016854296 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 106         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017057385 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 107         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016789507 |
| loss/dynamics_train_loss   | -49.2       |
| timestep                   | 108         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01666208 |
| loss/dynamics_train_loss   | -49.2      |
| timestep                   | 109        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016825372 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 110         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016748875 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 111         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016788835 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 112         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01657362 |
| loss/dynamics_train_loss   | -49.4      |
| timestep                   | 113        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01653937 |
| loss/dynamics_train_loss   | -49.4      |
| timestep                   | 114        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016722396 |
| loss/dynamics_train_loss   | -49.4       |
| timestep                   | 115         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016499067 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 116         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016314447 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 117         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016407697 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 118         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016289378 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 119         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01638871 |
| loss/dynamics_train_loss   | -49.6      |
| timestep                   | 120        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016286086 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 121         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016261322 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 122         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01611444 |
| loss/dynamics_train_loss   | -49.7      |
| timestep                   | 123        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016169984 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 124         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01607801 |
| loss/dynamics_train_loss   | -49.8      |
| timestep                   | 125        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016074318 |
| loss/dynamics_train_loss   | -49.8       |
| timestep                   | 126         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01616216 |
| loss/dynamics_train_loss   | -49.8      |
| timestep                   | 127        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01599219 |
| loss/dynamics_train_loss   | -49.8      |
| timestep                   | 128        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016104963 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 129         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01592108 |
| loss/dynamics_train_loss   | -49.9      |
| timestep                   | 130        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016032878 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 131         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015783044 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 132         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015756825 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 133         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0158068 |
| loss/dynamics_train_loss   | -50       |
| timestep                   | 134       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015921544 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 135         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015674029 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 136         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015730921 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 137         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015733544 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 138         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01572073 |
| loss/dynamics_train_loss   | -50.1      |
| timestep                   | 139        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015505707 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 140         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015486151 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 141         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015493902 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 142         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015482465 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 143         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015421274 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 144         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015503256 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 145         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015336092 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 146         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015392398 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 147         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015558168 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 148         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015375522 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 149         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0154466005 |
| loss/dynamics_train_loss   | -50.4        |
| timestep                   | 150          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015300999 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 151         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015284395 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 152         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015137775 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 153         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015310759 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 154         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0151803 |
| loss/dynamics_train_loss   | -50.5     |
| timestep                   | 155       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015125683 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 156         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015140047 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 157         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015003281 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 158         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015002181 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 159         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015129939 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 160         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014896767 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 161         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014897483 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 162         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014934471 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 163         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014897538 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 164         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014843993 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 165         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014717477 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 166         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014766218 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 167         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014758359 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 168         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014654224 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 169         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014594953 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 170         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014598645 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 171         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014646992 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 172         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014590544 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 173         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014487984 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 174         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014730071 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 175         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01444702 |
| loss/dynamics_train_loss   | -51        |
| timestep                   | 176        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01450537 |
| loss/dynamics_train_loss   | -51        |
| timestep                   | 177        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01451049 |
| loss/dynamics_train_loss   | -51        |
| timestep                   | 178        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014423189 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 179         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01457715 |
| loss/dynamics_train_loss   | -51        |
| timestep                   | 180        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014334122 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 181         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01444055 |
| loss/dynamics_train_loss   | -51.1      |
| timestep                   | 182        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014416275 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 183         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014317465 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 184         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014380339 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 185         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014332543 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 186         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014379032 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 187         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014396472 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 188         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014259358 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 189         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014373536 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 190         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014321109 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 191         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014158937 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 192         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014158835 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 193         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014091739 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 194         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014075312 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 195         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01422188 |
| loss/dynamics_train_loss   | -51.3      |
| timestep                   | 196        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014177273 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 197         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014083934 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 198         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014018102 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 199         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014142332 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 200         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014090848 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 201         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014039619 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 202         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013977726 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 203         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013999527 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 204         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014009823 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 205         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0140170725 |
| loss/dynamics_train_loss   | -51.5        |
| timestep                   | 206          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013943298 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 207         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013788497 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 208         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013753896 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 209         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013874479 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 210         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013810186 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 211         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013731259 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 212         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013776241 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 213         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013827382 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 214         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013649503 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 215         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013763261 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 216         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01369831 |
| loss/dynamics_train_loss   | -51.7      |
| timestep                   | 217        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013696793 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 218         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013651984 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 219         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0136605995 |
| loss/dynamics_train_loss   | -51.7        |
| timestep                   | 220          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013729575 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 221         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013692263 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 222         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013558005 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 223         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013497191 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 224         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013674581 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 225         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01351133 |
| loss/dynamics_train_loss   | -51.8      |
| timestep                   | 226        |
----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0134547325 |
| loss/dynamics_train_loss   | -51.8        |
| timestep                   | 227          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01361012 |
| loss/dynamics_train_loss   | -51.8      |
| timestep                   | 228        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013494251 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 229         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013348597 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 230         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0133967325 |
| loss/dynamics_train_loss   | -51.9        |
| timestep                   | 231          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013373377 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 232         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013448675 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 233         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013395123 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 234         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013387566 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 235         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01337388 |
| loss/dynamics_train_loss   | -51.9      |
| timestep                   | 236        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013426743 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 237         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013263218 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 238         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013431901 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 239         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013214153 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 240         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013350075 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 241         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013357341 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 242         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013547908 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 243         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013099532 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 244         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013133054 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 245         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0131378295 |
| loss/dynamics_train_loss   | -52.1        |
| timestep                   | 246          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013093425 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 247         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013048105 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 248         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012930786 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 249         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013117902 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 250         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013239548 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 251         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013028775 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 252         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0128782 |
| loss/dynamics_train_loss   | -52.1     |
| timestep                   | 253       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012986481 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 254         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013000073 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 255         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012955984 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 256         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012889059 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 257         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01289537 |
| loss/dynamics_train_loss   | -52.2      |
| timestep                   | 258        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01282872 |
| loss/dynamics_train_loss   | -52.2      |
| timestep                   | 259        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012945034 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 260         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013099904 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 261         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013136449 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 262         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012778388 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 263         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012887299 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 264         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012901875 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 265         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013042035 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 266         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012800793 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 267         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013240458 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 268         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01274199 |
| loss/dynamics_train_loss   | -52.3      |
| timestep                   | 269        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012952966 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 270         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012821537 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 271         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012813446 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 272         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012890014 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 273         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012734799 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 274         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012788406 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 275         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012638638 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 276         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012505072 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 277         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0126032485 |
| loss/dynamics_train_loss   | -52.4        |
| timestep                   | 278          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012627629 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 279         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012595614 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 280         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012593615 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 281         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012579466 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 282         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0125976205 |
| loss/dynamics_train_loss   | -52.5        |
| timestep                   | 283          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012527777 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 284         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0125274155 |
| loss/dynamics_train_loss   | -52.5        |
| timestep                   | 285          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012550133 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 286         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0124331135 |
| loss/dynamics_train_loss   | -52.6        |
| timestep                   | 287          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012564832 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 288         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012543619 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 289         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0125305075 |
| loss/dynamics_train_loss   | -52.6        |
| timestep                   | 290          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012440928 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 291         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01270148 |
| loss/dynamics_train_loss   | -52.6      |
| timestep                   | 292        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012402335 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 293         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012442882 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 294         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012320153 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 295         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012314345 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 296         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012457845 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 297         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012416074 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 298         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01244312 |
| loss/dynamics_train_loss   | -52.7      |
| timestep                   | 299        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012301237 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 300         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012242518 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 301         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0122469915 |
| loss/dynamics_train_loss   | -52.7        |
| timestep                   | 302          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01216775 |
| loss/dynamics_train_loss   | -52.7      |
| timestep                   | 303        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012360685 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 304         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012219219 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 305         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012303009 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 306         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012227824 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 307         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012185201 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 308         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012184178 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 309         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012424374 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 310         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012107158 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 311         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01212234 |
| loss/dynamics_train_loss   | -52.8      |
| timestep                   | 312        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012125472 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 313         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012152577 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 314         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012026124 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 315         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0122206295 |
| loss/dynamics_train_loss   | -52.9        |
| timestep                   | 316          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011996085 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 317         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01198396 |
| loss/dynamics_train_loss   | -52.9      |
| timestep                   | 318        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012050517 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 319         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012051056 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 320         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011965558 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 321         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012072278 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 322         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011876406 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 323         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01195794 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 324        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012003136 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 325         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011956076 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 326         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011865224 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 327         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011781655 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 328         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011842974 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 329         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011843575 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 330         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012040299 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 331         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011867799 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 332         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011835137 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 333         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011849049 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 334         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0118181715 |
| loss/dynamics_train_loss   | -53.1        |
| timestep                   | 335          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01175988 |
| loss/dynamics_train_loss   | -53.1      |
| timestep                   | 336        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01168785 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 337        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011801679 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 338         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011776789 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 339         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011637482 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 340         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0115653975 |
| loss/dynamics_train_loss   | -53.1        |
| timestep                   | 341          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011614991 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 342         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011657948 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 343         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011680053 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 344         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011593375 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 345         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011529547 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 346         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011484377 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 347         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011580219 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 348         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011537448 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 349         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011866023 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 350         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011436717 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 351         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011514276 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 352         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011460658 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 353         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011692176 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 354         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011426257 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 355         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011493574 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 356         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011352714 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 357         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011341199 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 358         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011348003 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 359         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0112312995 |
| loss/dynamics_train_loss   | -53.3        |
| timestep                   | 360          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011356203 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 361         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011328412 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 362         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011409616 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 363         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011188741 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 364         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011248153 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 365         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011222707 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 366         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011334486 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 367         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011534334 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 368         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011251865 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 369         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011289319 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 370         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01111695 |
| loss/dynamics_train_loss   | -53.4      |
| timestep                   | 371        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010982022 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 372         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01107817 |
| loss/dynamics_train_loss   | -53.4      |
| timestep                   | 373        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011320982 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 374         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011126685 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 375         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011012173 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 376         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011221686 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 377         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011216847 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 378         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011004559 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 379         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011141626 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 380         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010996242 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 381         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011063276 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 382         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0110188555 |
| loss/dynamics_train_loss   | -53.5        |
| timestep                   | 383          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011089457 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 384         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0111013325 |
| loss/dynamics_train_loss   | -53.5        |
| timestep                   | 385          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011195296 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 386         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01142258 |
| loss/dynamics_train_loss   | -53.5      |
| timestep                   | 387        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011086032 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 388         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010990593 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 389         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011003485 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 390         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010918252 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 391         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010900679 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 392         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0109376125 |
| loss/dynamics_train_loss   | -53.6        |
| timestep                   | 393          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010894509 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 394         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011226693 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 395         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010863021 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 396         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0108396355 |
| loss/dynamics_train_loss   | -53.6        |
| timestep                   | 397          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010827241 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 398         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010933968 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 399         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010944201 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 400         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010881412 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 401         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0108774835 |
| loss/dynamics_train_loss   | -53.7        |
| timestep                   | 402          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010851677 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 403         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0110262595 |
| loss/dynamics_train_loss   | -53.7        |
| timestep                   | 404          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010789732 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 405         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010921195 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 406         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010892687 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 407         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01094958 |
| loss/dynamics_train_loss   | -53.7      |
| timestep                   | 408        |
----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0108178845 |
| loss/dynamics_train_loss   | -53.7        |
| timestep                   | 409          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010929511 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 410         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010784952 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 411         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010757833 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 412         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010831991 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 413         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01077481 |
| loss/dynamics_train_loss   | -53.8      |
| timestep                   | 414        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010639769 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 415         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010748697 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 416         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010722241 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 417         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010621664 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 418         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0106825605 |
| loss/dynamics_train_loss   | -53.8        |
| timestep                   | 419          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010751028 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 420         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010680076 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 421         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010810233 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 422         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010651102 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 423         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010689217 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 424         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0105553735 |
| loss/dynamics_train_loss   | -53.9        |
| timestep                   | 425          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010536103 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 426         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010563682 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 427         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010664491 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 428         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010616634 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 429         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01064191 |
| loss/dynamics_train_loss   | -53.9      |
| timestep                   | 430        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010538158 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 431         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010602181 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 432         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010565799 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 433         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01052787 |
| loss/dynamics_train_loss   | -53.9      |
| timestep                   | 434        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010596551 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 435         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010483145 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 436         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010534197 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 437         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010632746 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 438         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010497102 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 439         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0104886 |
| loss/dynamics_train_loss   | -53.9     |
| timestep                   | 440       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010400456 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 441         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010536147 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 442         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010750597 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 443         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010521256 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 444         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010671385 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 445         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010514991 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 446         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010565569 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 447         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010389606 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 448         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010834309 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 449         |
-----------------------------------------------------------------------------
elites:[0, 3, 1, 4, 2] , holdout loss: 0.010310318320989609
num rollout transitions: 250000, reward mean: 4.7980
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 4.8707
num rollout transitions: 250000, reward mean: 4.8977
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.43e-10 |
| adv_dynamics_update/adv_log_prob   | 28.6     |
| adv_dynamics_update/adv_loss       | 2.12     |
| adv_dynamics_update/all_loss       | -53      |
| adv_dynamics_update/sl_loss        | -53      |
| alpha                              | 0.998    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.05     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | 5.17     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 27.8     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9588
num rollout transitions: 250000, reward mean: 4.9619
num rollout transitions: 250000, reward mean: 4.9481
num rollout transitions: 250000, reward mean: 4.9493
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-09 |
| adv_dynamics_update/adv_log_prob   | 28.3     |
| adv_dynamics_update/adv_loss       | 3.84     |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.94     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.15     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -39.7    |
| loss/alpha                         | -0.569   |
| loss/critic1                       | 2.95     |
| loss/critic2                       | 2.91     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9390
num rollout transitions: 250000, reward mean: 4.9306
num rollout transitions: 250000, reward mean: 4.9227
num rollout transitions: 250000, reward mean: 4.9231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 29       |
| adv_dynamics_update/adv_loss       | 2.71     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.858    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.15     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -68.7    |
| loss/alpha                         | -1.39    |
| loss/critic1                       | 4.1      |
| loss/critic2                       | 4.14     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9186
num rollout transitions: 250000, reward mean: 4.9143
num rollout transitions: 250000, reward mean: 4.8972
num rollout transitions: 250000, reward mean: 4.9050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-11 |
| adv_dynamics_update/adv_log_prob   | 32.6     |
| adv_dynamics_update/adv_loss       | 1.89     |
| adv_dynamics_update/all_loss       | -53.2    |
| adv_dynamics_update/sl_loss        | -53.2    |
| alpha                              | 0.78     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.21     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -92.8    |
| loss/alpha                         | -2.13    |
| loss/critic1                       | 6.61     |
| loss/critic2                       | 6.75     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 4000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8841
num rollout transitions: 250000, reward mean: 4.8738
num rollout transitions: 250000, reward mean: 4.8522
num rollout transitions: 250000, reward mean: 4.8388
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.55e-09 |
| adv_dynamics_update/adv_log_prob   | 36.2      |
| adv_dynamics_update/adv_loss       | 1.91      |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.71      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 2.37      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -112      |
| loss/alpha                         | -2.78     |
| loss/critic1                       | 9.24      |
| loss/critic2                       | 9.48      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.86      |
| timestep                           | 5000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8413
num rollout transitions: 250000, reward mean: 4.8301
num rollout transitions: 250000, reward mean: 4.8264
num rollout transitions: 250000, reward mean: 4.8315
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-09 |
| adv_dynamics_update/adv_log_prob   | 38.3      |
| adv_dynamics_update/adv_loss       | 2.35      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.647     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 1.29      |
| eval/normalized_episode_reward_std | 3.44      |
| loss/actor                         | -128      |
| loss/alpha                         | -3.18     |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.83      |
| timestep                           | 6000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8058
num rollout transitions: 250000, reward mean: 4.8096
num rollout transitions: 250000, reward mean: 4.8252
num rollout transitions: 250000, reward mean: 4.8297
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 39.4     |
| adv_dynamics_update/adv_loss       | 1.55     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.591    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 0.525    |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -143     |
| loss/alpha                         | -3.41    |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8335
num rollout transitions: 250000, reward mean: 4.8171
num rollout transitions: 250000, reward mean: 4.8070
num rollout transitions: 250000, reward mean: 4.8314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 40.5      |
| adv_dynamics_update/adv_loss       | 0.941     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.541     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 1.78      |
| eval/normalized_episode_reward_std | 4.73      |
| loss/actor                         | -155      |
| loss/alpha                         | -3.72     |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.82      |
| timestep                           | 8000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8564
num rollout transitions: 250000, reward mean: 4.8355
num rollout transitions: 250000, reward mean: 4.8606
num rollout transitions: 250000, reward mean: 4.8575
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.21e-10 |
| adv_dynamics_update/adv_log_prob   | 41.3      |
| adv_dynamics_update/adv_loss       | 0.956     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.493     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 0.857     |
| eval/normalized_episode_reward_std | 3.47      |
| loss/actor                         | -165      |
| loss/alpha                         | -4.02     |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.85      |
| timestep                           | 9000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8396
num rollout transitions: 250000, reward mean: 4.8508
num rollout transitions: 250000, reward mean: 4.8567
num rollout transitions: 250000, reward mean: 4.8794
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.24e-11 |
| adv_dynamics_update/adv_log_prob   | 42       |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.449    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 6.17     |
| eval/normalized_episode_reward_std | 9.28     |
| loss/actor                         | -174     |
| loss/alpha                         | -4.24    |
| loss/critic1                       | 17       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 10000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8738
num rollout transitions: 250000, reward mean: 4.8944
num rollout transitions: 250000, reward mean: 4.8634
num rollout transitions: 250000, reward mean: 4.8765
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.958     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.41      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 18        |
| eval/normalized_episode_reward_std | 14.7      |
| loss/actor                         | -182      |
| loss/alpha                         | -4.42     |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 11000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8786
num rollout transitions: 250000, reward mean: 4.9004
num rollout transitions: 250000, reward mean: 4.9006
num rollout transitions: 250000, reward mean: 4.8812
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.746    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.374    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27.2     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -191     |
| loss/alpha                         | -4.43    |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 12000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8756
num rollout transitions: 250000, reward mean: 4.8841
num rollout transitions: 250000, reward mean: 4.8845
num rollout transitions: 250000, reward mean: 4.8939
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.762    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.341    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.9     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -199     |
| loss/alpha                         | -4.35    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 13000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8784
num rollout transitions: 250000, reward mean: 4.8829
num rollout transitions: 250000, reward mean: 4.8714
num rollout transitions: 250000, reward mean: 4.8646
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 1.08     |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.313    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.7     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -207     |
| loss/alpha                         | -3.99    |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.87     |
| timestep                           | 14000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8566
num rollout transitions: 250000, reward mean: 4.8827
num rollout transitions: 250000, reward mean: 4.8642
num rollout transitions: 250000, reward mean: 4.8743
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.74      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.287     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 45.9      |
| eval/normalized_episode_reward_std | 10.4      |
| loss/actor                         | -216      |
| loss/alpha                         | -3.7      |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 15000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8658
num rollout transitions: 250000, reward mean: 4.8719
num rollout transitions: 250000, reward mean: 4.8794
num rollout transitions: 250000, reward mean: 4.8672
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.96e-11 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 1.4       |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.264     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 43.3      |
| eval/normalized_episode_reward_std | 15.6      |
| loss/actor                         | -225      |
| loss/alpha                         | -3.35     |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 16000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8842
num rollout transitions: 250000, reward mean: 4.8784
num rollout transitions: 250000, reward mean: 4.8732
num rollout transitions: 250000, reward mean: 4.9105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-09 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.398     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.243     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 45.1      |
| eval/normalized_episode_reward_std | 16.7      |
| loss/actor                         | -233      |
| loss/alpha                         | -3.02     |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 17000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8798
num rollout transitions: 250000, reward mean: 4.8955
num rollout transitions: 250000, reward mean: 4.8786
num rollout transitions: 250000, reward mean: 4.8736
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.224     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 38.5      |
| eval/normalized_episode_reward_std | 21.4      |
| loss/actor                         | -242      |
| loss/alpha                         | -2.61     |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 18000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8873
num rollout transitions: 250000, reward mean: 4.8858
num rollout transitions: 250000, reward mean: 4.8886
num rollout transitions: 250000, reward mean: 4.9000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.0183   |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.208     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 47.6      |
| eval/normalized_episode_reward_std | 18        |
| loss/actor                         | -251      |
| loss/alpha                         | -1.92     |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 19000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8686
num rollout transitions: 250000, reward mean: 4.9036
num rollout transitions: 250000, reward mean: 4.8865
num rollout transitions: 250000, reward mean: 4.8909
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-09 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.721    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.195    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.7     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -260     |
| loss/alpha                         | -1.4     |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 20000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8715
num rollout transitions: 250000, reward mean: 4.8825
num rollout transitions: 250000, reward mean: 4.8744
num rollout transitions: 250000, reward mean: 4.8959
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.09e-09 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.49      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.184     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 49        |
| eval/normalized_episode_reward_std | 11.2      |
| loss/actor                         | -269      |
| loss/alpha                         | -0.818    |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 21000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8936
num rollout transitions: 250000, reward mean: 4.9134
num rollout transitions: 250000, reward mean: 4.8821
num rollout transitions: 250000, reward mean: 4.8842
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.15e-09 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.553     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 34.3      |
| eval/normalized_episode_reward_std | 22.1      |
| loss/actor                         | -279      |
| loss/alpha                         | -0.245    |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 22000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8779
num rollout transitions: 250000, reward mean: 4.8829
num rollout transitions: 250000, reward mean: 4.9050
num rollout transitions: 250000, reward mean: 4.8941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.222   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 34.8     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -288     |
| loss/alpha                         | 0.0728   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 23000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8956
num rollout transitions: 250000, reward mean: 4.8747
num rollout transitions: 250000, reward mean: 4.8955
num rollout transitions: 250000, reward mean: 4.9073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.14e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.284     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 36.4      |
| eval/normalized_episode_reward_std | 21.9      |
| loss/actor                         | -297      |
| loss/alpha                         | 0.172     |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 20.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 24000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8807
num rollout transitions: 250000, reward mean: 4.8821
num rollout transitions: 250000, reward mean: 4.9061
num rollout transitions: 250000, reward mean: 4.8948
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.03      |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.186     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 48.9      |
| eval/normalized_episode_reward_std | 16.3      |
| loss/actor                         | -305      |
| loss/alpha                         | 0.132     |
| loss/critic1                       | 20.5      |
| loss/critic2                       | 20.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 25000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9054
num rollout transitions: 250000, reward mean: 4.8791
num rollout transitions: 250000, reward mean: 4.8975
num rollout transitions: 250000, reward mean: 4.9011
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -1.05    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42.7     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -314     |
| loss/alpha                         | 0.0524   |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 26000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9219
num rollout transitions: 250000, reward mean: 4.8947
num rollout transitions: 250000, reward mean: 4.8994
num rollout transitions: 250000, reward mean: 4.8914
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.94e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.986    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.19     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 35.3     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -322     |
| loss/alpha                         | 0.126    |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8940
num rollout transitions: 250000, reward mean: 4.8906
num rollout transitions: 250000, reward mean: 4.9054
num rollout transitions: 250000, reward mean: 4.8948
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.295    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.196    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 43.7     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -329     |
| loss/alpha                         | 0.0679   |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 22.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 28000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9059
num rollout transitions: 250000, reward mean: 4.8942
num rollout transitions: 250000, reward mean: 4.8834
num rollout transitions: 250000, reward mean: 4.8873
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.06e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.185    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.198    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.3     |
| eval/normalized_episode_reward_std | 13.4     |
| loss/actor                         | -336     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 22.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 29000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9055
num rollout transitions: 250000, reward mean: 4.9033
num rollout transitions: 250000, reward mean: 4.9132
num rollout transitions: 250000, reward mean: 4.9096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.269    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.199    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.1     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -343     |
| loss/alpha                         | 0.0122   |
| loss/critic1                       | 22.6     |
| loss/critic2                       | 22.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 30000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8933
num rollout transitions: 250000, reward mean: 4.9176
num rollout transitions: 250000, reward mean: 4.9128
num rollout transitions: 250000, reward mean: 4.8908
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.123     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.2       |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 50.4      |
| eval/normalized_episode_reward_std | 15.7      |
| loss/actor                         | -350      |
| loss/alpha                         | 0.0518    |
| loss/critic1                       | 22.7      |
| loss/critic2                       | 22.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 31000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8832
num rollout transitions: 250000, reward mean: 4.9197
num rollout transitions: 250000, reward mean: 4.9240
num rollout transitions: 250000, reward mean: 4.9057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.665     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.201     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 39.5      |
| eval/normalized_episode_reward_std | 21.9      |
| loss/actor                         | -356      |
| loss/alpha                         | -0.0345   |
| loss/critic1                       | 22.9      |
| loss/critic2                       | 22.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 32000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9071
num rollout transitions: 250000, reward mean: 4.9088
num rollout transitions: 250000, reward mean: 4.9172
num rollout transitions: 250000, reward mean: 4.9096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.846    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.202    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 40.5     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -363     |
| loss/alpha                         | 0.0746   |
| loss/critic1                       | 22.4     |
| loss/critic2                       | 22.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 33000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9001
num rollout transitions: 250000, reward mean: 4.9058
num rollout transitions: 250000, reward mean: 4.9137
num rollout transitions: 250000, reward mean: 4.8818
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.65e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.205     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 33.2      |
| eval/normalized_episode_reward_std | 20        |
| loss/actor                         | -368      |
| loss/alpha                         | 0.0818    |
| loss/critic1                       | 23.1      |
| loss/critic2                       | 22.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 34000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8935
num rollout transitions: 250000, reward mean: 4.9010
num rollout transitions: 250000, reward mean: 4.8991
num rollout transitions: 250000, reward mean: 4.9007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.359    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.206    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 47.5     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -374     |
| loss/alpha                         | -0.0518  |
| loss/critic1                       | 23       |
| loss/critic2                       | 22.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 35000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8898
num rollout transitions: 250000, reward mean: 4.8995
num rollout transitions: 250000, reward mean: 4.8948
num rollout transitions: 250000, reward mean: 4.9084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 1.19     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.206    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 27.5     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -380     |
| loss/alpha                         | 0.0592   |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 36000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8913
num rollout transitions: 250000, reward mean: 4.8965
num rollout transitions: 250000, reward mean: 4.8973
num rollout transitions: 250000, reward mean: 4.8954
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-09 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.503     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.209     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 42.6      |
| eval/normalized_episode_reward_std | 24.4      |
| loss/actor                         | -385      |
| loss/alpha                         | 0.0325    |
| loss/critic1                       | 22.2      |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 37000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9009
num rollout transitions: 250000, reward mean: 4.9088
num rollout transitions: 250000, reward mean: 4.9087
num rollout transitions: 250000, reward mean: 4.9162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.0731   |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.208     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 36.7      |
| eval/normalized_episode_reward_std | 21.2      |
| loss/actor                         | -390      |
| loss/alpha                         | -0.0551   |
| loss/critic1                       | 22.2      |
| loss/critic2                       | 22.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 38000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8938
num rollout transitions: 250000, reward mean: 4.9104
num rollout transitions: 250000, reward mean: 4.8998
num rollout transitions: 250000, reward mean: 4.9095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.284    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.204    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 44.8     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -395     |
| loss/alpha                         | -0.0914  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 39000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9001
num rollout transitions: 250000, reward mean: 4.8970
num rollout transitions: 250000, reward mean: 4.8994
num rollout transitions: 250000, reward mean: 4.9059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.201    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.3     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -400     |
| loss/alpha                         | -0.0383  |
| loss/critic1                       | 21.2     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 40000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8947
num rollout transitions: 250000, reward mean: 4.9074
num rollout transitions: 250000, reward mean: 4.9132
num rollout transitions: 250000, reward mean: 4.9153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-09 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.00451 |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.202    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56       |
| eval/normalized_episode_reward_std | 7.62     |
| loss/actor                         | -405     |
| loss/alpha                         | 0.0643   |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 20.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 41000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9119
num rollout transitions: 250000, reward mean: 4.9158
num rollout transitions: 250000, reward mean: 4.9269
num rollout transitions: 250000, reward mean: 4.8893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.649    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.204    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -409     |
| loss/alpha                         | 0.0456   |
| loss/critic1                       | 22       |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 42000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8962
num rollout transitions: 250000, reward mean: 4.9115
num rollout transitions: 250000, reward mean: 4.9250
num rollout transitions: 250000, reward mean: 4.9201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.478    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.202    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.8     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -414     |
| loss/alpha                         | -0.112   |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 43000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9268
num rollout transitions: 250000, reward mean: 4.9233
num rollout transitions: 250000, reward mean: 4.8998
num rollout transitions: 250000, reward mean: 4.9235
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.08e-09 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | -0.556   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.201    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.7     |
| eval/normalized_episode_reward_std | 5.94     |
| loss/actor                         | -418     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 23       |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 44000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9123
num rollout transitions: 250000, reward mean: 4.9136
num rollout transitions: 250000, reward mean: 4.9114
num rollout transitions: 250000, reward mean: 4.9121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-09 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.108    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.197     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59        |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -422      |
| loss/alpha                         | -0.0968   |
| loss/critic1                       | 21.7      |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 45000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9260
num rollout transitions: 250000, reward mean: 4.9186
num rollout transitions: 250000, reward mean: 4.9209
num rollout transitions: 250000, reward mean: 4.9062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.09e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.84    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.196    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.2     |
| eval/normalized_episode_reward_std | 8.67     |
| loss/actor                         | -427     |
| loss/alpha                         | 0.0645   |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 46000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9123
num rollout transitions: 250000, reward mean: 4.9254
num rollout transitions: 250000, reward mean: 4.9016
num rollout transitions: 250000, reward mean: 4.9092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.772    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.198     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.2      |
| eval/normalized_episode_reward_std | 7.7       |
| loss/actor                         | -431      |
| loss/alpha                         | -0.0011   |
| loss/critic1                       | 21.7      |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 47000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9226
num rollout transitions: 250000, reward mean: 4.9220
num rollout transitions: 250000, reward mean: 4.9252
num rollout transitions: 250000, reward mean: 4.9095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.1e-11 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.497    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.2      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.2     |
| eval/normalized_episode_reward_std | 11.8     |
| loss/actor                         | -434     |
| loss/alpha                         | 0.0465   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 48000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9086
num rollout transitions: 250000, reward mean: 4.9250
num rollout transitions: 250000, reward mean: 4.9141
num rollout transitions: 250000, reward mean: 4.9040
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.85e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.223     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.199     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.1      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -438      |
| loss/alpha                         | -0.0478   |
| loss/critic1                       | 21.5      |
| loss/critic2                       | 21.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 49000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9091
num rollout transitions: 250000, reward mean: 4.9146
num rollout transitions: 250000, reward mean: 4.8978
num rollout transitions: 250000, reward mean: 4.9176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.935   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.197    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.8     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -442     |
| loss/alpha                         | -0.0109  |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 50000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9130
num rollout transitions: 250000, reward mean: 4.9075
num rollout transitions: 250000, reward mean: 4.9085
num rollout transitions: 250000, reward mean: 4.9256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.31e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.694    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.198     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -445      |
| loss/alpha                         | -0.0396   |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 51000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9203
num rollout transitions: 250000, reward mean: 4.9184
num rollout transitions: 250000, reward mean: 4.9203
num rollout transitions: 250000, reward mean: 4.9188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.08e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.904    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.195    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.7     |
| eval/normalized_episode_reward_std | 2.49     |
| loss/actor                         | -449     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 52000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9118
num rollout transitions: 250000, reward mean: 4.9249
num rollout transitions: 250000, reward mean: 4.9154
num rollout transitions: 250000, reward mean: 4.9118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.59e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.646    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.194    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.7     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -452     |
| loss/alpha                         | -0.0406  |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9250
num rollout transitions: 250000, reward mean: 4.9326
num rollout transitions: 250000, reward mean: 4.9015
num rollout transitions: 250000, reward mean: 4.9161
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.55e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.391   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.193    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -455     |
| loss/alpha                         | 0.0495   |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9123
num rollout transitions: 250000, reward mean: 4.9222
num rollout transitions: 250000, reward mean: 4.9098
num rollout transitions: 250000, reward mean: 4.8979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.338    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.195    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -459     |
| loss/alpha                         | -0.0273  |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 55000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9142
num rollout transitions: 250000, reward mean: 4.9138
num rollout transitions: 250000, reward mean: 4.9278
num rollout transitions: 250000, reward mean: 4.9045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.75e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.368   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.195    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 2.52     |
| loss/actor                         | -462     |
| loss/alpha                         | 0.00751  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 56000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9206
num rollout transitions: 250000, reward mean: 4.9158
num rollout transitions: 250000, reward mean: 4.9413
num rollout transitions: 250000, reward mean: 4.9338
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.31e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.741   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.193    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -465     |
| loss/alpha                         | -0.0971  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 57000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9254
num rollout transitions: 250000, reward mean: 4.9153
num rollout transitions: 250000, reward mean: 4.9367
num rollout transitions: 250000, reward mean: 4.9264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.87e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.432    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.19      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.6      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -468      |
| loss/alpha                         | -0.00384  |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 19.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 58000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9364
num rollout transitions: 250000, reward mean: 4.9461
num rollout transitions: 250000, reward mean: 4.9291
num rollout transitions: 250000, reward mean: 4.9296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.12e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.611    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.188     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.5      |
| eval/normalized_episode_reward_std | 11        |
| loss/actor                         | -471      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 59000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9371
num rollout transitions: 250000, reward mean: 4.9378
num rollout transitions: 250000, reward mean: 4.9374
num rollout transitions: 250000, reward mean: 4.9462
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.03e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.504    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -474     |
| loss/alpha                         | 0.0317   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 60000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9240
num rollout transitions: 250000, reward mean: 4.9320
num rollout transitions: 250000, reward mean: 4.9259
num rollout transitions: 250000, reward mean: 4.9303
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.22e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.304    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.188     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.3      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -477      |
| loss/alpha                         | 0.0179    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 61000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9414
num rollout transitions: 250000, reward mean: 4.9245
num rollout transitions: 250000, reward mean: 4.9302
num rollout transitions: 250000, reward mean: 4.9436
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.389   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.186    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.8     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -480     |
| loss/alpha                         | -0.135   |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 62000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9447
num rollout transitions: 250000, reward mean: 4.9330
num rollout transitions: 250000, reward mean: 4.9359
num rollout transitions: 250000, reward mean: 4.9475
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.545     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.7      |
| eval/normalized_episode_reward_std | 2.44      |
| loss/actor                         | -483      |
| loss/alpha                         | 0.0274    |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 63000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9454
num rollout transitions: 250000, reward mean: 4.9239
num rollout transitions: 250000, reward mean: 4.9426
num rollout transitions: 250000, reward mean: 4.9454
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.251   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.184    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -486     |
| loss/alpha                         | -0.0387  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 64000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9366
num rollout transitions: 250000, reward mean: 4.9228
num rollout transitions: 250000, reward mean: 4.9298
num rollout transitions: 250000, reward mean: 4.9342
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.0846    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.5      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -488      |
| loss/alpha                         | -0.0227   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 65000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9385
num rollout transitions: 250000, reward mean: 4.9336
num rollout transitions: 250000, reward mean: 4.9343
num rollout transitions: 250000, reward mean: 4.9236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.88e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.184     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -491      |
| loss/alpha                         | 0.113     |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 66000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9300
num rollout transitions: 250000, reward mean: 4.9154
num rollout transitions: 250000, reward mean: 4.9400
num rollout transitions: 250000, reward mean: 4.9256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.205     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.186     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.9      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -494      |
| loss/alpha                         | 0.0276    |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 67000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9308
num rollout transitions: 250000, reward mean: 4.9319
num rollout transitions: 250000, reward mean: 4.9325
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.93e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.967    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -496     |
| loss/alpha                         | -0.0544  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9437
num rollout transitions: 250000, reward mean: 4.9401
num rollout transitions: 250000, reward mean: 4.9393
num rollout transitions: 250000, reward mean: 4.9350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.52      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.184     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.6      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -498      |
| loss/alpha                         | -0.0694   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 69000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9543
num rollout transitions: 250000, reward mean: 4.9391
num rollout transitions: 250000, reward mean: 4.9265
num rollout transitions: 250000, reward mean: 4.9381
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 1.23     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -501     |
| loss/alpha                         | -0.0656  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 70000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9356
num rollout transitions: 250000, reward mean: 4.9251
num rollout transitions: 250000, reward mean: 4.9394
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-09 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.664    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -503     |
| loss/alpha                         | -0.0342  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9419
num rollout transitions: 250000, reward mean: 4.9263
num rollout transitions: 250000, reward mean: 4.9466
num rollout transitions: 250000, reward mean: 4.9338
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.142    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.3      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -506      |
| loss/alpha                         | -0.0305   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 72000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9484
num rollout transitions: 250000, reward mean: 4.9586
num rollout transitions: 250000, reward mean: 4.9499
num rollout transitions: 250000, reward mean: 4.9525
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.54e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.22     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.6     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -508     |
| loss/alpha                         | 0.0178   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 73000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9433
num rollout transitions: 250000, reward mean: 4.9561
num rollout transitions: 250000, reward mean: 4.9438
num rollout transitions: 250000, reward mean: 4.9281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.725   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.7     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -510     |
| loss/alpha                         | -0.135   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9439
num rollout transitions: 250000, reward mean: 4.9551
num rollout transitions: 250000, reward mean: 4.9505
num rollout transitions: 250000, reward mean: 4.9626
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.536    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -512     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 75000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9410
num rollout transitions: 250000, reward mean: 4.9406
num rollout transitions: 250000, reward mean: 4.9481
num rollout transitions: 250000, reward mean: 4.9502
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.323   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -515     |
| loss/alpha                         | 0.0643   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 76000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9447
num rollout transitions: 250000, reward mean: 4.9438
num rollout transitions: 250000, reward mean: 4.9283
num rollout transitions: 250000, reward mean: 4.9437
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.483    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -517     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 77000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9371
num rollout transitions: 250000, reward mean: 4.9395
num rollout transitions: 250000, reward mean: 4.9374
num rollout transitions: 250000, reward mean: 4.9493
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.264    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 2.48     |
| loss/actor                         | -519     |
| loss/alpha                         | -0.0442  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 78000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9482
num rollout transitions: 250000, reward mean: 4.9457
num rollout transitions: 250000, reward mean: 4.9454
num rollout transitions: 250000, reward mean: 4.9454
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.0413    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.7      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -521      |
| loss/alpha                         | -0.0605   |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 79000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9433
num rollout transitions: 250000, reward mean: 4.9479
num rollout transitions: 250000, reward mean: 4.9455
num rollout transitions: 250000, reward mean: 4.9623
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.982    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -523      |
| loss/alpha                         | -0.05     |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 80000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9513
num rollout transitions: 250000, reward mean: 4.9515
num rollout transitions: 250000, reward mean: 4.9418
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.0343  |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -525     |
| loss/alpha                         | -0.0335  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 81000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9463
num rollout transitions: 250000, reward mean: 4.9526
num rollout transitions: 250000, reward mean: 4.9687
num rollout transitions: 250000, reward mean: 4.9470
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 1.26     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -527     |
| loss/alpha                         | -0.0654  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 82000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9485
num rollout transitions: 250000, reward mean: 4.9472
num rollout transitions: 250000, reward mean: 4.9496
num rollout transitions: 250000, reward mean: 4.9526
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.321    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -529     |
| loss/alpha                         | 0.0607   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 83000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9475
num rollout transitions: 250000, reward mean: 4.9586
num rollout transitions: 250000, reward mean: 4.9432
num rollout transitions: 250000, reward mean: 4.9549
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-09  |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.0694  |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -531     |
| loss/alpha                         | 0.0756   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 84000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9434
num rollout transitions: 250000, reward mean: 4.9537
num rollout transitions: 250000, reward mean: 4.9569
num rollout transitions: 250000, reward mean: 4.9576
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.6e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.699    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -533     |
| loss/alpha                         | 0.00752  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 85000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9573
num rollout transitions: 250000, reward mean: 4.9544
num rollout transitions: 250000, reward mean: 4.9546
num rollout transitions: 250000, reward mean: 4.9455
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-09 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -1.04     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.6      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -535      |
| loss/alpha                         | -0.0719   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 86000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9518
num rollout transitions: 250000, reward mean: 4.9525
num rollout transitions: 250000, reward mean: 4.9634
num rollout transitions: 250000, reward mean: 4.9592
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.622    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 2.47     |
| loss/actor                         | -537     |
| loss/alpha                         | -0.00973 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 87000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9461
num rollout transitions: 250000, reward mean: 4.9582
num rollout transitions: 250000, reward mean: 4.9483
num rollout transitions: 250000, reward mean: 4.9555
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 1.41      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.4      |
| eval/normalized_episode_reward_std | 2.5       |
| loss/actor                         | -539      |
| loss/alpha                         | -0.0654   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 88000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
num rollout transitions: 250000, reward mean: 4.9489
num rollout transitions: 250000, reward mean: 4.9457
num rollout transitions: 250000, reward mean: 4.9613
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.29e-09 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.177    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.1      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -540      |
| loss/alpha                         | -0.0439   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 89000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9614
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9520
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.52e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.777     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.2      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -542      |
| loss/alpha                         | -0.00561  |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 90000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9488
num rollout transitions: 250000, reward mean: 4.9628
num rollout transitions: 250000, reward mean: 4.9645
num rollout transitions: 250000, reward mean: 4.9666
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.32e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.569     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63        |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -544      |
| loss/alpha                         | 0.0486    |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 91000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9566
num rollout transitions: 250000, reward mean: 4.9656
num rollout transitions: 250000, reward mean: 4.9621
num rollout transitions: 250000, reward mean: 4.9578
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.47e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.191    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -546     |
| loss/alpha                         | 0.0262   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 92000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9492
num rollout transitions: 250000, reward mean: 4.9600
num rollout transitions: 250000, reward mean: 4.9656
num rollout transitions: 250000, reward mean: 4.9704
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.244   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -548     |
| loss/alpha                         | -0.0938  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 93000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9627
num rollout transitions: 250000, reward mean: 4.9642
num rollout transitions: 250000, reward mean: 4.9464
num rollout transitions: 250000, reward mean: 4.9673
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 1.57     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 2.47     |
| loss/actor                         | -549     |
| loss/alpha                         | 0.0591   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 94000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9457
num rollout transitions: 250000, reward mean: 4.9440
num rollout transitions: 250000, reward mean: 4.9570
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.04e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.365     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 2.48      |
| loss/actor                         | -551      |
| loss/alpha                         | 0.0727    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 95000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9484
num rollout transitions: 250000, reward mean: 4.9571
num rollout transitions: 250000, reward mean: 4.9577
num rollout transitions: 250000, reward mean: 4.9633
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.03e-11 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.259    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -553     |
| loss/alpha                         | -0.0837  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9668
num rollout transitions: 250000, reward mean: 4.9417
num rollout transitions: 250000, reward mean: 4.9499
num rollout transitions: 250000, reward mean: 4.9608
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-09 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.362     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.1      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -554      |
| loss/alpha                         | -0.0772   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 97000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9624
num rollout transitions: 250000, reward mean: 4.9672
num rollout transitions: 250000, reward mean: 4.9682
num rollout transitions: 250000, reward mean: 4.9556
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10  |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.353     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.2      |
| eval/normalized_episode_reward_std | 2.51      |
| loss/actor                         | -555      |
| loss/alpha                         | -0.000288 |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 98000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9588
num rollout transitions: 250000, reward mean: 4.9547
num rollout transitions: 250000, reward mean: 4.9578
num rollout transitions: 250000, reward mean: 4.9573
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.457    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -557      |
| loss/alpha                         | 0.0197    |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 99000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9535
num rollout transitions: 250000, reward mean: 4.9612
num rollout transitions: 250000, reward mean: 4.9513
num rollout transitions: 250000, reward mean: 4.9649
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.0693  |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -558     |
| loss/alpha                         | 0.032    |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 100000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9500
num rollout transitions: 250000, reward mean: 4.9604
num rollout transitions: 250000, reward mean: 4.9506
num rollout transitions: 250000, reward mean: 4.9538
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.53e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.774    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -560      |
| loss/alpha                         | -0.066    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 101000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9664
num rollout transitions: 250000, reward mean: 4.9615
num rollout transitions: 250000, reward mean: 4.9724
num rollout transitions: 250000, reward mean: 4.9726
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.08e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.212    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.9      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -561      |
| loss/alpha                         | -0.049    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 102000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9639
num rollout transitions: 250000, reward mean: 4.9686
num rollout transitions: 250000, reward mean: 4.9656
num rollout transitions: 250000, reward mean: 4.9510
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.1      |
| eval/normalized_episode_reward_std | 2.54      |
| loss/actor                         | -562      |
| loss/alpha                         | 0.11      |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 103000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9574
num rollout transitions: 250000, reward mean: 4.9432
num rollout transitions: 250000, reward mean: 4.9624
num rollout transitions: 250000, reward mean: 4.9513
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.332    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.4      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -564      |
| loss/alpha                         | 0.13      |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 104000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9688
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9671
num rollout transitions: 250000, reward mean: 4.9584
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.242     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -565      |
| loss/alpha                         | -0.0175   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 105000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9541
num rollout transitions: 250000, reward mean: 4.9648
num rollout transitions: 250000, reward mean: 4.9469
num rollout transitions: 250000, reward mean: 4.9490
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.74e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -566     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9720
num rollout transitions: 250000, reward mean: 4.9660
num rollout transitions: 250000, reward mean: 4.9659
num rollout transitions: 250000, reward mean: 4.9561
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.819     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.5      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -568      |
| loss/alpha                         | -0.0641   |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 107000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9565
num rollout transitions: 250000, reward mean: 4.9622
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9540
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.63e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.986    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.2      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -569      |
| loss/alpha                         | -0.00193  |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 108000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9682
num rollout transitions: 250000, reward mean: 4.9561
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9642
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.223    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -570     |
| loss/alpha                         | -0.0746  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 109000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9664
num rollout transitions: 250000, reward mean: 4.9585
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.8e-09 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.551   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.49     |
| loss/actor                         | -571     |
| loss/alpha                         | -0.011   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 110000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9567
num rollout transitions: 250000, reward mean: 4.9629
num rollout transitions: 250000, reward mean: 4.9584
num rollout transitions: 250000, reward mean: 4.9534
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.147    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -572     |
| loss/alpha                         | -0.0482  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9496
num rollout transitions: 250000, reward mean: 4.9598
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9716
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.277     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -573      |
| loss/alpha                         | 0.0162    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 112000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9746
num rollout transitions: 250000, reward mean: 4.9645
num rollout transitions: 250000, reward mean: 4.9737
num rollout transitions: 250000, reward mean: 4.9576
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.0171   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -574     |
| loss/alpha                         | 0.021    |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 113000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9562
num rollout transitions: 250000, reward mean: 4.9694
num rollout transitions: 250000, reward mean: 4.9566
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.991    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -576      |
| loss/alpha                         | -0.0421   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 114000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
num rollout transitions: 250000, reward mean: 4.9709
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9750
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.546    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.8     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -577     |
| loss/alpha                         | 0.044    |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 115000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9551
num rollout transitions: 250000, reward mean: 4.9697
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.95e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.139    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -578     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 116000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9665
num rollout transitions: 250000, reward mean: 4.9575
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 4.9669
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.155   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -579     |
| loss/alpha                         | 0.0087   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 117000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9587
num rollout transitions: 250000, reward mean: 4.9525
num rollout transitions: 250000, reward mean: 4.9651
num rollout transitions: 250000, reward mean: 4.9647
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.258     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.4      |
| eval/normalized_episode_reward_std | 2.49      |
| loss/actor                         | -580      |
| loss/alpha                         | -0.0349   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 118000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9696
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.651   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -581     |
| loss/alpha                         | -0.14    |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 119000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9629
num rollout transitions: 250000, reward mean: 4.9607
num rollout transitions: 250000, reward mean: 4.9844
num rollout transitions: 250000, reward mean: 4.9680
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 1.59      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.7      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -582      |
| loss/alpha                         | 0.0954    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 120000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9790
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.459     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -584      |
| loss/alpha                         | 0.0375    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 121000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9678
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9482
num rollout transitions: 250000, reward mean: 4.9633
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.472    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -585     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 122000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9709
num rollout transitions: 250000, reward mean: 4.9721
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9776
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.69e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.23    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -586     |
| loss/alpha                         | -0.0566  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 123000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9824
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9733
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.0849    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -587      |
| loss/alpha                         | 0.0033    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 124000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
num rollout transitions: 250000, reward mean: 4.9587
num rollout transitions: 250000, reward mean: 4.9734
num rollout transitions: 250000, reward mean: 4.9722
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.0138   |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -588      |
| loss/alpha                         | 0.0826    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 125000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9669
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9772
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.312   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 2.44     |
| loss/actor                         | -590     |
| loss/alpha                         | -0.157   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 126000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
num rollout transitions: 250000, reward mean: 4.9689
num rollout transitions: 250000, reward mean: 4.9675
num rollout transitions: 250000, reward mean: 4.9738
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.34e-09 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.0972    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.9      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -591      |
| loss/alpha                         | -0.0232   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 127000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9696
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -592     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 128000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9677
num rollout transitions: 250000, reward mean: 4.9714
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.693    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -593     |
| loss/alpha                         | 0.0358   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 129000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9830
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -594     |
| loss/alpha                         | -0.0411  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9730
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9755
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.52e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.469    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -595     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9768
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9744
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 1.52     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 2.48     |
| loss/actor                         | -596     |
| loss/alpha                         | -0.0985  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 132000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9642
num rollout transitions: 250000, reward mean: 4.9677
num rollout transitions: 250000, reward mean: 4.9693
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.0934    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.2      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -597      |
| loss/alpha                         | 0.114     |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 133000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.987     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.1      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -598      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 134000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9637
num rollout transitions: 250000, reward mean: 4.9577
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9900
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.153    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.5      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -599      |
| loss/alpha                         | 0.00238   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 135000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9679
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.942   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -600     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9725
num rollout transitions: 250000, reward mean: 4.9742
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.666    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -601     |
| loss/alpha                         | -0.0162  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 137000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9774
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.68     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -601      |
| loss/alpha                         | 0.0352    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 138000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9757
num rollout transitions: 250000, reward mean: 4.9610
num rollout transitions: 250000, reward mean: 4.9657
num rollout transitions: 250000, reward mean: 4.9807
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.395     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -602      |
| loss/alpha                         | -0.0343   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 139000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9742
num rollout transitions: 250000, reward mean: 4.9723
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.113    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.8      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -603      |
| loss/alpha                         | -0.0837   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 140000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9626
num rollout transitions: 250000, reward mean: 4.9770
num rollout transitions: 250000, reward mean: 4.9792
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.0864  |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -604     |
| loss/alpha                         | -0.0439  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 141000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9655
num rollout transitions: 250000, reward mean: 4.9747
num rollout transitions: 250000, reward mean: 4.9734
num rollout transitions: 250000, reward mean: 4.9819
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -604      |
| loss/alpha                         | 0.0117    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 142000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9777
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.665    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -605     |
| loss/alpha                         | -0.00952 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9679
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9755
num rollout transitions: 250000, reward mean: 4.9900
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -1.26     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -606      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 144000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9740
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 1.1      |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -606     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9709
num rollout transitions: 250000, reward mean: 4.9662
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9814
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.24      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.9      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -607      |
| loss/alpha                         | 0.0809    |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 146000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9720
num rollout transitions: 250000, reward mean: 4.9777
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.32      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -607      |
| loss/alpha                         | -0.156    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 147000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9742
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9745
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.887   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -608     |
| loss/alpha                         | -0.0485  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 148000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9826
num rollout transitions: 250000, reward mean: 4.9654
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-09 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.0281  |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -609     |
| loss/alpha                         | -0.077   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9740
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.852   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -610     |
| loss/alpha                         | 0.094    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 150000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9715
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9780
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.636    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -610      |
| loss/alpha                         | -0.062    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 151000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9952
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.214    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -611     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 152000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9758
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.552     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -612      |
| loss/alpha                         | 0.109     |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 153000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.102    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -613     |
| loss/alpha                         | 0.0644   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 154000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9708
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.069     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.3      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -614      |
| loss/alpha                         | 0.0042    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 155000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9682
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.562    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.8      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -614      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 156000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.943    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -615     |
| loss/alpha                         | 0.00957  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 157000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9806
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.666     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -616      |
| loss/alpha                         | 0.0176    |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 158000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9848
num rollout transitions: 250000, reward mean: 4.9812
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.283     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -616      |
| loss/alpha                         | -0.0177   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 159000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9835
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.484     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.9      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -617      |
| loss/alpha                         | -0.177    |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 160000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9783
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9866
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.435     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -618      |
| loss/alpha                         | -0.0345   |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 161000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.159   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -618     |
| loss/alpha                         | 0.114    |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 162000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9687
num rollout transitions: 250000, reward mean: 4.9860
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.216     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -619      |
| loss/alpha                         | -0.00311  |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 163000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9839
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.35e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.378     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -620      |
| loss/alpha                         | -0.0136   |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 164000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9937
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.419    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -621     |
| loss/alpha                         | 0.134    |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 165000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9818
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 4.9810
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.397    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -621      |
| loss/alpha                         | -0.0146   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 166000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9852
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.827    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -622     |
| loss/alpha                         | 0.0461   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 167000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9894
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.84e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.000128 |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 2.51      |
| loss/actor                         | -622      |
| loss/alpha                         | -0.0321   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 168000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9920
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.525    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.4      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -623      |
| loss/alpha                         | 0.00446   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 169000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.357   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -623     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 170000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 1.08     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -624     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 171000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9845
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9833
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -625     |
| loss/alpha                         | -0.0652  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 172000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9807
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9783
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -625      |
| loss/alpha                         | 0.0625    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 173000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9746
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9898
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.258     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -625      |
| loss/alpha                         | 0.053     |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 174000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9724
num rollout transitions: 250000, reward mean: 4.9784
num rollout transitions: 250000, reward mean: 4.9845
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.406    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -626      |
| loss/alpha                         | -0.00194  |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 175000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9687
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9910
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.518     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -626      |
| loss/alpha                         | -0.0385   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 176000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
num rollout transitions: 250000, reward mean: 4.9808
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9755
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.673     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.5      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -626      |
| loss/alpha                         | -0.0137   |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 177000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9702
num rollout transitions: 250000, reward mean: 4.9820
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.0598    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67        |
| eval/normalized_episode_reward_std | 7.18      |
| loss/actor                         | -627      |
| loss/alpha                         | 0.0244    |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 178000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9824
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.588   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -627     |
| loss/alpha                         | 0.0348   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 179000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9742
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9882
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -1.03     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.3      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -627      |
| loss/alpha                         | -0.0419   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 180000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0039
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.22e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.379     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.6      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -627      |
| loss/alpha                         | -0.135    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 181000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9848
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9843
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.914     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.4      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -627      |
| loss/alpha                         | -0.0273   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 182000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9865
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.4     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -628     |
| loss/alpha                         | 0.0427   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.49e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.145    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -628     |
| loss/alpha                         | -0.0575  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9927
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10   |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -628      |
| loss/alpha                         | -0.000268 |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 185000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9844
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9891
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.454     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.9      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -629      |
| loss/alpha                         | 0.0447    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 186000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9785
num rollout transitions: 250000, reward mean: 4.9701
num rollout transitions: 250000, reward mean: 4.9844
num rollout transitions: 250000, reward mean: 4.9767
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.224    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -629     |
| loss/alpha                         | 0.00487  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 187000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9744
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.953    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.132   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 188000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9734
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3e-10    |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0209  |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -630     |
| loss/alpha                         | 0.0304   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 189000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.83     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -630     |
| loss/alpha                         | 0.118    |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9847
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.48     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.1      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -630      |
| loss/alpha                         | -0.036    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 191000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9742
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9769
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.352   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -630     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9791
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.00456  |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -631      |
| loss/alpha                         | -0.0231   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 193000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9765
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0732   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -631     |
| loss/alpha                         | 0.000192 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 194000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.5e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.531    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 2.5      |
| loss/actor                         | -631     |
| loss/alpha                         | -0.0107  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 195000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9767
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9865
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.343    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.00289 |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9847
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.706     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -632      |
| loss/alpha                         | 0.0647    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 197000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9801
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.643    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -632     |
| loss/alpha                         | 0.0579   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 198000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.407   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 5.62     |
| loss/actor                         | -633     |
| loss/alpha                         | -0.0484  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 199000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9865
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.22e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.29      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -633      |
| loss/alpha                         | -0.0448   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 200000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 5.0027
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.314     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -634      |
| loss/alpha                         | 0.0441    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 201000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9921
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.701    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -634     |
| loss/alpha                         | -0.00179 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 202000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9909
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0883   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -634     |
| loss/alpha                         | 0.0316   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 203000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.427     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.9      |
| eval/normalized_episode_reward_std | 2.5       |
| loss/actor                         | -635      |
| loss/alpha                         | -0.0427   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 204000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.379    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0515  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 205000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.918    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -635     |
| loss/alpha                         | 0.0804   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 206000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0035
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -1.11     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.2      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -635      |
| loss/alpha                         | -0.0937   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 207000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9915
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.96e-12 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.0952  |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -636     |
| loss/alpha                         | 0.0792   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 208000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0003
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -636     |
| loss/alpha                         | 0.0898   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9897
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.403    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.46      |
| loss/actor                         | -636      |
| loss/alpha                         | -0.0816   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 210000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.07      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0412   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 211000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.589    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -637     |
| loss/alpha                         | 0.0327   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 212000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0013
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.00494   |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -638      |
| loss/alpha                         | -0.0658   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 213000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.308    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.0602   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 214000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9849
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.6e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.829    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.00245 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 215000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9739
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.812   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -639     |
| loss/alpha                         | 0.0902   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 216000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.73e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.834   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.0791  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 217000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9929
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.466     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.4      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -640      |
| loss/alpha                         | -0.0879   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 218000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.132    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0557   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 219000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.768   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 220000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.213    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 221000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9883
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.587    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -641      |
| loss/alpha                         | -0.0737   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 222000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.352    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -641      |
| loss/alpha                         | -0.166    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 223000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.467   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -641     |
| loss/alpha                         | 0.149    |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.764    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -641     |
| loss/alpha                         | -0.0598  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.417    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -642      |
| loss/alpha                         | 0.041     |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 226000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9935
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.603     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.2      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -642      |
| loss/alpha                         | 0.0263    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 227000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.255     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.9      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -642      |
| loss/alpha                         | -0.0217   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 228000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.61     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -642     |
| loss/alpha                         | 0.00013  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.748    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.0641  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 230000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.357     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -643      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 231000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.508    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -643     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 232000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.15e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.435    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.48      |
| loss/actor                         | -644      |
| loss/alpha                         | 0.0438    |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 233000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.908   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.00166  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9936
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.509    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -644      |
| loss/alpha                         | -0.00308  |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 235000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.529    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.074    |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 236000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9953
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.8e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.714   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 237000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.631   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.0374  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-09   |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.717    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 239000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0082
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.39      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -645      |
| loss/alpha                         | -0.0376   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 240000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -645     |
| loss/alpha                         | -0.092   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 241000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.524    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.5      |
| eval/normalized_episode_reward_std | 13.2      |
| loss/actor                         | -645      |
| loss/alpha                         | 0.00968   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 242000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9923
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.147     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -645      |
| loss/alpha                         | 0.0369    |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 243000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 4.9965
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.194    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 7.7       |
| loss/actor                         | -646      |
| loss/alpha                         | -0.00547  |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 244000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0033
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.163    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -646      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 245000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9915
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.572   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -647     |
| loss/alpha                         | -0.00807 |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0021
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.14e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.509   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -647     |
| loss/alpha                         | -0.0286  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 247000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.22     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -648     |
| loss/alpha                         | 0.00635  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 248000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9956
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.00592  |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -648      |
| loss/alpha                         | 0.0321    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 249000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.94      |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -648      |
| loss/alpha                         | 0.0866    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 250000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.373   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -648     |
| loss/alpha                         | -0.1     |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 251000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.991     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.2      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -649      |
| loss/alpha                         | 0.05      |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 252000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.901    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.6      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -649      |
| loss/alpha                         | 0.0514    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 253000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.784   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -649     |
| loss/alpha                         | 0.0746   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 254000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-11  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.965    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -650     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.196   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -650     |
| loss/alpha                         | -0.0484  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 256000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.167    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -650     |
| loss/alpha                         | 0.0376   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 257000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.47     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -651     |
| loss/alpha                         | -0.0261  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.59     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -651     |
| loss/alpha                         | 0.031    |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 259000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9980
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.537    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -651     |
| loss/alpha                         | 0.128    |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 260000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0122
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.128    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -652      |
| loss/alpha                         | -0.0295   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 261000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9960
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.188    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -652      |
| loss/alpha                         | -0.000841 |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 262000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.118    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -652     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9818
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.287   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -652     |
| loss/alpha                         | -0.175   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 264000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.202     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -652      |
| loss/alpha                         | 0.0334    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 265000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.207    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -652      |
| loss/alpha                         | 0.0068    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 266000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0075
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.28     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.51      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0142   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 267000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-11  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.385   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9977
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.562     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0607   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 269000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9927
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.517     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -653      |
| loss/alpha                         | 0.00246   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 270000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.978    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0741   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 271000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9892
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.442   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -654     |
| loss/alpha                         | 0.111    |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 272000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.291    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -654     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 273000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9975
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.452    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -654     |
| loss/alpha                         | -0.031   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 274000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0124
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.42      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.0709   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 275000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.376   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0422  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 276000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.517   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.000843 |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.882     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.032    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 278000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9930
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.327   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.0865   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0056
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.358     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -655      |
| loss/alpha                         | 0.0769    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 280000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -1.26    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.106   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 281000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-11  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0798  |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 282000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.72e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0752    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -655      |
| loss/alpha                         | 0.0197    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 283000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.362    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.000772 |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 284000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.286     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -655      |
| loss/alpha                         | 0.115     |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 285000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 1.81     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -656     |
| loss/alpha                         | -0.0901  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 286000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0454   |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -656      |
| loss/alpha                         | 0.0358    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 287000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.566    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.00683  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 288000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.236   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -656     |
| loss/alpha                         | -0.0962  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 289000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.692   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.00539  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 290000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9980
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.989    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.0545   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 291000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.423     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -656      |
| loss/alpha                         | 0.0728    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 292000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.19     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.0264  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 293000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0696    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.1      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -657      |
| loss/alpha                         | -0.0611   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 294000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.316    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -657     |
| loss/alpha                         | 0.0209   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 295000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.707    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.4      |
| eval/normalized_episode_reward_std | 10.1      |
| loss/actor                         | -657      |
| loss/alpha                         | -0.0339   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 296000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 4.9965
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.383     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -657      |
| loss/alpha                         | 0.00316   |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 297000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.569     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -657      |
| loss/alpha                         | -0.0492   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 298000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9926
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.704   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.6e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.274   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -658     |
| loss/alpha                         | 0.0613   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 300000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.278    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -658     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 301000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0561    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0653   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 302000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.7       |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0544   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 303000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0067
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.07e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.437    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -658      |
| loss/alpha                         | 0.00722   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 304000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.165   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0347  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 305000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.14e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -658     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 306000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.851     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.2      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -659      |
| loss/alpha                         | -0.0249   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 307000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.15    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0499   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 308000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.639    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0521   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 309000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 4.9882
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0826   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0271  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 310000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.56e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0956  |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -660     |
| loss/alpha                         | 0.0318   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 311000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9937
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.363     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.0798    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 312000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.263     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.0353    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 313000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0020
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0212    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.039    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 314000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.242    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.6      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.038    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 315000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.398    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0862  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 316000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.531    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0708  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 317000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.353    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0906    |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 318000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.542   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.46     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0454   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.285    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0374  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 320000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.8      |
| eval/normalized_episode_reward_std | 21.1      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0241    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 321000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9961
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.17     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.00179  |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 322000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0474    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.0585   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 323000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.888    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.133   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 324000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.362    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0698   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 325000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0284    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0392    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 326000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0599  |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0904   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 327000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.659    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.586    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.0375   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 329000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9993
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -1.22    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0076
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.0376   |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.51      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.00624   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 331000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.46      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.00181   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 332000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0028
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.519     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -662      |
| loss/alpha                         | -0.108    |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 333000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.69e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.894   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0105   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.402     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.0374   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 335000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.904   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.119    |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 336000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.313   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -663     |
| loss/alpha                         | -0.00703 |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 337000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0023
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.71      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -663      |
| loss/alpha                         | -0.0166   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 338000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.18    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 339000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.0554   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 340000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.522    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -664     |
| loss/alpha                         | -0.0624  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 341000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9911
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.105    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -664      |
| loss/alpha                         | -0.0589   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 342000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.4      |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -664     |
| loss/alpha                         | -0.0101  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 343000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 5.0011
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.00702   |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0593    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 344000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0828   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.00937  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.685   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.0407   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 346000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.194   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.0949   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 347000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.571    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -664     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 348000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-09 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.375    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -664      |
| loss/alpha                         | -0.0884   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 349000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.148    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.5      |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0179   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 350000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0643    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0217    |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 351000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.566    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0751  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 352000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0908    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 10.3      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0177    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 353000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.397   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 354000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-11   |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.175    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.00333 |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 355000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.853    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 356000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.404    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0243    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 357000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -1.07     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0202   |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 358000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.938   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0544  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 359000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.606    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0422  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 360000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9919
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.166    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0123   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 361000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0849    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0333   |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 362000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.65      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -667      |
| loss/alpha                         | 0.0496    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 363000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.294     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -667      |
| loss/alpha                         | 0.0823    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 364000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0477   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 365000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.628    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.057   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.364     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -667      |
| loss/alpha                         | 0.0181    |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 367000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.201    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.00934  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 368000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0010
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0553  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 369000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.25     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 370000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.199     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -667      |
| loss/alpha                         | 0.177     |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 371000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.34      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.0036    |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 372000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0295   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.0558  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 373000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0199
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.292     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -668      |
| loss/alpha                         | -0.0277   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 374000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9950
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.0766    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.046     |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 375000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.029    |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 376000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.0842   |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.03      |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 377000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.539    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0395  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 378000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0163    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0868   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 379000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.197   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.107    |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 380000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0555   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0444  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.693     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0582   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 382000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.469    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.069     |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 383000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.568    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0272   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 384000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.184    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.0534    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 385000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.38e-12 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.678   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.154   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.38     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0508   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0122
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.871     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.0242    |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 388000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.611    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 389000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.00685 |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 390000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.763   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.135   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 391000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.965     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.04     |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 392000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0120
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.48e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -1.26     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.133     |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 393000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.734     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.9      |
| eval/normalized_episode_reward_std | 14.8      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0391   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 394000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.469    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0447   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 395000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9948
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.345     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.00706   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 396000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0046
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.158    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0589    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 397000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0070
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.873    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0384    |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 398000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0454  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.179     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.0262   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 400000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.256    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.853   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 402000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0687   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0624  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 403000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.353    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 404000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7e-11    |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.801   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0104
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.214    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.015    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 406000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.618     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -670      |
| loss/alpha                         | -0.00378  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 407000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.719     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0415    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 408000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.61     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0469  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0084
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.565     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0377    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 410000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.341    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0547  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 411000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.51      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0712   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 412000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0253
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.205    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0614   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.706     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0382    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 414000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.45     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.00766  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 415000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.03      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0508    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 416000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.467   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0565  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 417000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0037
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.418     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0593    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 418000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.083   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.00377  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 419000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0165
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.749     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0573   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 420000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.377   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0015  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-11   |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.43     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0551  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 422000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9998
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.43     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.07     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 423000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.79      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.0865    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 424000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.578    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.0593    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 425000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.624   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.00913  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 426000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-09 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.258    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0988  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.353    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.731    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -674      |
| loss/alpha                         | 0.081     |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 429000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.366     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.0632    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 430000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9923
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.703     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0165   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 431000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.286     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0816   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 432000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.871   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0987   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 433000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.9e-11  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.99     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0321   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 434000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.433     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0382   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 435000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.257   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0522  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.22     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 437000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -1.4     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 8.5      |
| loss/actor                         | -675     |
| loss/alpha                         | -0.038   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 438000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.421   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0797   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.0626    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -675      |
| loss/alpha                         | 0.00126   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 440000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.02     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0809  |
| loss/critic1                       | 17       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 441000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.105   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.00192 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 442000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0447    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.00588  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 443000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9984
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.28      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -675      |
| loss/alpha                         | 0.0447    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 444000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0152
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -1.43     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.00632  |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 445000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.643    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.00164  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 446000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0136
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.297     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0522    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 447000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0229
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.0788  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 448000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.278   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0634  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 449000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.559   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.00729 |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 450000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.501   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0516   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 451000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.731     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0902   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 452000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 4.9988
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.547     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0133   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 453000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.0196   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0551  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 454000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.998    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0805   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 455000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.15e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.25     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 19        |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0265   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 456000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0465   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0765   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 457000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0767  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 458000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.694    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0997   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.158    |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 460000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.547    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.00289 |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 461000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0144
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.221     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 21.6      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0951   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 462000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.21     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.00388  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 463000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.182    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 4.04     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0595  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 464000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.85e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.395    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.0211  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 465000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.181   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0164   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 1.57      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0947   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 467000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.784     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.49      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0888    |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 468000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.545   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.117   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 469000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.306    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0816   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 470000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.558    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 471000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0090
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.25      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0721   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 472000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.629    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 6.9       |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0213   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 473000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.236    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.035    |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 474000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 4.9984
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.745     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.00232   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 475000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0021
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.22e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.946     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 15.5      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0231   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 476000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.504    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0712   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 477000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.417     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.00497  |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 478000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.46     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0611   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 479000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0101
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.33     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0586   |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 480000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.66    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.031    |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 481000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.21      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.126     |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 482000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10   |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0421   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 483000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.38      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.197    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 484000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.346    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -679      |
| loss/alpha                         | 0.116     |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 485000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.567   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0397   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 486000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.0756  |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0904  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 487000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.776     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0812    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 488000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0042
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.817     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.00107   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 489000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.391     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0414   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 490000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.619     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0301   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 491000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.127   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0339   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.98e-12 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.139     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0777    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 493000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.171   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.00896  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.395     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 6.03      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.00461   |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 495000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.299    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0114   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 496000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.177    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 497000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.723    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.084   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 498000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.3       |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 8.19      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0122   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 499000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.223    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 500000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0056
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 5.51      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.000246 |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 501000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.216    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 7.12     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.131    |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.189   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.016    |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 503000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.73     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 504000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.534    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0473   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0196
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.36     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0916   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 506000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.108     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0372    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 507000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.04      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.00658   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 508000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0017
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.16      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 13.5      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0287   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 509000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9930
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-09  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.448    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0506   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 510000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0037
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.348    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.00175  |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 511000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0023    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0213   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 512000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.89     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.111   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.19e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.608    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0533   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 514000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.0475   |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0723    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 515000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0144
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.413    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0891   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 516000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0241
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.512   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0238  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 517000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.217     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0198    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 518000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 4.9964
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.037     |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 519000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.199   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0315   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 520000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.639    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.017    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 521000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.237     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 8.24      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0986    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 522000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.29e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.299     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0334   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 523000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -1.43    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0221  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.351    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -680     |
| loss/alpha                         | 0.122    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 525000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.464     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.00668  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 526000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.425    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.00182  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 527000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.563    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0235   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 528000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.359    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0904  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.26     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.00608 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 530000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3e-10    |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0885  |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0204  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 531000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.152    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.51      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0283   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 532000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.465    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0502   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 533000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0336   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0821   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 534000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0282   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0304   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.592    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0992  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 536000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.299     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0165   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 537000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.19e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.73    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0743   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 538000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0174
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.195     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0689   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 539000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.311    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0676   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 540000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 20.7      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0897   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 541000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.587     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0331   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 542000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.239   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0831   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 543000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.07      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0571   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 544000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0003
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.584     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.00423   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 545000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.0541   |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.047     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 546000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.06e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.57    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0582  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 547000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.407    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0502   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 548000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0252  |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -681     |
| loss/alpha                         | -0.00859 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 549000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.203   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0486   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 550000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0655   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 551000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.328    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0753   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 552000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.702    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 553000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0046
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.696    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0653    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 554000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.554    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.00753   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 555000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0101
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.353    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0247   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 556000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0173
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.702     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0781   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 557000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.244    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0577   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.0986  |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 560000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0024
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.508     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.104    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 561000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -1.32     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0761    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 562000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.0707    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 7.4       |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0688    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 563000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.136     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.028    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 564000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9891
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.415   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0453   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 565000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.121   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -682     |
| loss/alpha                         | 0.00973  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 566000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.92e-12 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.825    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0965  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 567000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.407     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.65      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0776    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 568000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.884   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -682     |
| loss/alpha                         | 0.062    |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.208     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 4.59      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0628   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 570000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 2        |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0137   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 571000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.55     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.104   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0131
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.454     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.205    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 573000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0008
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0484   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 574000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.967    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0975   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-12 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.561    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.112    |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 576000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.356    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.101   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 577000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.299    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.2      |
| eval/normalized_episode_reward_std | 21.2      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0495   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 578000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.69e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.357     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0927    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 579000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0939  |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.445     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0743   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 581000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.0504   |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0597   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 582000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0008
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.299    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.33      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0193   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 583000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.251     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0584    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 584000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.594   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0482   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 585000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 1.42     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 586000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.235    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0747   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 587000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.022    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 588000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.185    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.135   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 589000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.863    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 4.94     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.00606  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 590000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0934    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0259   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 591000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.628    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0566    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 592000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.305     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 7.84      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0352   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 593000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0039
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -1.25     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.44      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.00455   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 594000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.943    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0623   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.86e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.0227   |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0589   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 596000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.745     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.39      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.00919   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 597000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.251    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0344   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 598000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.426     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 18.8      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0535   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 599000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.76      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 600000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.379    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.027   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 601000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.897    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0446   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 602000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.133    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 6.58      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0433    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 603000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.427    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0669   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 604000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 605000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.138     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0383   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 606000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.141    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00615  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 607000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0112
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.15e-13 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.14     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.00289  |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 608000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.647    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 14.2      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00394   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 609000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.632     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0236   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 610000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.256    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0957    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 611000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0016
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.317    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0772  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.426    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0393   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 613000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 1.05      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0199    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 614000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.368    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 6.56     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0632   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 615000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.416    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0182   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 616000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.72     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0557   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 617000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0220
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.382     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.00761  |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 618000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.639    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0201  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 619000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0441   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 620000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.751    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0155   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 621000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.111   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0325  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 622000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -1.08     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0696   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 623000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.163    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.73     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0138   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 625000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0685    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.117     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 626000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.065     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.113    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 627000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.548    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0113   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 628000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0040
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.273     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 22.1      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0656   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 629000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.886    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00498  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 630000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 1.05     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0547  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 631000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.515     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0534   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 632000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.837     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.8      |
| eval/normalized_episode_reward_std | 20.8      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0952    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 633000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.131   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0608  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.0628   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.029    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.77      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.55      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.00755  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 636000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0718   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0168
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.15e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.709     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0455   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 638000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.47e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.025    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0855   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 639000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.354    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0392   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 640000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.708   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0457  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 641000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.99e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.734   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0028  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 642000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.242   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0248  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9982
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.786   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 644000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.95e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.185    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0025  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.203    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0971   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.408   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0344  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 647000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.756     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.00203  |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 648000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.72e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.3      |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0601   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.605    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 10.5     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0922  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 650000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.0619   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0593   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 651000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9921
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.39      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -686      |
| loss/alpha                         | 0.00351   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 652000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0101
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.659     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0279    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 653000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.211    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0269   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 654000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.162     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0255   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 655000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0218
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.0522  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0464  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 656000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 4.9953
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.406     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0129   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 657000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.475    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0598  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.249    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 659000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.519    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0896   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 660000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.102   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0282  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -1.22    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0614  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 662000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.129    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.045   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 663000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.399    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0296   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 664000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.618     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0224   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 665000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.45      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0398    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 666000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0104
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.961     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 11        |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0787   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 667000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.29e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.193   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0423   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 668000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0118
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.41      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.93      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0186   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 669000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0297
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.864     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.134     |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 670000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.699   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 671000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.218    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0358   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 672000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.546     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0833   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 673000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.759     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.62      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0435   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 674000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-10   |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.159   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0575   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 675000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0246
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0648    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0394   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 676000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.261    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 6.97      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0798    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 677000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0674    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0283    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 678000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0195
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.932     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0312   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 679000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.0746   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 680000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.767    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0443   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 681000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.566   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0643  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 682000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-09 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.238    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0585  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 683000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.485     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 4.05      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0726   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 684000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.494     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0524    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 685000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0060
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.233    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -688      |
| loss/alpha                         | 0.00705   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 686000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.44    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.00494  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 687000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0243  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0323   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 688000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.85e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.458    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.00197  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 689000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.902    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.121   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.625   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 691000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0155
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3e-10   |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.327    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0648  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 692000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.545     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.00783   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 693000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9987
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.0804   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0958   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 694000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.319     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0716    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 695000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.109    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 696000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -1.13    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0251  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 697000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.807     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 4.05      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0416    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 698000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.428     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 20.6      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0701    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 699000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0023
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.0493   |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 4.39      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0073    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 700000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.495    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.144     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 701000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.554     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0198   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 702000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9884
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.461   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0503  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 703000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.559     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0565    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 704000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.589   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.094   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 705000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0048
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.266    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 706000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.22     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 5.9      |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0997  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 707000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.979     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 9.56      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0393    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 708000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.24      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0846    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 709000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.496     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.48      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0469    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 710000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9983
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.907     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0505   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 711000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0189
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.458    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0511   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 712000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.584    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 4.02     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0764  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 713000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0317   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.038   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 714000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.386     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 8.62      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.116     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 715000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.11      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.032     |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 716000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9976
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.775     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.71      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0227   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 717000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.0893   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.86      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0357   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 718000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.0749   |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0107    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 719000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.717     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0251    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 720000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.959     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 19.4      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0234    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 721000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.281    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 722000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.599    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0514  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 723000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.334    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.00579 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 724000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.902    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 725000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.368    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 726000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.276     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0069   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 727000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 1.16     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 728000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.288    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0261  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 729000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.227   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0485   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 730000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9990
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.363   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.221    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0379  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.673    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 9.95     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0382  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.723     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 7.97      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.0427    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 734000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.0724   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.146     |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 735000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.265    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0748  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 736000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 737000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0141  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 738000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.612   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 739000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.703     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.0261    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 740000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.847    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.475   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 742000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.495    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 743000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-11  |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.804     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.000473 |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 744000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0232
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.201   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0714   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.434     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -692      |
| loss/alpha                         | -0.00667  |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 746000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.82      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0233    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 747000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0789  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0548  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.00793  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0414   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.544   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.134    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.62     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0793  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 751000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.513     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0187    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 752000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0175
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.075     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0532   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 753000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.404    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0536   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 754000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.212     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.113     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 755000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0142
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.058    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 756000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.168   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0717   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 757000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.76e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.309   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0626  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.13    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0416  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 759000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.143    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0484    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 760000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.488     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -692      |
| loss/alpha                         | 0.00734   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 761000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.29e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.325    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.138   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 762000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.084    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0924  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 763000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.347    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0814   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 764000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.92e-12 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.306   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0711   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 765000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.379    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.00798  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 766000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.116    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 767000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.686     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.0573    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 768000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.599     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0288   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 769000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.0816  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 770000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.536    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0586   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 771000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0444   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0501   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 772000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.166    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0522  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.598    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00686  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.968    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -692     |
| loss/alpha                         | -0.032   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 775000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.501    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 9.61     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.000884 |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 776000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.178    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0772   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 777000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.243    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0727  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 778000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0114
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0202   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0563   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 779000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.263     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.66      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.0301   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 780000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0122   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.316     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.0503    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 782000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.158    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.00878 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.405    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.33      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0103   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 784000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0149
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.269     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.41      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.0415   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 785000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.483     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.39      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0499   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 786000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.277   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 787000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.469     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0149    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 788000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.587   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.86     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.00207 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.89e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.797     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0759   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 790000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9957
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 11.9      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0409    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 791000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9771
num rollout transitions: 250000, reward mean: 4.9971
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.884     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0479   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 792000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9944
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 5.55      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0237   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 793000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.51     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 8.44     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0627   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.398   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.000729 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 795000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.552    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0534   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 796000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.415    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00339  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 797000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.198    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0558  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.512    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0254   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 799000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.109    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 8.59      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0734    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 800000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.758    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.106     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 801000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.282     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.115    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 802000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.646    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00278  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0142
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.4       |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.00722  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 804000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0022
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.273     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.0509   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 805000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0052
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.483     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 19.8      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.013     |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 806000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.179   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 807000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 4.9963
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.32     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0558   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 1.11      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.148     |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 809000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.21     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 4.22     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.057   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 810000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.059   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.62      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.0714   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 812000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.0621   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 12.4      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.124     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 813000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.588    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 11.1      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0601   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 814000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.646   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 815000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.193     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.1      |
| eval/normalized_episode_reward_std | 24.4      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.00329  |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 816000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.54     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0485   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 817000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.55e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.0694   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 12        |
| loss/actor                         | -695      |
| loss/alpha                         | -0.00532  |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 818000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0332   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 27.4     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.00851  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0143
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.01e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.403     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0215    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 820000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.18      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -695      |
| loss/alpha                         | 0.00842   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 821000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.154   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 822000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.365   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0663   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.1e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.1      |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -695     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.32     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.883    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 4.24     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0026  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.691    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.51      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.00309  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 827000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.925    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0353   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.889   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0916  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.205     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0195   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 830000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.474     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 20.6      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0275    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 831000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0149
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.85e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.912     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0339   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 832000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.759    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0539    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 833000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 4.9998
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.458     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0237   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 834000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0046
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.49      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 15.1      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.064     |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 835000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.398    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0695  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 836000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.686   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0722  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.405     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0123    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 838000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-11  |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.0213  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 6.99     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0711   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 839000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.722    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.122    |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.29     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -693      |
| loss/alpha                         | -0.0424   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 841000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.73      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.048    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 842000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.163     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 19.7      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.00542   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 843000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.445     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0109   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 844000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.26      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0142   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 845000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0173
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.13      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 8.11      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0424    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 846000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.6e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0326   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -694     |
| loss/alpha                         | 0.00556  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 847000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.501   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 14.3     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0293  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 848000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.844     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 6.96      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0679   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 849000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.255     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 3.76      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.00484  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 850000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.00587  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.72     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.36      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0549   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 852000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0013
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.205    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0214    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 853000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.392   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0367   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 854000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.321    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0455   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 855000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9910
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.208   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0256  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 856000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.301    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0596   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 857000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9874
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.22e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.212    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 858000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.36     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0734   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 859000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0825    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.00431   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 860000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.00478  |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0426   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 861000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.715     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.57      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0187    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 862000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.11e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.0346  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 863000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.488     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.53      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.025     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 864000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.7      |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.79     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0766  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 865000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.416   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00827  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 866000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0174
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.447    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.67      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0417    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 867000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.592    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.033     |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 868000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.248    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0127  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 869000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0464   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0228   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.275   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 871000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0283
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.284    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.65      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.053    |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 872000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.0176    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0558   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 873000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.228     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.00375  |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 874000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0207
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.203     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0326    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 875000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -1.27     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 10.1      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.164     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 876000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.22      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.42      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0467   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 877000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.797     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.55      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0347   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 878000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.485    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.74     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.00108 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.107    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.107    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 880000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.0303   |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0741    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 881000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.532     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -696      |
| loss/alpha                         | -0.046    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 882000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0072
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.421     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.00427   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 883000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.812     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 17.8      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.00105   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 884000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0949    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 11.9      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.012     |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 885000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.52e-12 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.00527  |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 4.01      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0476    |
| loss/critic1                       | 12        |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 886000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -1.18    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.67     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0236   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.43e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.645   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 888000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.53     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0786    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 889000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.219    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.2      |
| eval/normalized_episode_reward_std | 3.55      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0361   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 890000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.143     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.33      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.000227 |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 891000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.22e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.0712   |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0284   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 892000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0220
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0553  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0135
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0556    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0439    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 894000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0868   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 895000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.453     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0194    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 896000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.366    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.125     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 897000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 4.9963
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.638     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0777   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 898000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.598    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.017    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 899000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.3       |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0545   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 900000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.758     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.77      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0321   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 901000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.76    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0248  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 902000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.89e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.983    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0636   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.513    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0584  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.472    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0571   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 905000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.579   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00344  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 906000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 4.9975
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.92      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0247   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 907000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.605    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 5.97      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0906   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 908000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0484    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.56      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0723   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 909000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.731   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0746   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.671    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.153    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 911000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.62e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.233     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.000873 |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 912000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.386   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0223  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 913000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0653  |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0389  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 914000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.432    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0714  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.28     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.00354 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 916000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.72e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.351    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -1.17    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00892  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.35     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 5.15      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0214   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 919000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-09 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.533     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0309    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 920000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.302    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.00264 |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 921000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.174    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 922000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.166    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.096   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 923000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0552   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00458  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.582     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 4.44      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0695    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 925000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.766     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0981    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 926000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0135
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.642     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.029    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 927000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.3       |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 21.3      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0835   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 928000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.619    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0433   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 929000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.779     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.74      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0215    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 930000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.259    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 931000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0114
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.476    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0483  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 932000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.589    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.121    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 933000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.124    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.625    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0714  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0166    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 19.5      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0481   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 936000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.516     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.00708   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 937000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.28      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0209   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 938000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.07     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.143    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 939000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0181
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.377   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0789  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 940000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.256     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 4.86      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0519    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 941000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0687  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 942000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.324   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0862   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 943000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.359   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 14.2     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.00152 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 944000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.232    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.08e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.338    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 21.3      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0189    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 946000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.154    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 5.2       |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0513   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 947000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0584    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.2      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0695   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 948000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.65e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.703    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0455   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 949000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.539   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0629  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 950000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.756    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0466   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 951000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.448    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 952000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.538    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0074   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 953000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.308   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0753  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 954000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9983
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.342     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0569   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 955000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.637     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 6.84      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0617    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 956000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.04     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.142    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 957000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.496    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.19      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 15.2      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0123    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 959000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.413    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 6.28      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0308    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 960000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0537   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -696     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0004
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0719    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 7.65      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0235    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 962000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.13     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.6      |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0454   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 963000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.351     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.61      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.11     |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 964000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0187
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0127   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0626  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 965000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.339    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.108     |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 966000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.395    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 18.4      |
| loss/actor                         | -698      |
| loss/alpha                         | -0.00496  |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 967000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.661   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -698     |
| loss/alpha                         | -0.00391 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.925     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -698      |
| loss/alpha                         | 0.0224    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 969000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-10   |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.37     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 4.86     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0157   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 970000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.446    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -698      |
| loss/alpha                         | -0.0997   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 971000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.174     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.05     |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 972000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0048
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.412    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0366  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 973000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.158    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0996   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 974000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0127
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.324    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 9.74      |
| loss/actor                         | -699      |
| loss/alpha                         | 0.00763   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 975000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.204   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.566   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0484  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 977000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.993   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 978000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.518    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0907  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0186   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 980000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.49     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.00272  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.243     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.56      |
| loss/actor                         | -699      |
| loss/alpha                         | 0.00952   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 982000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.614    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0533  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 983000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.324   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82       |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0536   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 984000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.553    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 985000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0207
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.529    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 6.76      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.00513  |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 986000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.474    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0295  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 987000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0319  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 988000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.836     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0403    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 989000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.3       |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 5.16      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0578    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 990000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.754    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0144   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 991000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.49e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.16     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0348  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0231
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.377     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.79      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0039    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 993000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.11     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 9.67     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0887  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0235
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.143    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0294   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 995000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0033
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.458     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.71      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0791    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 996000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.72     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.054    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 997000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0171
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.429     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0535   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 998000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.431    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0937    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 999000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9922
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.515     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0484    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1000000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.157    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.0797   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1001000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0094
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.541    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 9.6       |
| loss/actor                         | -699      |
| loss/alpha                         | -0.0774   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1002000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.9      |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1003000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.162    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 4.49     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0597   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1004000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.463     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.49      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.0167   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1005000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.00532  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -699     |
| loss/alpha                         | 0.07     |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1006000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0624   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.081    |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1007000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0566  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1008000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.906   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 7.24     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.054   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1009000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.372    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0522   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1010000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.85     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0301   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1011000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.505   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 8.84     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0349   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1012000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0220
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.52     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1013000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.833     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0346   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1014000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.261   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.00455 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1015000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.941    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 5.79     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0705  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1016000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0135
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.177     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0587    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1017000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0016
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.379    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0538    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1018000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0395   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0549  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1019000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.741    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0218  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1020000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-13 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.815     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 6.85      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0845   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1021000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.9e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.36     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -700     |
| loss/alpha                         | -0.00569 |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1022000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.355     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0152    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1023000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.34e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.38     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0445  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1024000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.214   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0258   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1025000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.805    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0475    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1026000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.523   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -699     |
| loss/alpha                         | -0.15    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1027000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.408    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 20.1      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0462    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1028000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.209     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0811    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1029000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.3      |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1030000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.534   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0962   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1031000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.662    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.97     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1032000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.775    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0267    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1033000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.344    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1034000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.63e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.763    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0495  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1035000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.32      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 5.65      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0644   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1036000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.3e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.758    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0393  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1037000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.119    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.00902 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1038000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.354     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0467    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1039000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0104
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0863    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0621    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1040000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.154    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.42      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.0013   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1041000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.704   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 5.8      |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1042000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.812    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0739  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1043000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.444    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 7.05      |
| loss/actor                         | -700      |
| loss/alpha                         | -0.00764  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1044000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.553    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0478    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1045000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 4.9993
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.563   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -700     |
| loss/alpha                         | 0.0171   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1046000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.184    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0353  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1047000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0165
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0276    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.66      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0243    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1048000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.896    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1049000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0252
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1050000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.576    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.078    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1051000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.8e-11  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.947    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.00584 |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1052000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.813    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.00206  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1053000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0211
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.27     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0173   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1054000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.689     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.65      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1055000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 4.9992
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.399    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0462   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1056000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.263   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 9.79     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0763  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1057000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.85e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.918    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0515   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1058000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.141     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.8      |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0987    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1059000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0187
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.126   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0384  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1060000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.652     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.58      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0389   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1061000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0112
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.368     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 18.6      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0328    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1062000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.02     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.00108  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1063000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.705    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0607  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1064000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.543    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0355   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1065000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9968
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.284   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0612  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1066000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.365    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0132  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1067000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-12 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.546    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 9.24     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0349   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1068000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.859    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0685   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1069000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.549     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 15        |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0463    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1070000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0954  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1071000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.22    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1072000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.683    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0612   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1073000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0968    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0966    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1074000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.255   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0234  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1075000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0159
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.361    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -702      |
| loss/alpha                         | -0.023    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1076000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.311    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0312  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1077000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.314   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0621  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1078000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0171
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.121     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0257    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1079000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0182
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.534   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.041   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1080000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.297    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1081000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0122
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.261    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0833    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1082000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 1.17      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0146   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1083000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0174
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.621     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00848  |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1084000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0366   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0204   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1085000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.144   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -703     |
| loss/alpha                         | 0.04     |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1086000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 4.9969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.626     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0169   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1087000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0172
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.656     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0532    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1088000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0039  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 8.48     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0474  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1089000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0746    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0379   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1090000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.243    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.00569  |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1091000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.574   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.00589 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1092000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9905
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.41      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -703      |
| loss/alpha                         | -0.00866  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1093000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.37      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0209   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1094000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.1       |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.42      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0753    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1095000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0122
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.458    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.00638  |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1096000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.158    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0255   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1097000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.869    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.035    |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1098000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.355    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0493   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1099000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.091    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.128    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1100000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.703     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 27.4      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0849   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1101000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.413     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0254   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1102000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-10    |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0889   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.089   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1103000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0187
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.358    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1104000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.875    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1105000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.289    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0439  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1106000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.303    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1107000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0857   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1108000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0152
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.57e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.972     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.029     |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1109000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0260
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0101  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1110000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.432    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.99      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.107     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1111000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.637    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0144    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1112000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.478    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0519  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1113000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0143
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.0368   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 14.8      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00654  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1114000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.56     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 9.51     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.104   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1115000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0239    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 9.7       |
| loss/actor                         | -703      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1116000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.91     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1117000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.401    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0146    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1118000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0290
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.314    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0307    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1119000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.437     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0209   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1120000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.246    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0066   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1121000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0007
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.665     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00387  |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1122000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.5e-11  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.449    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -701     |
| loss/alpha                         | 0.034    |
| loss/critic1                       | 12       |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1123000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.564    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.005   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1124000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.574    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0466   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1125000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.394     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 20.9      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0238   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1126000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0201
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.26      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0068    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1127000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.644     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0215    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1128000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.827    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0689  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1129000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.959    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.5      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.114    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1130000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.619    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 4.36     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.047    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1131000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0507   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 7.77     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0644   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1132000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.866    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0408   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1133000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0021
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.616     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.79      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.03      |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1134000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.62      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.75      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0151   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1135000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0184
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.57e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.138     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0601   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1136000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.198   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0239  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1137000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.736    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1138000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 9.14     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0638   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1139000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0214
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.25     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0271    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1140000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0146  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0228  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1141000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0276   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.091    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1142000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.429     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.122     |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1143000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.285    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1144000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.794   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1145000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.28     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 14.5      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0129    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1146000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0907   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0438    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1147000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.025     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00165  |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1148000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.378     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.062    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1149000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.119     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0359    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1150000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.179    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 5.12     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0508  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1151000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.5e-11  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0435   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0419  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1152000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0037
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.49e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.184     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00502  |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1153000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.608    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1154000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0909  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.00494  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1155000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.45e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.245     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0276    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1156000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0246
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.607     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0944   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1157000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.441    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.00535  |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1158000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0729    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.000182  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1159000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.282     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0188    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1160000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.453   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0516  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1161000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.7e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.0158  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.017   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1162000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.341     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0524    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1163000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.126    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0783   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1164000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.243     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0796   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1165000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.239    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1166000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.2      |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0232   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1167000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.336    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0371  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1168000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.119    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0381   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1169000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.124    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0348   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1170000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0355   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0752  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1171000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.333    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 19.5      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0476   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1172000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.83      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 9.13      |
| loss/actor                         | -701      |
| loss/alpha                         | 0.0817    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1173000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.817    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.00165  |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1174000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.337   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1175000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.743    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.00942 |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1176000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.232    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -701     |
| loss/alpha                         | -0.0439  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1177000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.319     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.00815   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1178000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 7.09     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0294   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1179000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.206    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0435   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1180000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.36    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -702     |
| loss/alpha                         | 0.026    |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1181000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0566  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0562  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1182000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0598    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0464   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1183000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.573    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0423  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1184000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.8e-11  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.251   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -702     |
| loss/alpha                         | 0.00893  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1185000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.06e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.987   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0929   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1186000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-11  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.591    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1187000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.262     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 4.1       |
| loss/actor                         | -702      |
| loss/alpha                         | 0.0507    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1188000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.126    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0503   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1189000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.323    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0643  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1190000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.49      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0141    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1191000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0469   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.0335   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1192000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9917
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.731    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.0778   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1193000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1194000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9925
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.409    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1195000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.838     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -702      |
| loss/alpha                         | -0.0273   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1196000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.452    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 22.7     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0155   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1197000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1198000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.59      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.2      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0687    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1199000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.552    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -703      |
| loss/alpha                         | 0.03      |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1200000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.263   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0804  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1201000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.44     |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1202000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.3       |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -704      |
| loss/alpha                         | -0.0668   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1203000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.116    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -704     |
| loss/alpha                         | 0.13     |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1204000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0035
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.09      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -704      |
| loss/alpha                         | -0.00194  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1205000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.651   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0538  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1206000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.649   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1207000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0172
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.253     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.0637   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1208000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.408   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1209000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.27     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0186   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1210000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.134    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0074   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1211000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.674    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0615   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1212000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0168
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.429    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 8.39      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.018    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1213000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.529    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0233   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1214000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.31e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.828    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.000522 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1215000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.579    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 15.1      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.0459    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1216000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.89e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.119    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0373   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1217000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.369    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 9.3      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00441  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1218000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0195
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.314    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.00834  |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1219000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0297
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.03e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.726     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0918   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1220000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.8      |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 10.3     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0458  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1221000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.715    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 9.7      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0689   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1222000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.444     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0149    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1223000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0028
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.15     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0269    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1224000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.298    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.69     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0419  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1225000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 4.9977
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.265     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.9      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0164   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1226000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0545  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0672   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1227000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.52     |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 6.39     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0409   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1228000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.85e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.232     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0719   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1229000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.913    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 4.36     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00921  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1230000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0027
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.42      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0189    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1231000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.0907    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0206    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1232000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0127
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -1.34     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0741   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1233000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.944   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0304  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1234000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.858    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0095    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1235000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.126    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -707     |
| loss/alpha                         | -0.00673 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1236000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.168    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0555   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1237000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.248   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1238000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.618    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0686  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1239000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.718    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0127  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1240000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0127
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.09e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.000852 |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0171    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1241000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.359   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.000236 |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1242000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.201    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0621   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1243000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.442    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0657  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1244000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.268     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.00536   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1245000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.227    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0426   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1246000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.283     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0565    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1247000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.307    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0416  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1248000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.327     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.00681  |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1249000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.876    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 6.12     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0684   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1250000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.943   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0472  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1251000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.05e-12 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.158   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.046   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1252000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.613     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.034    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1253000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.0312   |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0605    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1254000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.387    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0868   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1255000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.138    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0319    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1256000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.913     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -707      |
| loss/alpha                         | -0.131    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1257000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0619  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0372  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1258000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.53     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0444   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1259000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.295    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81        |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0981    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1260000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.492    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0139   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1261000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.209   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0396   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1262000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.012   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1263000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.112   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0268  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1264000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.07e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.732    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0352  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1265000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.895    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00828  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1266000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00686  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1267000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.616     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 16.7      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0969   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1268000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.914    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0804   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1269000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0143
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.09      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0183   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1270000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0149
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.357    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 22.1      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.12     |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1271000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.461   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0552   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1272000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0185
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.141     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0501    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1273000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.125    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0391    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1274000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0018
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.265    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0555   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1275000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.423     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0811   |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1276000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.276    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0247   |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1277000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.264    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.021    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1278000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.951     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 13.8      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0344    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1279000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0158
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-10   |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.000421 |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1280000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.488    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00881  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1281000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.191    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0843    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1282000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.511   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0581  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1283000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0299
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.23    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1284000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.201   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.112    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1285000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.499    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1286000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0582    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.2      |
| eval/normalized_episode_reward_std | 3.44      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.0522   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1287000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.276    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.7      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.15      |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1288000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0023
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.804     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.116     |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1289000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.475     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.0997   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1290000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.5e-11  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.295    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.000476 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1291000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.371    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0787  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1292000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.443     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.0905    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1293000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.68e-12 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.409   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.122   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1294000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0991   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0543   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1295000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.392    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.2     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0336   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1296000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.301   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -705     |
| loss/alpha                         | -0.122   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1297000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.111    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.00508  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1298000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.886   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1299000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.437    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.00046 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1300000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.71     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.055    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1301000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.0868  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0473   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1302000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.807    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -705     |
| loss/alpha                         | -0.00442 |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1303000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0178
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.701    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.0605   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1304000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0297
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.32      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -705      |
| loss/alpha                         | -0.000561 |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1305000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.176     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.0481    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1306000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.138   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0899   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1307000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0904   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.2     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0171   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1308000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.0352    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.49      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.121    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1309000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0774    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.00961   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1310000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.3     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0389   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1311000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.829    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 8.39     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1312000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.154    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82       |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0631   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1313000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.114    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1314000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.242   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1315000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-11  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00344  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1316000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.108   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0545   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1317000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0182
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.399   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0359  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1318000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.4     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0587  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1319000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.787   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0622  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1320000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.349     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0505   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1321000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.498   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0258   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1322000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0992   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.5     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0706  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1323000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.47     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0861   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1324000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.228     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0485    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1325000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0919    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 4.29      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0711    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1326000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.745   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1327000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0013
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0442    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0745   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1328000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.193   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.034   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1329000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.08     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0668   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1330000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.00392  |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 10.6      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0247   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1331000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0155
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.395    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 7.43     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.00877 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1332000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.984    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.00207 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1333000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0182
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.347    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 3.51      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.116    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1334000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.565   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0985   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1335000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.362   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1336000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.75     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.3      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0565   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1337000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0118
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.00465   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.013     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1338000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.32      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0112   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1339000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.26      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0194   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1340000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -1.49    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0157   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1341000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.757    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0622  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1342000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.42     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0681   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1343000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.00302  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1344000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.968    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1345000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.652     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 11        |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0886   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1346000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0435
num rollout transitions: 250000, reward mean: 5.0253
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0916  |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1347000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0229
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.983    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00217  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1348000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.18      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 22.4      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00498   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1349000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.661     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00418   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1350000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0954  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0612  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1351000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.558    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0395   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1352000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.236   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0778  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1353000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.624    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1354000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.0115   |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0382    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1355000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.396     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.7      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0471    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1356000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.292   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0361   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1357000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.745    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0352   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1358000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 4.14      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0391   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1359000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.342   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00887  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1360000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.44e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.491    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.000775 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1361000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.165    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82       |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1362000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.2e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.474    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0669   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1363000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.518    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0362  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1364000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -1.08     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0159   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1365000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.92e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0239   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1366000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.327     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -705      |
| loss/alpha                         | -0.039    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1367000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.117   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0267  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1368000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.628    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0331   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1369000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0144
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.2      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0419    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1370000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.924     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0648    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1371000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.132   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.098   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1372000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.767    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.00618  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1373000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.33      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.076     |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1374000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-12 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.44     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0427  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1375000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.0736   |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -705      |
| loss/alpha                         | -0.0413   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1376000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.7e-11  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.597    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0404   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1377000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.422   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0381  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1378000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0205
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.268     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.3      |
| eval/normalized_episode_reward_std | 3.39      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.0959    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1379000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.559    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -705     |
| loss/alpha                         | 0.00113  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1380000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.101   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0633  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1381000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.115     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0758    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1382000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0161
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.344   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.69     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0598   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1383000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0224
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.567     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0868   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1384000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.316     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00613   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1385000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0604  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0204   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1386000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0329
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.305    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0783   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1387000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.304    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -707     |
| loss/alpha                         | -0.05    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1388000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.457    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0214   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1389000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.41      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.138     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1390000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.494    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.048    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1391000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0282
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.168   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1392000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.624    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -707     |
| loss/alpha                         | -0.0732  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1393000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.374    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.106    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1394000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.00188  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1395000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0374    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.016     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1396000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.81e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.423    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.47      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.07      |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1397000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0178
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.768    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0245  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1398000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.671    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1399000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.76e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.612     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.00857  |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1400000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0591    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 4.04      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0295    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1401000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.141    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0847   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1402000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-11  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0705   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0307   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1403000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.354   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1404000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.108     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0602   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1405000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.506    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0553   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1406000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.117    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 22        |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0421   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1407000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.557   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0478   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1408000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0222
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.228     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0888    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1409000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0142
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.35      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.073    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1410000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -1.04     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 18.5      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.00477  |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1411000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.292     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0151    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1412000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.228   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.00744  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1413000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.605    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -707      |
| loss/alpha                         | -0.00133  |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1414000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.425   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 4.22     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1415000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.636    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0698   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1416000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.104   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0286   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1417000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.416   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1418000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.358    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.00531  |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1419000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.325   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0255  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1420000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0325
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.791     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.00858  |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1421000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.789     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -709      |
| loss/alpha                         | 0.0551    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1422000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.373   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1423000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0705   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1424000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.391    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0194  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1425000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.235    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.00174  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1426000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0311
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.27e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.865     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0215   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1427000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.666    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 17.9      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0147    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1428000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.476     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.00045   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1429000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9993
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.434    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0506  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1430000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.171   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0492   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1431000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9982
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.609    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0414   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1432000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.715     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0235   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1433000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.58e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.139    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 20       |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0309   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1434000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.811    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 14.6     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.078   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1435000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0136
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.612    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0218    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1436000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.217    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1437000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.244    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0376   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1438000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.0884  |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0979   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1439000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0366    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0125   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1440000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.059   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1441000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.172    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0925  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1442000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.299    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0442   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1443000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0364
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.717     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.000353 |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1444000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.000795  |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 4.17      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0129    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1445000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.13e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.165     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0629   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1446000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.102     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0403    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1447000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0178
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.96e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0531  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1448000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.567    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.139    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1449000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.651    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.065     |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1450000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0131
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.555     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -708      |
| loss/alpha                         | -0.134    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1451000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.382    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0651    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1452000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.342    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.00422 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1453000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0789   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0343   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1454000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.885    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0351   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1455000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.419     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -709      |
| loss/alpha                         | -0.0852   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1456000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.86e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.188    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1457000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0199
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0493    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -709      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1458000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.0752  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.044    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1459000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.331    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1460000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0708  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.0111   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1461000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.523    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0271   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1462000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-12 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.973    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 15.6     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.0389   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1463000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.186    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.00349 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1464000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0320
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.975     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.00154  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1465000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.215     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.00322  |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1466000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-10   |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -1.07    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1467000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.128   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.000676 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1468000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.546    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -711     |
| loss/alpha                         | 0.129    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1469000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.03    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0702  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1470000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0237
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.801     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0406    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1471000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0595   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.11    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1472000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.453    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0307    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1473000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0424
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.256    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1474000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0408
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.134     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0402    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1475000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.366     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0458   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1476000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.591    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0468    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1477000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0198
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.338     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.00206  |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1478000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.1e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.195    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0263   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1479000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -1.24    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0092   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1480000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0275
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.59e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.357    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1481000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0332
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.634    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0972  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1482000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.365    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1483000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.504    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.128    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1484000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.0561   |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -711      |
| loss/alpha                         | 0.00217   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1485000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0097
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.213     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0536   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1486000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0634   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1487000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.51     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1488000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.637    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0459   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1489000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.143    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0334  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1490000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.822     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0398    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1491000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.659    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.00987 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1492000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.522   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 8.45     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0613  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1493000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0122
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.352     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0407    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1494000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.75     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0345   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1495000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0252
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.806     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0667   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1496000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.232    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0576  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1497000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.438    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0668   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1498000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0332
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.192    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00269 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1499000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.449    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0191   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1500000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0225
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.367     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0675    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1501000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.283    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0352   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1502000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0167
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.564     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0308   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1503000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.258    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0683   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1504000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.558     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0251   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1505000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.152    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0713   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1506000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0194
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.791     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0286    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1507000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.119    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0347  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1508000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.113   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0525   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1509000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0104
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.12      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.133    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1510000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.289    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0861   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1511000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.912     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0811    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1512000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.22e-12 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.195   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00469 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1513000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0475  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0525  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1514000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0136
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.12e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.173     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0215    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1515000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.248    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0987   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1516000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.0341   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0512   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1517000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.211    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0453   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1518000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.983    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0586   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1519000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.404    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0449   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1520000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0364    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0921   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1521000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0084
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.84e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.822     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0548   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1522000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0193
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.865     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.101     |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1523000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.395    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1524000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.138    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.8      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0653   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1525000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.0219   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0779   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1526000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.476    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0877  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1527000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.397    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00392  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1528000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.27e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.435    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0356   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1529000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.849    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0435   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1530000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0168
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.728     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0298    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1531000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0511  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1532000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.334    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0739  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1533000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.745    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0554   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1534000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.07      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0266   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1535000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.64e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.262    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1536000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.01      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0949    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1537000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.395    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0269    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1538000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.571   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1539000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0418   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0316  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1540000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.542    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0428  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1541000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0077
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.422    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0103    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1542000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0538    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0575    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1543000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.3e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.193    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.074    |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1544000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.261   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0811  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1545000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0182
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.33      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0131    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1546000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.00114  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1547000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.31e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.322    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0316   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1548000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.724   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00127  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1549000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.95e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.415     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 19.1      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0421   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1550000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.784     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1551000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.303    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.000689 |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1552000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0806    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0545    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1553000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0291
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1.37      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0571    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1554000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0275
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0112    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0262   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1555000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0749   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.113    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1556000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0175
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.902    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.128     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1557000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.17e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0796   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0631   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1558000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0246
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.357    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0409   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1559000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0361
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.29e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.266     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0768   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1560000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.0729  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -711     |
| loss/alpha                         | 0.00848  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1561000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0256
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.26     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0858   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1562000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.027    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0626   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1563000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.158    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1564000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.271   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1565000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.204    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0274   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1566000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.605    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00697 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1567000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.609     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0021   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1568000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0131
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.332     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0203    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1569000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.782    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -712      |
| loss/alpha                         | 0.016     |
| loss/critic1                       | 12        |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1570000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.615    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00737  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1571000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.265    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.057   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1572000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.206   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00888 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1573000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.191    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0294    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1574000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.037    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1575000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0185
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.121     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 22.7      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0849   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1576000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0350
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0492  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0931  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1577000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.657    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0177   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1578000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0689   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0265   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1579000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-12 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.191     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0328    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1580000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.4e-11  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0969   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0013   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1581000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.476    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0714    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1582000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0205
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.271     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.00125  |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1583000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.125    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.044     |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1584000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.163    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0911  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1585000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.955    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00777  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1586000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-11  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.182    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00216 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1587000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.224    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0454    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1588000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-11   |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.181    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0974   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1589000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.0373   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0276    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1590000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0193
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.366     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 14.1      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.00555   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1591000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.564    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0425   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1592000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.233    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1593000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0207
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.99e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.424    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.157    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1594000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.433     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0769   |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1595000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.395    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00692  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1596000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0925   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0344  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1597000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.386   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.152    |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1598000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.131    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0465  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1599000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.839    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0182   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1600000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0193
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.35     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0187   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1601000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.204    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0609   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1602000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0207
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.000617  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1603000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.321    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1604000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0234  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0774  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1605000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.00131   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0696   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1606000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.743   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0776   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1607000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.548   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.034    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1608000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.355   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0524  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1609000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.199    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1610000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.52    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0571  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1611000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.31e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.00969  |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.2      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0012    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1612000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.583   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.00984 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1613000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0669    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0183    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1614000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0200
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.126    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0868   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1615000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.357    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.000671 |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1616000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.228   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1617000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.639     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0959    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1618000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.363   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1619000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.979    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -712      |
| loss/alpha                         | 0.059     |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1620000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0350
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0524  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1621000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.183    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0378  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1622000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10   |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.187   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 9.25     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.051   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1623000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0323
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.688     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0874    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1624000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0343
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.425    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.09    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1625000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0411
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.617    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00962  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1626000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.553     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.000505  |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1627000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.735    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1628000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.777    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0249    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1629000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.945   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0402  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1630000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0124
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.0349   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0439   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1631000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.284   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00742  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1632000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.534    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00339 |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1633000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.366    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0397   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1634000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.09      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.102    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1635000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.119    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0795   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1636000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.123    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0102  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1637000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.176     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0113   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1638000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0241
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.105   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0419  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1639000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.945   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00731  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1640000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0506    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1641000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.534   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0316  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1642000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0076
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.438     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0312   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1643000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0250
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0745    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0068    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1644000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.437   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0625   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1645000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0327
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.09      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0205   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1646000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.345    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1647000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.388     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.00779   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1648000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.63e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0682    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1649000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.173   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.082   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1650000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.82    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0556   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1651000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0299
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.388   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0957   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1652000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0218
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.577     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0127    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1653000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.374   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00218  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1654000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.823    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00535  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1655000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.398    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1656000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.313    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0822  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1657000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0332
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.193    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1658000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0283
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.044    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0129    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1659000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.486    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0375   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1660000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.3      |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00461  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1661000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0729  |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1662000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0279
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.452     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0439   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1663000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.786     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0296    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1664000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0282
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.665     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0599    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1665000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.81     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0566   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1666000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0793    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 11.5      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0235   |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1667000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.468    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0159    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1668000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.555   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0488   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1669000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0351
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.27      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.00208   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1670000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.645     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0249    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1671000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.958   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0775  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1672000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0992  |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0283   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1673000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.46     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0263  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1674000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.549    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.00845  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1675000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.402    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0206   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1676000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.745    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1677000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0248
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.274     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.022     |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1678000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.0858    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.114     |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1679000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.141   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0151   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1680000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.435    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0819  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1681000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0002
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.899    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0373   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1682000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0236
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.68     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0142   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1683000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.67e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.369     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0102   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1684000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.63     |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.00934 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1685000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0819  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1686000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.331    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 13.8     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0112   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1687000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.19e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.485     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0437    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1688000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0316
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.606    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.48      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0928    |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1689000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0744   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0252   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1690000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0271
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.317     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.047     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1691000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.61      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.00807   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1692000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.327    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0177   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1693000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.911     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0691    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1694000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.526   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0578  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1695000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.951    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.55     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0364  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1696000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.236    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0339   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1697000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.128    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0608   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1698000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.486    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0099   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1699000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.105     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0345   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1700000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.276   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0578   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1701000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0118
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.66e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.192     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0221   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1702000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0914   |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0851   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1703000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.11      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0578   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1704000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.772    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0409    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1705000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0325
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.342   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1706000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.926    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1707000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.444     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0367    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1708000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0247
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.39      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 9.19      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0163    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1709000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.591    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.01    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1710000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.484    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0109   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1711000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0351
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.041    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1712000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0983  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0552  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1713000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.347   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0314  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1714000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0291
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.92     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0921   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1715000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.806    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1716000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0131
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.905     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0631    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1717000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.344   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.00107  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1718000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.872    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.087   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1719000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.974     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0256    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1720000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0982   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1721000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.00662 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1722000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0354
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.0866    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.00163  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1723000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0226
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.298    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0609   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1724000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.08e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.49     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0615    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1725000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.418   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0881  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1726000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.257     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0437   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1727000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.502    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0509   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1728000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.339    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0533    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1729000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0021
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0882   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0429   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1730000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.172    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1731000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0992   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0473  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1732000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.726     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 5.3       |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0119   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1733000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.378    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.00947 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1734000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.33    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0281   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1735000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0506  |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0692   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1736000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.192   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1737000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.102     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0387    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1738000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.374   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0547   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1739000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.958    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0824  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1740000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.552    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0393   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1741000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.762   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0921  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1742000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0405   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0263   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1743000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.246    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.147     |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1744000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.85e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.512    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0157    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1745000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0318
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.224     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.124    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1746000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.074    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0152  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1747000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.366   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0628   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1748000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.00111  |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0551   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1749000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.16      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -716      |
| loss/alpha                         | 0.095     |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1750000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0280
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.0924   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0349  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1751000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.872    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0399  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1752000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0269
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.136    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0395   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1753000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.808    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1754000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0124
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.83      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.111     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1755000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.8e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.138   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0178   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1756000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0367
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.17      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0434   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1757000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.147     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0539   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1758000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0334
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.293    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1759000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.324   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0659  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1760000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0397
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0765    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1761000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.861     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0225    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1762000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.871    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0532   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1763000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0232
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.62e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.144     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0476    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1764000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.408     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0295   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1765000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0267
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.00273   |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.044    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1766000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0084
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.345    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0146    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1767000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0241
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.68e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.515     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0845   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1768000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.463    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00259 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1769000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.297   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.64     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0932   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1770000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.9e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.0322   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0489   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1771000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.43     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1772000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.2     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1773000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0242
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4e-10    |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.872    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00113 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1774000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.942   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1775000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -1.1     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0476   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1776000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -1.28    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0418   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1777000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1778000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.227   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.088   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1779000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.922   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00565  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1780000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.506   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00331 |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1781000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.805    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1782000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.13     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1783000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0179
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.321    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 5.71      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0156   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1784000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.447    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0392    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1785000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.197    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1786000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.465   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1787000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0200
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.317     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0709    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1788000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0288
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.57      |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.00239  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1789000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.228    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00504  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1790000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.35e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.56      |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0612    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1791000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.355     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0754   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1792000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.646   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.111   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1793000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0181
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.02     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1794000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.8e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.593    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0542   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1795000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.171    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0375    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1796000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.261   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 4.3      |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0371  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1797000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.825     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0471   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1798000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.767    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -715     |
| loss/alpha                         | 0.00568  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1799000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0226
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.768    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0109   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1800000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.462     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0355    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1801000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0617    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 18.7      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0341    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1802000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.785    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0517  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1803000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.555     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0269    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1804000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.00297  |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00815   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1805000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0208
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.184    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0252   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1806000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0196
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.36      |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0537    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1807000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0159
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.66      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0555   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1808000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.223    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1809000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0240
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.287     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0468    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1810000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.866     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0347    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1811000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.681    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0941   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1812000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.0637    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0465   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1813000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.459    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1814000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0149
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.157     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.00199  |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1815000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.403    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 5.84     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0175  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1816000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.416   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0344   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1817000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -1.11    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0587   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1818000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.00791  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.018   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1819000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0292  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1820000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0363
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.617     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.017    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1821000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.364    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0118   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1822000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.0587    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0351   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1823000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0215
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0661   |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0554    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1824000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.258    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0283   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1825000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.489    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1826000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0155
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.204    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00381  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1827000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.11     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0594   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1828000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0417
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.08e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.248    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.000841 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1829000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.261   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00253  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1830000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.94e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.747    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 8.36      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0179    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1831000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.783    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1832000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0215
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.269     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0159   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1833000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.188    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0339   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1834000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.813    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0409   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1835000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0257
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0334    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1836000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.265    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0492    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1837000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.71e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.888     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00482  |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1838000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0717  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1839000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -1.07     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0697    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1840000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.694    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1841000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.732     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0556   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1842000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0313
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.229    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 8.31      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0576    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1843000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.119   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0656   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1844000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.774     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0302   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1845000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0184
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.177    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.001    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1846000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.84     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0351  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1847000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.13      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0632   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1848000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.0149  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0115   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1849000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.567    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0244    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1850000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.41e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.419    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0741   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1851000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.733     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0141    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1852000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0253
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.115    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0553   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1853000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.12     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0347   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1854000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.929    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0131   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1855000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0274
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.563    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00133   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1856000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0310
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.784   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0448  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1857000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.173     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0162   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1858000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.902    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0828   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1859000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.14    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0582  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1860000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0362
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.264    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0875   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1861000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.00479  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00598  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1862000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1863000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.498    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0287   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1864000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.42      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0126   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1865000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.209    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1866000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.686   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0193  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1867000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0311    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1868000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0287
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.7      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0135    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1869000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0251
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.418    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0913   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1870000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0335
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.276     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0717    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1871000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0179
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.827    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0388    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1872000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.17    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.113   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1873000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0218
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.36     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0512   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1874000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0157
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.214    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00583   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1875000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.786     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0374    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1876000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0299
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.958    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00367  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1877000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.158   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0506  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1878000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10   |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.792   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0467   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1879000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.703     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.053    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1880000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.22     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0541  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1881000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.323    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0181   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1882000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0361
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-12 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1883000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.599    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0707  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1884000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0205
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.268    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0439    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1885000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.321    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0287    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1886000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.38      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.026    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1887000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.472    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0268  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1888000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.259    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0863   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1889000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0454
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1890000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.474    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00155  |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1891000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-12 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0817   |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00886   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1892000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.25      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 8.28      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0182    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1893000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.327    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0567  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1894000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.547     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0178   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1895000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.525    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0241    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1896000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0781    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.112     |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1897000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0170
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.476     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0571   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1898000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-12  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.405    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1899000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.261    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0418  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1900000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.539     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0348   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1901000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.0122   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00745  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1902000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.294    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0708  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1903000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.595    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0203  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1904000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.96e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.269   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0939   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1905000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.404   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.52     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0599   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1906000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.0669   |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00231   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1907000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.321     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.049     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1908000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0616   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1909000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0274
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.438    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0211   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1910000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.316    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00386   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1911000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.742    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0343  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1912000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.32     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0944  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1913000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.617    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1914000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0179
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.268     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0318    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1915000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.373   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0591   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1916000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0291
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.567    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0604   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1917000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0218
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.0472  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0123  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1918000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.711    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0462    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1919000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0416
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.245   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0435  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1920000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.486     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0425   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1921000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.085    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0431   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1922000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.774     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.056    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1923000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.299     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.066     |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1924000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.304    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00527  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1925000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.279    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0827   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1926000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0042
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.261     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.112    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1927000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1.2       |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0188    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1928000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.157   |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1929000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.257    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0334    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1930000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0193
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.24e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.621    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00803  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1931000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.51e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.134     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0253   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1932000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0300
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.622     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0226   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1933000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0265
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.0182   |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1934000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.00709   |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0171   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1935000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.39      |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0277   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1936000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.049    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0388   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1937000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.0285  |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0495   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1938000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.587    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0285   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1939000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -1.23    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0406  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1940000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0315
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.161     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0164   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1941000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.0118  |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0163   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1942000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.41      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00797   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1943000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0402
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.047   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0552   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1944000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0296
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.172    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0684   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1945000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0196
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.034     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00157   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1946000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.909     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0811   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1947000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.39      |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0496   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1948000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.23     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1949000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.483    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0894   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1950000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0257
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.24e-11  |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.000574 |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0659   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1951000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.225    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0176   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1952000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0663  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1953000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.43e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.84     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0467   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1954000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0265
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.0769    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0614    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1955000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0207
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.477    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0807   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1956000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00208  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1957000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.36     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00487  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1958000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.0434  |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0702  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1959000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.0223   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0793  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1960000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.279    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0791  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1961000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.361   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1962000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.0578   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.165    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1963000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.8e-11  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.926    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0732   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1964000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.742    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0335  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1965000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.126    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0986   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1966000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.151    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0544   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1967000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.31e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0737   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1968000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.328     |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00505   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1969000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0242
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.524    |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.053   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1970000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0189   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00511 |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1971000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0273
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.153     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.000988  |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1972000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0173
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.463     |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00717   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1973000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.707    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0157    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1974000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.0692  |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0181  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1975000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.8e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.0207  |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0814   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1976000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0195
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.0124   |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0562    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1977000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.648    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1978000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.492    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0456  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1979000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.88e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.963    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0492  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1980000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.0392   |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0163    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1981000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.366   |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1982000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0421
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.446     |
| adv_dynamics_update/all_loss       | -57.3     |
| adv_dynamics_update/sl_loss        | -57.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0643    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1983000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.469    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.1     |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1984000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.887     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0117    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1985000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0082
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.221    |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0379    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1986000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.211    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0253   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1987000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.835    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0629  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1988000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0294
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.324    |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 13.7      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0591   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1989000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.268    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.00354  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1990000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.317    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0488   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1991000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0393
num rollout transitions: 250000, reward mean: 5.0422
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.327     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0309    |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1992000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0393
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.31e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.0693  |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.033   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1993000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.537    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0955   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1994000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.271    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.035   |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1995000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0389
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.363    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0317   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1996000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0235
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0797    |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.00991   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1997000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.47     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0223   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1998000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0152
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.124    |
| adv_dynamics_update/all_loss       | -57.2     |
| adv_dynamics_update/sl_loss        | -57.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0523    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1999000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.675    |
| adv_dynamics_update/all_loss       | -57.2    |
| adv_dynamics_update/sl_loss        | -57.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 2000000  |
----------------------------------------------------------------------------------
total time: 127713.07s
