Pretraining policy
Training dynamics:
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.061806727 |
| loss/dynamics_train_loss   | -18.6       |
| timestep                   | 1           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.045140054 |
| loss/dynamics_train_loss   | -28.6       |
| timestep                   | 2           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03907121 |
| loss/dynamics_train_loss   | -31.7      |
| timestep                   | 3          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.035490252 |
| loss/dynamics_train_loss   | -33.5       |
| timestep                   | 4           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03265595 |
| loss/dynamics_train_loss   | -34.8      |
| timestep                   | 5          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.030882657 |
| loss/dynamics_train_loss   | -35.8       |
| timestep                   | 6           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.029252183 |
| loss/dynamics_train_loss   | -36.7       |
| timestep                   | 7           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028231299 |
| loss/dynamics_train_loss   | -37.4       |
| timestep                   | 8           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02728944 |
| loss/dynamics_train_loss   | -38        |
| timestep                   | 9          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026547218 |
| loss/dynamics_train_loss   | -38.6       |
| timestep                   | 10          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026018288 |
| loss/dynamics_train_loss   | -39         |
| timestep                   | 11          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025233438 |
| loss/dynamics_train_loss   | -39.5       |
| timestep                   | 12          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024810249 |
| loss/dynamics_train_loss   | -39.9       |
| timestep                   | 13          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024308205 |
| loss/dynamics_train_loss   | -40.2       |
| timestep                   | 14          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023962347 |
| loss/dynamics_train_loss   | -40.6       |
| timestep                   | 15          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023425551 |
| loss/dynamics_train_loss   | -40.9       |
| timestep                   | 16          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023159342 |
| loss/dynamics_train_loss   | -41.1       |
| timestep                   | 17          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022880642 |
| loss/dynamics_train_loss   | -41.4       |
| timestep                   | 18          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022537945 |
| loss/dynamics_train_loss   | -41.7       |
| timestep                   | 19          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022197112 |
| loss/dynamics_train_loss   | -41.9       |
| timestep                   | 20          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021984816 |
| loss/dynamics_train_loss   | -42.1       |
| timestep                   | 21          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021625403 |
| loss/dynamics_train_loss   | -42.3       |
| timestep                   | 22          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02133817 |
| loss/dynamics_train_loss   | -42.5      |
| timestep                   | 23         |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0210959 |
| loss/dynamics_train_loss   | -42.7     |
| timestep                   | 24        |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020912012 |
| loss/dynamics_train_loss   | -42.9       |
| timestep                   | 25          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020579426 |
| loss/dynamics_train_loss   | -43.1       |
| timestep                   | 26          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020407192 |
| loss/dynamics_train_loss   | -43.2       |
| timestep                   | 27          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02003296 |
| loss/dynamics_train_loss   | -43.4      |
| timestep                   | 28         |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01999944 |
| loss/dynamics_train_loss   | -43.5      |
| timestep                   | 29         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019836282 |
| loss/dynamics_train_loss   | -43.7       |
| timestep                   | 30          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01965204 |
| loss/dynamics_train_loss   | -43.8      |
| timestep                   | 31         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019451175 |
| loss/dynamics_train_loss   | -44         |
| timestep                   | 32          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019366976 |
| loss/dynamics_train_loss   | -44.1       |
| timestep                   | 33          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019162849 |
| loss/dynamics_train_loss   | -44.3       |
| timestep                   | 34          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01915247 |
| loss/dynamics_train_loss   | -44.4      |
| timestep                   | 35         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018960198 |
| loss/dynamics_train_loss   | -44.5       |
| timestep                   | 36          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018948328 |
| loss/dynamics_train_loss   | -44.6       |
| timestep                   | 37          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018634679 |
| loss/dynamics_train_loss   | -44.7       |
| timestep                   | 38          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018440226 |
| loss/dynamics_train_loss   | -44.9       |
| timestep                   | 39          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01846287 |
| loss/dynamics_train_loss   | -45        |
| timestep                   | 40         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018164963 |
| loss/dynamics_train_loss   | -45.1       |
| timestep                   | 41          |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0181447 |
| loss/dynamics_train_loss   | -45.2     |
| timestep                   | 42        |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017941358 |
| loss/dynamics_train_loss   | -45.3       |
| timestep                   | 43          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017838798 |
| loss/dynamics_train_loss   | -45.4       |
| timestep                   | 44          |
-----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017694 |
| loss/dynamics_train_loss   | -45.5    |
| timestep                   | 45       |
--------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017500669 |
| loss/dynamics_train_loss   | -45.6       |
| timestep                   | 46          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017535448 |
| loss/dynamics_train_loss   | -45.7       |
| timestep                   | 47          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017476704 |
| loss/dynamics_train_loss   | -45.8       |
| timestep                   | 48          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017374367 |
| loss/dynamics_train_loss   | -45.9       |
| timestep                   | 49          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017279178 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 50          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016977856 |
| loss/dynamics_train_loss   | -46.1       |
| timestep                   | 51          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017050402 |
| loss/dynamics_train_loss   | -46.2       |
| timestep                   | 52          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016935429 |
| loss/dynamics_train_loss   | -46.2       |
| timestep                   | 53          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016936522 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 54          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016865183 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 55          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016646655 |
| loss/dynamics_train_loss   | -46.5       |
| timestep                   | 56          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016642919 |
| loss/dynamics_train_loss   | -46.6       |
| timestep                   | 57          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01662332 |
| loss/dynamics_train_loss   | -46.7      |
| timestep                   | 58         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016432405 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 59          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01631631 |
| loss/dynamics_train_loss   | -46.8      |
| timestep                   | 60         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016241862 |
| loss/dynamics_train_loss   | -46.9       |
| timestep                   | 61          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016033847 |
| loss/dynamics_train_loss   | -47         |
| timestep                   | 62          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016136011 |
| loss/dynamics_train_loss   | -47         |
| timestep                   | 63          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015996631 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 64          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01596957 |
| loss/dynamics_train_loss   | -47.2      |
| timestep                   | 65         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015924957 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 66          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01585776 |
| loss/dynamics_train_loss   | -47.3      |
| timestep                   | 67         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015583808 |
| loss/dynamics_train_loss   | -47.4       |
| timestep                   | 68          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015629811 |
| loss/dynamics_train_loss   | -47.4       |
| timestep                   | 69          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015434414 |
| loss/dynamics_train_loss   | -47.5       |
| timestep                   | 70          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015647369 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 71          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015541365 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 72          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015291599 |
| loss/dynamics_train_loss   | -47.7       |
| timestep                   | 73          |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0154048 |
| loss/dynamics_train_loss   | -47.7     |
| timestep                   | 74        |
---------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0153060015 |
| loss/dynamics_train_loss   | -47.8        |
| timestep                   | 75           |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015172566 |
| loss/dynamics_train_loss   | -47.9       |
| timestep                   | 76          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014990498 |
| loss/dynamics_train_loss   | -47.9       |
| timestep                   | 77          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014949756 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 78          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015298383 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 79          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014759585 |
| loss/dynamics_train_loss   | -48.1       |
| timestep                   | 80          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014910648 |
| loss/dynamics_train_loss   | -48.1       |
| timestep                   | 81          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01474126 |
| loss/dynamics_train_loss   | -48.2      |
| timestep                   | 82         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014868358 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 83          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014829581 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 84          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014710436 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 85          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014587221 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 86          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014449482 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 87          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014408509 |
| loss/dynamics_train_loss   | -48.5       |
| timestep                   | 88          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014362305 |
| loss/dynamics_train_loss   | -48.5       |
| timestep                   | 89          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014198805 |
| loss/dynamics_train_loss   | -48.6       |
| timestep                   | 90          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014182475 |
| loss/dynamics_train_loss   | -48.6       |
| timestep                   | 91          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01418412 |
| loss/dynamics_train_loss   | -48.7      |
| timestep                   | 92         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013977167 |
| loss/dynamics_train_loss   | -48.7       |
| timestep                   | 93          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01390021 |
| loss/dynamics_train_loss   | -48.7      |
| timestep                   | 94         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013897074 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 95          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014108887 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 96          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013902957 |
| loss/dynamics_train_loss   | -48.9       |
| timestep                   | 97          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013825916 |
| loss/dynamics_train_loss   | -48.9       |
| timestep                   | 98          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013693956 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 99          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013555311 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 100         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013641966 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 101         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013673333 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 102         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01364253 |
| loss/dynamics_train_loss   | -49.1      |
| timestep                   | 103        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013533394 |
| loss/dynamics_train_loss   | -49.2       |
| timestep                   | 104         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013692565 |
| loss/dynamics_train_loss   | -49.2       |
| timestep                   | 105         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013410556 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 106         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013344456 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 107         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0132249165 |
| loss/dynamics_train_loss   | -49.3        |
| timestep                   | 108          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013343111 |
| loss/dynamics_train_loss   | -49.4       |
| timestep                   | 109         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013186103 |
| loss/dynamics_train_loss   | -49.4       |
| timestep                   | 110         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0131318215 |
| loss/dynamics_train_loss   | -49.4        |
| timestep                   | 111          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01320014 |
| loss/dynamics_train_loss   | -49.5      |
| timestep                   | 112        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013142848 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 113         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01309784 |
| loss/dynamics_train_loss   | -49.6      |
| timestep                   | 114        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01294207 |
| loss/dynamics_train_loss   | -49.6      |
| timestep                   | 115        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01290863 |
| loss/dynamics_train_loss   | -49.6      |
| timestep                   | 116        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012860298 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 117         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012977002 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 118         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012747797 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 119         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0127546415 |
| loss/dynamics_train_loss   | -49.8        |
| timestep                   | 120          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012780117 |
| loss/dynamics_train_loss   | -49.8       |
| timestep                   | 121         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012838773 |
| loss/dynamics_train_loss   | -49.8       |
| timestep                   | 122         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012706858 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 123         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012626779 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 124         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012787911 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 125         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012550841 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 126         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012619905 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 127         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012617705 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 128         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012483956 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 129         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012764394 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 130         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012409801 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 131         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0124054225 |
| loss/dynamics_train_loss   | -50.1        |
| timestep                   | 132          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012363937 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 133         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0123538505 |
| loss/dynamics_train_loss   | -50.2        |
| timestep                   | 134          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012247684 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 135         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012235676 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 136         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012378955 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 137         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012230585 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 138         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012336639 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 139         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012103895 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 140         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012262258 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 141         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012207115 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 142         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012159421 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 143         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01214429 |
| loss/dynamics_train_loss   | -50.5      |
| timestep                   | 144        |
----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0118880635 |
| loss/dynamics_train_loss   | -50.5        |
| timestep                   | 145          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011977647 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 146         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01200862 |
| loss/dynamics_train_loss   | -50.5      |
| timestep                   | 147        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01191889 |
| loss/dynamics_train_loss   | -50.5      |
| timestep                   | 148        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011928916 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 149         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011897598 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 150         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011871705 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 151         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011932119 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 152         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011808641 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 153         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011778688 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 154         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011785957 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 155         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0117319245 |
| loss/dynamics_train_loss   | -50.8        |
| timestep                   | 156          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011830858 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 157         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011624309 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 158         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011782523 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 159         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011535372 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 160         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011634286 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 161         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011701388 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 162         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011723764 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 163         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011567816 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 164         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011766752 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 165         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011544442 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 166         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011637351 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 167         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011519685 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 168         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011580192 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 169         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011471243 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 170         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011568261 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 171         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011409467 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 172         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011544761 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 173         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01135281 |
| loss/dynamics_train_loss   | -51.1      |
| timestep                   | 174        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011546031 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 175         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011395433 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 176         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011437563 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 177         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011341674 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 178         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011624935 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 179         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011320761 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 180         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011341589 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 181         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011331683 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 182         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011278755 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 183         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011279477 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 184         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011174096 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 185         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01128256 |
| loss/dynamics_train_loss   | -51.4      |
| timestep                   | 186        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011187162 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 187         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011282266 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 188         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011150758 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 189         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011332892 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 190         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011132913 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 191         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011144256 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 192         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0111749 |
| loss/dynamics_train_loss   | -51.5     |
| timestep                   | 193       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011186694 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 194         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0111478735 |
| loss/dynamics_train_loss   | -51.5        |
| timestep                   | 195          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011123406 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 196         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011306581 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 197         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011149129 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 198         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010972519 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 199         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011045548 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 200         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01110612 |
| loss/dynamics_train_loss   | -51.6      |
| timestep                   | 201        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011089841 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 202         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011058329 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 203         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01095551 |
| loss/dynamics_train_loss   | -51.7      |
| timestep                   | 204        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01102698 |
| loss/dynamics_train_loss   | -51.7      |
| timestep                   | 205        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010906858 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 206         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010876827 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 207         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0109097585 |
| loss/dynamics_train_loss   | -51.7        |
| timestep                   | 208          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010970447 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 209         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010951246 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 210         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010772749 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 211         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010914127 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 212         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010683449 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 213         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010750191 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 214         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010871154 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 215         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010837156 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 216         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010925914 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 217         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010808495 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 218         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0108364085 |
| loss/dynamics_train_loss   | -51.9        |
| timestep                   | 219          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010798071 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 220         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010717006 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 221         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010805349 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 222         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01080381 |
| loss/dynamics_train_loss   | -52        |
| timestep                   | 223        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01083913 |
| loss/dynamics_train_loss   | -52        |
| timestep                   | 224        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010806446 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 225         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010692011 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 226         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010759404 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 227         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010618827 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 228         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01056375 |
| loss/dynamics_train_loss   | -52.1      |
| timestep                   | 229        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010546679 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 230         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01066813 |
| loss/dynamics_train_loss   | -52.1      |
| timestep                   | 231        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010690961 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 232         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01073496 |
| loss/dynamics_train_loss   | -52.1      |
| timestep                   | 233        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010577036 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 234         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010604578 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 235         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010669628 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 236         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010575605 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 237         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010765527 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 238         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010564545 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 239         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010427786 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 240         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010620893 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 241         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010745658 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 242         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010488306 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 243         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010539454 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 244         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010431945 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 245         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010457992 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 246         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010494519 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 247         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010546893 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 248         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010301941 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 249         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010358413 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 250         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0104579395 |
| loss/dynamics_train_loss   | -52.4        |
| timestep                   | 251          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010538811 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 252         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010284312 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 253         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010299079 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 254         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01022696 |
| loss/dynamics_train_loss   | -52.4      |
| timestep                   | 255        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010240851 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 256         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010258675 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 257         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010205883 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 258         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01034336 |
| loss/dynamics_train_loss   | -52.5      |
| timestep                   | 259        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010257928 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 260         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0102685345 |
| loss/dynamics_train_loss   | -52.5        |
| timestep                   | 261          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01020032 |
| loss/dynamics_train_loss   | -52.5      |
| timestep                   | 262        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010232218 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 263         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010234768 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 264         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010326372 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 265         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010286302 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 266         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0102516245 |
| loss/dynamics_train_loss   | -52.6        |
| timestep                   | 267          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010195895 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 268         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010058188 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 269         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01020213 |
| loss/dynamics_train_loss   | -52.6      |
| timestep                   | 270        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010203974 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 271         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010125285 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 272         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010029246 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 273         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009956667 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 274         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010169246 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 275         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010210801 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 276         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010008691 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 277         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009966543 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 278         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009964702 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 279         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009964384 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 280         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010077483 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 281         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009964822 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 282         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00988389 |
| loss/dynamics_train_loss   | -52.8      |
| timestep                   | 283        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009919298 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 284         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009935252 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 285         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0101330485 |
| loss/dynamics_train_loss   | -52.8        |
| timestep                   | 286          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009841533 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 287         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009882322 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 288         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009861393 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 289         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009871034 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 290         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009854345 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 291         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009703591 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 292         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00980772 |
| loss/dynamics_train_loss   | -52.9      |
| timestep                   | 293        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009932399 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 294         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009642537 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 295         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009774015 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 296         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009695068 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 297         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009728944 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 298         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009851951 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 299         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0097889025 |
| loss/dynamics_train_loss   | -53          |
| timestep                   | 300          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00984735 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 301        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009882869 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 302         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009646611 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 303         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00974225 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 304        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009738961 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 305         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00978416 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 306        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009600227 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 307         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009957311 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 308         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009594453 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 309         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009754149 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 310         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009846495 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 311         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009838558 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 312         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00960956 |
| loss/dynamics_train_loss   | -53.1      |
| timestep                   | 313        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009805411 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 314         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009720783 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 315         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009851978 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 316         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009790936 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 317         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009584954 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 318         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00957945 |
| loss/dynamics_train_loss   | -53.2      |
| timestep                   | 319        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009519307 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 320         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009864591 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 321         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009766401 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 322         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009565104 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 323         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0095775 |
| loss/dynamics_train_loss   | -53.2     |
| timestep                   | 324       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009617671 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 325         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009820295 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 326         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009742689 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 327         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009818603 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 328         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009786051 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 329         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0096631665 |
| loss/dynamics_train_loss   | -53.3        |
| timestep                   | 330          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009623548 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 331         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009599134 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 332         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009558329 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 333         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009577768 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 334         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009705004 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 335         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009762183 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 336         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009440012 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 337         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009680162 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 338         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009779449 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 339         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009544445 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 340         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009567311 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 341         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009700833 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 342         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009514725 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 343         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009591816 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 344         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009538794 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 345         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009762559 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 346         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009663366 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 347         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009505301 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 348         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009797034 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 349         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009339658 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 350         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009343522 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 351         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009551627 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 352         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009707743 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 353         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009383472 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 354         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009462684 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 355         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009432755 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 356         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009479939 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 357         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009354574 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 358         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009424189 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 359         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009462375 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 360         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009395589 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 361         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009451402 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 362         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009322048 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 363         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009301398 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 364         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009744025 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 365         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0093907835 |
| loss/dynamics_train_loss   | -53.6        |
| timestep                   | 366          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009355903 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 367         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009459354 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 368         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009626904 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 369         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009376362 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 370         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009529398 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 371         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009310721 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 372         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009490704 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 373         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00936235 |
| loss/dynamics_train_loss   | -53.7      |
| timestep                   | 374        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009280178 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 375         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009266581 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 376         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009365163 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 377         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009421264 |
| loss/dynamics_train_loss   | -53.7       |
| timestep                   | 378         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00927573 |
| loss/dynamics_train_loss   | -53.8      |
| timestep                   | 379        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009417626 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 380         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009308359 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 381         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00925691 |
| loss/dynamics_train_loss   | -53.8      |
| timestep                   | 382        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009377976 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 383         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009187469 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 384         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009295783 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 385         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009280628 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 386         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009056179 |
| loss/dynamics_train_loss   | -53.8       |
| timestep                   | 387         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0093039395 |
| loss/dynamics_train_loss   | -53.8        |
| timestep                   | 388          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00924341 |
| loss/dynamics_train_loss   | -53.9      |
| timestep                   | 389        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00914852 |
| loss/dynamics_train_loss   | -53.9      |
| timestep                   | 390        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009142091 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 391         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009117397 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 392         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009115284 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 393         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009134358 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 394         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009383203 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 395         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009129391 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 396         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009247042 |
| loss/dynamics_train_loss   | -53.9       |
| timestep                   | 397         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0091835065 |
| loss/dynamics_train_loss   | -53.9        |
| timestep                   | 398          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009076817 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 399         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009120842 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 400         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009088404 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 401         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009107791 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 402         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009289181 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 403         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00902259 |
| loss/dynamics_train_loss   | -54        |
| timestep                   | 404        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.00925972 |
| loss/dynamics_train_loss   | -54        |
| timestep                   | 405        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009038555 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 406         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009141371 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 407         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.008959535 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 408         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.008916912 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 409         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.008989745 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 410         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0091364 |
| loss/dynamics_train_loss   | -54       |
| timestep                   | 411       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009182657 |
| loss/dynamics_train_loss   | -54.1       |
| timestep                   | 412         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009070059 |
| loss/dynamics_train_loss   | -54         |
| timestep                   | 413         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.009314829 |
| loss/dynamics_train_loss   | -54.1       |
| timestep                   | 414         |
-----------------------------------------------------------------------------
elites:[4, 0, 3, 6, 2] , holdout loss: 0.008762553334236145
num rollout transitions: 250000, reward mean: 4.7950
num rollout transitions: 250000, reward mean: 5.0483
num rollout transitions: 250000, reward mean: 4.8853
num rollout transitions: 250000, reward mean: 4.9126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.77e-09 |
| adv_dynamics_update/adv_log_prob   | 29.6     |
| adv_dynamics_update/adv_loss       | 1.66     |
| adv_dynamics_update/all_loss       | -52.9    |
| adv_dynamics_update/sl_loss        | -52.9    |
| alpha                              | 0.999    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.23     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | 6.79     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 27.2     |
| loss/critic2                       | 26.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9215
num rollout transitions: 250000, reward mean: 4.9212
num rollout transitions: 250000, reward mean: 4.9232
num rollout transitions: 250000, reward mean: 4.9202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-09 |
| adv_dynamics_update/adv_log_prob   | 28.3     |
| adv_dynamics_update/adv_loss       | 4.12     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.943    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.12     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -38.5    |
| loss/alpha                         | -0.541   |
| loss/critic1                       | 2.96     |
| loss/critic2                       | 3.02     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9178
num rollout transitions: 250000, reward mean: 4.9044
num rollout transitions: 250000, reward mean: 4.9122
num rollout transitions: 250000, reward mean: 4.9007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-11 |
| adv_dynamics_update/adv_log_prob   | 30.2     |
| adv_dynamics_update/adv_loss       | 2.97     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.861    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.11     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -67.7    |
| loss/alpha                         | -1.37    |
| loss/critic1                       | 4.06     |
| loss/critic2                       | 3.98     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8985
num rollout transitions: 250000, reward mean: 4.8980
num rollout transitions: 250000, reward mean: 4.8879
num rollout transitions: 250000, reward mean: 4.8906
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 33.6      |
| adv_dynamics_update/adv_loss       | 2.68      |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.783     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 2.18      |
| eval/normalized_episode_reward_std | 2.26      |
| loss/actor                         | -91.9     |
| loss/alpha                         | -2.15     |
| loss/critic1                       | 6.48      |
| loss/critic2                       | 6.32      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 4000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8874
num rollout transitions: 250000, reward mean: 4.8710
num rollout transitions: 250000, reward mean: 4.8690
num rollout transitions: 250000, reward mean: 4.8488
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.98e-10 |
| adv_dynamics_update/adv_log_prob   | 36.4      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.712     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 2.8       |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -112      |
| loss/alpha                         | -2.79     |
| loss/critic1                       | 9.32      |
| loss/critic2                       | 9.12      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 5000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8550
num rollout transitions: 250000, reward mean: 4.8583
num rollout transitions: 250000, reward mean: 4.8588
num rollout transitions: 250000, reward mean: 4.8407
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 38.5      |
| adv_dynamics_update/adv_loss       | 2.15      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.648     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 0.733     |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -128      |
| loss/alpha                         | -3.26     |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.85      |
| timestep                           | 6000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8464
num rollout transitions: 250000, reward mean: 4.8321
num rollout transitions: 250000, reward mean: 4.8446
num rollout transitions: 250000, reward mean: 4.8197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 40.3     |
| adv_dynamics_update/adv_loss       | 1.93     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.592    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2        |
| eval/normalized_episode_reward_std | 4.45     |
| loss/actor                         | -142     |
| loss/alpha                         | -3.58    |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.84     |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8124
num rollout transitions: 250000, reward mean: 4.8251
num rollout transitions: 250000, reward mean: 4.8431
num rollout transitions: 250000, reward mean: 4.8424
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-10 |
| adv_dynamics_update/adv_log_prob   | 41.5      |
| adv_dynamics_update/adv_loss       | 0.991     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.54      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | -0.872    |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -154      |
| loss/alpha                         | -3.9      |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.83      |
| timestep                           | 8000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8482
num rollout transitions: 250000, reward mean: 4.8764
num rollout transitions: 250000, reward mean: 4.8722
num rollout transitions: 250000, reward mean: 4.8671
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.78e-10 |
| adv_dynamics_update/adv_log_prob   | 42.4      |
| adv_dynamics_update/adv_loss       | 1.1       |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.493     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 4.79      |
| eval/normalized_episode_reward_std | 8.08      |
| loss/actor                         | -165      |
| loss/alpha                         | -4.11     |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 9000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8667
num rollout transitions: 250000, reward mean: 4.8866
num rollout transitions: 250000, reward mean: 4.9066
num rollout transitions: 250000, reward mean: 4.8606
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 42.4      |
| adv_dynamics_update/adv_loss       | 0.806     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.45      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 3.49      |
| eval/normalized_episode_reward_std | 12.6      |
| loss/actor                         | -175      |
| loss/alpha                         | -4.21     |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 10000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8502
num rollout transitions: 250000, reward mean: 4.8815
num rollout transitions: 250000, reward mean: 4.8757
num rollout transitions: 250000, reward mean: 4.8911
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.75e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.952     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.411     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 11.7      |
| eval/normalized_episode_reward_std | 12.2      |
| loss/actor                         | -183      |
| loss/alpha                         | -4.36     |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 11000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8792
num rollout transitions: 250000, reward mean: 4.9001
num rollout transitions: 250000, reward mean: 4.8949
num rollout transitions: 250000, reward mean: 4.8767
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.375     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 12.2      |
| eval/normalized_episode_reward_std | 12.7      |
| loss/actor                         | -192      |
| loss/alpha                         | -4.45     |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 12000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9015
num rollout transitions: 250000, reward mean: 4.8719
num rollout transitions: 250000, reward mean: 4.9108
num rollout transitions: 250000, reward mean: 4.8788
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.606    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.343    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 9.1      |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -200     |
| loss/alpha                         | -4.29    |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 13000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8939
num rollout transitions: 250000, reward mean: 4.8876
num rollout transitions: 250000, reward mean: 4.8931
num rollout transitions: 250000, reward mean: 4.8897
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.314     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 19.3      |
| eval/normalized_episode_reward_std | 23        |
| loss/actor                         | -208      |
| loss/alpha                         | -4.02     |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 14000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8838
num rollout transitions: 250000, reward mean: 4.8803
num rollout transitions: 250000, reward mean: 4.8847
num rollout transitions: 250000, reward mean: 4.8969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.288     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 9.21      |
| eval/normalized_episode_reward_std | 18.7      |
| loss/actor                         | -217      |
| loss/alpha                         | -3.68     |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 15000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8907
num rollout transitions: 250000, reward mean: 4.9040
num rollout transitions: 250000, reward mean: 4.8811
num rollout transitions: 250000, reward mean: 4.9039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-09 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 1.16     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.266    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 20.5     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -225     |
| loss/alpha                         | -3.13    |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 16000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8846
num rollout transitions: 250000, reward mean: 4.8914
num rollout transitions: 250000, reward mean: 4.8936
num rollout transitions: 250000, reward mean: 4.8808
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.569    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.245    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 33.4     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -234     |
| loss/alpha                         | -2.7     |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 17000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8892
num rollout transitions: 250000, reward mean: 4.8948
num rollout transitions: 250000, reward mean: 4.8967
num rollout transitions: 250000, reward mean: 4.8859
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.454     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.228     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 29.5      |
| eval/normalized_episode_reward_std | 24        |
| loss/actor                         | -243      |
| loss/alpha                         | -2.07     |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 18000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8901
num rollout transitions: 250000, reward mean: 4.8885
num rollout transitions: 250000, reward mean: 4.8874
num rollout transitions: 250000, reward mean: 4.8800
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-09 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.213     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 28.7      |
| eval/normalized_episode_reward_std | 25.3      |
| loss/actor                         | -252      |
| loss/alpha                         | -1.43     |
| loss/critic1                       | 20        |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 19000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8712
num rollout transitions: 250000, reward mean: 4.8846
num rollout transitions: 250000, reward mean: 4.8747
num rollout transitions: 250000, reward mean: 4.8718
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.72e-09 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.201     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 18        |
| eval/normalized_episode_reward_std | 19.4      |
| loss/actor                         | -260      |
| loss/alpha                         | -0.991    |
| loss/critic1                       | 20.7      |
| loss/critic2                       | 19.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 20000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8960
num rollout transitions: 250000, reward mean: 4.8866
num rollout transitions: 250000, reward mean: 4.8829
num rollout transitions: 250000, reward mean: 4.8717
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.2      |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.19     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 19.7     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -269     |
| loss/alpha                         | -0.685   |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 21000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8939
num rollout transitions: 250000, reward mean: 4.8889
num rollout transitions: 250000, reward mean: 4.8698
num rollout transitions: 250000, reward mean: 4.8665
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 29.9     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -278     |
| loss/alpha                         | -0.331   |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 22000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8774
num rollout transitions: 250000, reward mean: 4.8765
num rollout transitions: 250000, reward mean: 4.8705
num rollout transitions: 250000, reward mean: 4.8838
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 30.5      |
| eval/normalized_episode_reward_std | 14.1      |
| loss/actor                         | -287      |
| loss/alpha                         | 0.0139    |
| loss/critic1                       | 21.4      |
| loss/critic2                       | 20        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 23000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8686
num rollout transitions: 250000, reward mean: 4.8809
num rollout transitions: 250000, reward mean: 4.8753
num rollout transitions: 250000, reward mean: 4.8926
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.5     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -294     |
| loss/alpha                         | 0.0611   |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 24000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8920
num rollout transitions: 250000, reward mean: 4.8684
num rollout transitions: 250000, reward mean: 4.8718
num rollout transitions: 250000, reward mean: 4.8830
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.02e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 1.6       |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 30.8      |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -302      |
| loss/alpha                         | 0.133     |
| loss/critic1                       | 21.5      |
| loss/critic2                       | 20.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 25000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8766
num rollout transitions: 250000, reward mean: 4.8826
num rollout transitions: 250000, reward mean: 4.8889
num rollout transitions: 250000, reward mean: 4.8900
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.04e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.434    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.188    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 30.2     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -310     |
| loss/alpha                         | 0.0977   |
| loss/critic1                       | 22       |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 26000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8738
num rollout transitions: 250000, reward mean: 4.8963
num rollout transitions: 250000, reward mean: 4.8703
num rollout transitions: 250000, reward mean: 4.8864
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.505    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.193    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.5     |
| eval/normalized_episode_reward_std | 19.1     |
| loss/actor                         | -317     |
| loss/alpha                         | 0.0408   |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9048
num rollout transitions: 250000, reward mean: 4.8870
num rollout transitions: 250000, reward mean: 4.8943
num rollout transitions: 250000, reward mean: 4.8838
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.44e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 1.3      |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.192    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 24.2     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -324     |
| loss/alpha                         | -0.0382  |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 28000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8775
num rollout transitions: 250000, reward mean: 4.8936
num rollout transitions: 250000, reward mean: 4.8978
num rollout transitions: 250000, reward mean: 4.9008
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.672     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.193     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 40.8      |
| eval/normalized_episode_reward_std | 18.8      |
| loss/actor                         | -331      |
| loss/alpha                         | 0.0752    |
| loss/critic1                       | 22.7      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 29000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9021
num rollout transitions: 250000, reward mean: 4.9004
num rollout transitions: 250000, reward mean: 4.8995
num rollout transitions: 250000, reward mean: 4.8782
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.56e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.579     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.195     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 24.1      |
| eval/normalized_episode_reward_std | 20.5      |
| loss/actor                         | -338      |
| loss/alpha                         | 0.0284    |
| loss/critic1                       | 22.6      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 30000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9161
num rollout transitions: 250000, reward mean: 4.8979
num rollout transitions: 250000, reward mean: 4.8857
num rollout transitions: 250000, reward mean: 4.8829
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.63      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.196     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 50.2      |
| eval/normalized_episode_reward_std | 18.5      |
| loss/actor                         | -344      |
| loss/alpha                         | 0.0157    |
| loss/critic1                       | 23.7      |
| loss/critic2                       | 21.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 31000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8797
num rollout transitions: 250000, reward mean: 4.8882
num rollout transitions: 250000, reward mean: 4.8788
num rollout transitions: 250000, reward mean: 4.9020
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.964     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.197     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 46        |
| eval/normalized_episode_reward_std | 21.6      |
| loss/actor                         | -351      |
| loss/alpha                         | 0.0382    |
| loss/critic1                       | 23        |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 32000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9099
num rollout transitions: 250000, reward mean: 4.9058
num rollout transitions: 250000, reward mean: 4.9002
num rollout transitions: 250000, reward mean: 4.8849
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.177   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.197    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 42.5     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -357     |
| loss/alpha                         | -0.00129 |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 33000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9150
num rollout transitions: 250000, reward mean: 4.8847
num rollout transitions: 250000, reward mean: 4.8930
num rollout transitions: 250000, reward mean: 4.8749
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.3e-09 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 1.23     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.196    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 46.7     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -363     |
| loss/alpha                         | -0.0658  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 22.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 34000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8866
num rollout transitions: 250000, reward mean: 4.9025
num rollout transitions: 250000, reward mean: 4.8943
num rollout transitions: 250000, reward mean: 4.9217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.83e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.286     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 39.8      |
| eval/normalized_episode_reward_std | 22        |
| loss/actor                         | -368      |
| loss/alpha                         | -0.0273   |
| loss/critic1                       | 23.4      |
| loss/critic2                       | 21.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 35000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8816
num rollout transitions: 250000, reward mean: 4.8904
num rollout transitions: 250000, reward mean: 4.9111
num rollout transitions: 250000, reward mean: 4.9020
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-09 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 42.8      |
| eval/normalized_episode_reward_std | 22.5      |
| loss/actor                         | -374      |
| loss/alpha                         | -0.00181  |
| loss/critic1                       | 23.6      |
| loss/critic2                       | 21.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 36000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8922
num rollout transitions: 250000, reward mean: 4.8995
num rollout transitions: 250000, reward mean: 4.9014
num rollout transitions: 250000, reward mean: 4.8940
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.318     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 41        |
| eval/normalized_episode_reward_std | 26.8      |
| loss/actor                         | -380      |
| loss/alpha                         | 0.0369    |
| loss/critic1                       | 23.7      |
| loss/critic2                       | 22        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 37000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9180
num rollout transitions: 250000, reward mean: 4.9075
num rollout transitions: 250000, reward mean: 4.8933
num rollout transitions: 250000, reward mean: 4.9029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.443   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.196    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 5.33     |
| loss/actor                         | -385     |
| loss/alpha                         | 0.0246   |
| loss/critic1                       | 24.2     |
| loss/critic2                       | 22.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 38000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8972
num rollout transitions: 250000, reward mean: 4.9201
num rollout transitions: 250000, reward mean: 4.9039
num rollout transitions: 250000, reward mean: 4.9104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 1.37     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.197    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 49.3     |
| eval/normalized_episode_reward_std | 18.9     |
| loss/actor                         | -390     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 24.8     |
| loss/critic2                       | 23       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 39000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8928
num rollout transitions: 250000, reward mean: 4.9226
num rollout transitions: 250000, reward mean: 4.8976
num rollout transitions: 250000, reward mean: 4.8981
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.09e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 1.37     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.198    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 43.1     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -394     |
| loss/alpha                         | 0.00268  |
| loss/critic1                       | 25.3     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.9      |
| timestep                           | 40000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9099
num rollout transitions: 250000, reward mean: 4.9076
num rollout transitions: 250000, reward mean: 4.8888
num rollout transitions: 250000, reward mean: 4.9033
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 1.31      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.198     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56.6      |
| eval/normalized_episode_reward_std | 11.1      |
| loss/actor                         | -399      |
| loss/alpha                         | -0.0286   |
| loss/critic1                       | 24.9      |
| loss/critic2                       | 23        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 41000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9068
num rollout transitions: 250000, reward mean: 4.9055
num rollout transitions: 250000, reward mean: 4.9154
num rollout transitions: 250000, reward mean: 4.9019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.106    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.195     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 55.3      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -404      |
| loss/alpha                         | -0.0216   |
| loss/critic1                       | 24.1      |
| loss/critic2                       | 22.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 42000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9176
num rollout transitions: 250000, reward mean: 4.9109
num rollout transitions: 250000, reward mean: 4.9086
num rollout transitions: 250000, reward mean: 4.9053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 1.07      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.195     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.8      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -409      |
| loss/alpha                         | -0.00691  |
| loss/critic1                       | 24        |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 43000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9197
num rollout transitions: 250000, reward mean: 4.9285
num rollout transitions: 250000, reward mean: 4.9254
num rollout transitions: 250000, reward mean: 4.9115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.398     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56.7      |
| eval/normalized_episode_reward_std | 17.7      |
| loss/actor                         | -413      |
| loss/alpha                         | 0.0309    |
| loss/critic1                       | 24.7      |
| loss/critic2                       | 23.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 44000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9278
num rollout transitions: 250000, reward mean: 4.9223
num rollout transitions: 250000, reward mean: 4.9314
num rollout transitions: 250000, reward mean: 4.9124
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | -0.419    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.195     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -417      |
| loss/alpha                         | -0.0214   |
| loss/critic1                       | 24.3      |
| loss/critic2                       | 22.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 45000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9187
num rollout transitions: 250000, reward mean: 4.9099
num rollout transitions: 250000, reward mean: 4.9211
num rollout transitions: 250000, reward mean: 4.9345
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | 1.46      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.192     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 54.8      |
| eval/normalized_episode_reward_std | 18.2      |
| loss/actor                         | -421      |
| loss/alpha                         | -0.129    |
| loss/critic1                       | 23.3      |
| loss/critic2                       | 21.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 46000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9168
num rollout transitions: 250000, reward mean: 4.9171
num rollout transitions: 250000, reward mean: 4.9194
num rollout transitions: 250000, reward mean: 4.9094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.42e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | -0.447   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -426     |
| loss/alpha                         | 0.0023   |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 47000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9050
num rollout transitions: 250000, reward mean: 4.9192
num rollout transitions: 250000, reward mean: 4.9229
num rollout transitions: 250000, reward mean: 4.9342
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.07e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.226    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.188    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.8     |
| eval/normalized_episode_reward_std | 12.9     |
| loss/actor                         | -430     |
| loss/alpha                         | -0.0843  |
| loss/critic1                       | 23       |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 48000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9265
num rollout transitions: 250000, reward mean: 4.9254
num rollout transitions: 250000, reward mean: 4.9400
num rollout transitions: 250000, reward mean: 4.9360
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.66e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.665     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.9      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -434      |
| loss/alpha                         | -0.0743   |
| loss/critic1                       | 24.1      |
| loss/critic2                       | 22.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 49000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9263
num rollout transitions: 250000, reward mean: 4.9109
num rollout transitions: 250000, reward mean: 4.9342
num rollout transitions: 250000, reward mean: 4.9392
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.364     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.6      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -438      |
| loss/alpha                         | 0.00403   |
| loss/critic1                       | 22.9      |
| loss/critic2                       | 21.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 50000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9173
num rollout transitions: 250000, reward mean: 4.9235
num rollout transitions: 250000, reward mean: 4.9222
num rollout transitions: 250000, reward mean: 4.9287
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.18e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 1.05     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.184    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -442     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 51000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9250
num rollout transitions: 250000, reward mean: 4.9180
num rollout transitions: 250000, reward mean: 4.9349
num rollout transitions: 250000, reward mean: 4.9225
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.625     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.184     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59.7      |
| eval/normalized_episode_reward_std | 9.19      |
| loss/actor                         | -445      |
| loss/alpha                         | 0.0317    |
| loss/critic1                       | 23        |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 52000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9231
num rollout transitions: 250000, reward mean: 4.9231
num rollout transitions: 250000, reward mean: 4.9307
num rollout transitions: 250000, reward mean: 4.9496
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-09 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.29     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -449     |
| loss/alpha                         | 0.0427   |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9246
num rollout transitions: 250000, reward mean: 4.9217
num rollout transitions: 250000, reward mean: 4.9423
num rollout transitions: 250000, reward mean: 4.9385
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.89e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.715    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.186    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -452     |
| loss/alpha                         | -0.0511  |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9420
num rollout transitions: 250000, reward mean: 4.9405
num rollout transitions: 250000, reward mean: 4.9352
num rollout transitions: 250000, reward mean: 4.9289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.304    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.4      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -456      |
| loss/alpha                         | -0.0306   |
| loss/critic1                       | 22.9      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 55000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9353
num rollout transitions: 250000, reward mean: 4.9200
num rollout transitions: 250000, reward mean: 4.9205
num rollout transitions: 250000, reward mean: 4.9218
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.409     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.8      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -459      |
| loss/alpha                         | -0.0764   |
| loss/critic1                       | 21.2      |
| loss/critic2                       | 19.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 56000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9251
num rollout transitions: 250000, reward mean: 4.9421
num rollout transitions: 250000, reward mean: 4.9373
num rollout transitions: 250000, reward mean: 4.9403
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.113    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.5     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -462     |
| loss/alpha                         | -0.152   |
| loss/critic1                       | 22.1     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 57000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9392
num rollout transitions: 250000, reward mean: 4.9323
num rollout transitions: 250000, reward mean: 4.9450
num rollout transitions: 250000, reward mean: 4.9538
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.422   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -465     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 58000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9403
num rollout transitions: 250000, reward mean: 4.9464
num rollout transitions: 250000, reward mean: 4.9446
num rollout transitions: 250000, reward mean: 4.9349
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -1.06    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -468     |
| loss/alpha                         | -0.0526  |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 59000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9516
num rollout transitions: 250000, reward mean: 4.9490
num rollout transitions: 250000, reward mean: 4.9366
num rollout transitions: 250000, reward mean: 4.9322
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.254     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -471      |
| loss/alpha                         | 0.0266    |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 19.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 60000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9271
num rollout transitions: 250000, reward mean: 4.9477
num rollout transitions: 250000, reward mean: 4.9343
num rollout transitions: 250000, reward mean: 4.9301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.91e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.645     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.7      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -474      |
| loss/alpha                         | -0.0775   |
| loss/critic1                       | 21        |
| loss/critic2                       | 19.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 61000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9515
num rollout transitions: 250000, reward mean: 4.9356
num rollout transitions: 250000, reward mean: 4.9490
num rollout transitions: 250000, reward mean: 4.9361
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.83e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.594    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -477     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 21.1     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 62000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9366
num rollout transitions: 250000, reward mean: 4.9354
num rollout transitions: 250000, reward mean: 4.9334
num rollout transitions: 250000, reward mean: 4.9220
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.975   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -480     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 20.8     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 63000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9417
num rollout transitions: 250000, reward mean: 4.9300
num rollout transitions: 250000, reward mean: 4.9429
num rollout transitions: 250000, reward mean: 4.9282
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.231    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.2      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -483      |
| loss/alpha                         | 0.0122    |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 64000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9415
num rollout transitions: 250000, reward mean: 4.9322
num rollout transitions: 250000, reward mean: 4.9474
num rollout transitions: 250000, reward mean: 4.9355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.668     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.5      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -485      |
| loss/alpha                         | -0.106    |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 19.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 65000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9334
num rollout transitions: 250000, reward mean: 4.9563
num rollout transitions: 250000, reward mean: 4.9372
num rollout transitions: 250000, reward mean: 4.9483
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.249    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -488      |
| loss/alpha                         | 0.0307    |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 19.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 66000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9478
num rollout transitions: 250000, reward mean: 4.9535
num rollout transitions: 250000, reward mean: 4.9482
num rollout transitions: 250000, reward mean: 4.9546
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-09 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.566    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -490     |
| loss/alpha                         | 0.00287  |
| loss/critic1                       | 21       |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 67000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9426
num rollout transitions: 250000, reward mean: 4.9541
num rollout transitions: 250000, reward mean: 4.9531
num rollout transitions: 250000, reward mean: 4.9546
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-09 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.59     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -492     |
| loss/alpha                         | -0.0271  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9646
num rollout transitions: 250000, reward mean: 4.9456
num rollout transitions: 250000, reward mean: 4.9408
num rollout transitions: 250000, reward mean: 4.9507
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.22e-09 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.291     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.1      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -494      |
| loss/alpha                         | -0.0703   |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 19.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 69000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9208
num rollout transitions: 250000, reward mean: 4.9258
num rollout transitions: 250000, reward mean: 4.9328
num rollout transitions: 250000, reward mean: 4.9401
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.99e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.828    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.2      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -497      |
| loss/alpha                         | -0.0338   |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 70000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9465
num rollout transitions: 250000, reward mean: 4.9499
num rollout transitions: 250000, reward mean: 4.9506
num rollout transitions: 250000, reward mean: 4.9506
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.01e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.234    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -499     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9589
num rollout transitions: 250000, reward mean: 4.9353
num rollout transitions: 250000, reward mean: 4.9459
num rollout transitions: 250000, reward mean: 4.9509
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.19e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 1.17      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -501      |
| loss/alpha                         | 0.0316    |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 72000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9350
num rollout transitions: 250000, reward mean: 4.9423
num rollout transitions: 250000, reward mean: 4.9244
num rollout transitions: 250000, reward mean: 4.9486
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-09 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.431    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.6      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -504      |
| loss/alpha                         | -0.042    |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 73000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9352
num rollout transitions: 250000, reward mean: 4.9301
num rollout transitions: 250000, reward mean: 4.9475
num rollout transitions: 250000, reward mean: 4.9603
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.665    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -506     |
| loss/alpha                         | -0.00878 |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9605
num rollout transitions: 250000, reward mean: 4.9673
num rollout transitions: 250000, reward mean: 4.9575
num rollout transitions: 250000, reward mean: 4.9424
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.235   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -508     |
| loss/alpha                         | 0.0042   |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 75000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9352
num rollout transitions: 250000, reward mean: 4.9469
num rollout transitions: 250000, reward mean: 4.9537
num rollout transitions: 250000, reward mean: 4.9494
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-10  |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.878     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -510      |
| loss/alpha                         | -0.000387 |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 18.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 76000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9513
num rollout transitions: 250000, reward mean: 4.9611
num rollout transitions: 250000, reward mean: 4.9565
num rollout transitions: 250000, reward mean: 4.9547
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.555    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -512      |
| loss/alpha                         | -0.014    |
| loss/critic1                       | 21.1      |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 77000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9545
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9488
num rollout transitions: 250000, reward mean: 4.9539
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.0563   |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.8      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -514      |
| loss/alpha                         | -0.0954   |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 78000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9614
num rollout transitions: 250000, reward mean: 4.9642
num rollout transitions: 250000, reward mean: 4.9532
num rollout transitions: 250000, reward mean: 4.9475
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.243   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -516     |
| loss/alpha                         | 0.0612   |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 19.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 79000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9435
num rollout transitions: 250000, reward mean: 4.9471
num rollout transitions: 250000, reward mean: 4.9532
num rollout transitions: 250000, reward mean: 4.9589
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-13 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.359    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -518     |
| loss/alpha                         | 0.0386   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 80000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9613
num rollout transitions: 250000, reward mean: 4.9570
num rollout transitions: 250000, reward mean: 4.9565
num rollout transitions: 250000, reward mean: 4.9647
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.0815   |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -520      |
| loss/alpha                         | -0.114    |
| loss/critic1                       | 20.7      |
| loss/critic2                       | 19.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 81000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9615
num rollout transitions: 250000, reward mean: 4.9499
num rollout transitions: 250000, reward mean: 4.9490
num rollout transitions: 250000, reward mean: 4.9496
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.774    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63        |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -522      |
| loss/alpha                         | -0.0901   |
| loss/critic1                       | 20        |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 82000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9527
num rollout transitions: 250000, reward mean: 4.9645
num rollout transitions: 250000, reward mean: 4.9460
num rollout transitions: 250000, reward mean: 4.9545
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-09 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.529   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -525     |
| loss/alpha                         | 0.105    |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 83000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9359
num rollout transitions: 250000, reward mean: 4.9443
num rollout transitions: 250000, reward mean: 4.9520
num rollout transitions: 250000, reward mean: 4.9423
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.425     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.4      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -527      |
| loss/alpha                         | -0.0159   |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 84000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9653
num rollout transitions: 250000, reward mean: 4.9612
num rollout transitions: 250000, reward mean: 4.9564
num rollout transitions: 250000, reward mean: 4.9584
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.667    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -529     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 85000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9525
num rollout transitions: 250000, reward mean: 4.9471
num rollout transitions: 250000, reward mean: 4.9447
num rollout transitions: 250000, reward mean: 4.9608
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.721    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -530     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 86000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9663
num rollout transitions: 250000, reward mean: 4.9496
num rollout transitions: 250000, reward mean: 4.9636
num rollout transitions: 250000, reward mean: 4.9592
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.0567    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.6      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -532      |
| loss/alpha                         | -0.0151   |
| loss/critic1                       | 20.2      |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 87000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9508
num rollout transitions: 250000, reward mean: 4.9742
num rollout transitions: 250000, reward mean: 4.9652
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.4e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.923    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -534     |
| loss/alpha                         | -0.0882  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 88000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9738
num rollout transitions: 250000, reward mean: 4.9633
num rollout transitions: 250000, reward mean: 4.9658
num rollout transitions: 250000, reward mean: 4.9551
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.239   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -536     |
| loss/alpha                         | -0.0106  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 89000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9602
num rollout transitions: 250000, reward mean: 4.9678
num rollout transitions: 250000, reward mean: 4.9655
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.0547   |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.9      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -538      |
| loss/alpha                         | 0.0298    |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 90000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9585
num rollout transitions: 250000, reward mean: 4.9625
num rollout transitions: 250000, reward mean: 4.9617
num rollout transitions: 250000, reward mean: 4.9636
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.91e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.978    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -540      |
| loss/alpha                         | -0.0513   |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 91000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9633
num rollout transitions: 250000, reward mean: 4.9661
num rollout transitions: 250000, reward mean: 4.9629
num rollout transitions: 250000, reward mean: 4.9583
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.299     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.4      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -542      |
| loss/alpha                         | 0.0368    |
| loss/critic1                       | 20        |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 92000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9599
num rollout transitions: 250000, reward mean: 4.9615
num rollout transitions: 250000, reward mean: 4.9471
num rollout transitions: 250000, reward mean: 4.9516
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.732     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.8      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -544      |
| loss/alpha                         | 0.0784    |
| loss/critic1                       | 20        |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 93000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9530
num rollout transitions: 250000, reward mean: 4.9528
num rollout transitions: 250000, reward mean: 4.9632
num rollout transitions: 250000, reward mean: 4.9478
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.22     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -545      |
| loss/alpha                         | -0.0423   |
| loss/critic1                       | 19.9      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 94000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9802
num rollout transitions: 250000, reward mean: 4.9643
num rollout transitions: 250000, reward mean: 4.9581
num rollout transitions: 250000, reward mean: 4.9583
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-09 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.543    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -547     |
| loss/alpha                         | -0.0345  |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 95000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9559
num rollout transitions: 250000, reward mean: 4.9591
num rollout transitions: 250000, reward mean: 4.9544
num rollout transitions: 250000, reward mean: 4.9575
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.755   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -549     |
| loss/alpha                         | -0.0344  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9709
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9578
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-12 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.0222    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59.4      |
| eval/normalized_episode_reward_std | 20.6      |
| loss/actor                         | -550      |
| loss/alpha                         | 0.0137    |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 97000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9715
num rollout transitions: 250000, reward mean: 4.9624
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 1.98     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -552     |
| loss/alpha                         | 0.0256   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 98000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9684
num rollout transitions: 250000, reward mean: 4.9709
num rollout transitions: 250000, reward mean: 4.9867
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.0537   |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.9      |
| eval/normalized_episode_reward_std | 13.2      |
| loss/actor                         | -553      |
| loss/alpha                         | -0.0212   |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 99000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9714
num rollout transitions: 250000, reward mean: 4.9673
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.21e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.594    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.3      |
| eval/normalized_episode_reward_std | 2.49      |
| loss/actor                         | -555      |
| loss/alpha                         | 0.0964    |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 100000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9603
num rollout transitions: 250000, reward mean: 4.9678
num rollout transitions: 250000, reward mean: 4.9768
num rollout transitions: 250000, reward mean: 4.9743
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.19     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -557     |
| loss/alpha                         | 0.0774   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 101000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9621
num rollout transitions: 250000, reward mean: 4.9676
num rollout transitions: 250000, reward mean: 4.9721
num rollout transitions: 250000, reward mean: 4.9548
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.744    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.6     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -558     |
| loss/alpha                         | 0.0432   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 102000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9553
num rollout transitions: 250000, reward mean: 4.9538
num rollout transitions: 250000, reward mean: 4.9676
num rollout transitions: 250000, reward mean: 4.9693
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.227   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -559     |
| loss/alpha                         | -0.0348  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 103000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9599
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9485
num rollout transitions: 250000, reward mean: 4.9740
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.296     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -561      |
| loss/alpha                         | -0.0414   |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 104000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9656
num rollout transitions: 250000, reward mean: 4.9591
num rollout transitions: 250000, reward mean: 4.9593
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.139    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 9.42     |
| loss/actor                         | -562     |
| loss/alpha                         | -0.0181  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 105000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9635
num rollout transitions: 250000, reward mean: 4.9647
num rollout transitions: 250000, reward mean: 4.9595
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-09 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.0132   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -563     |
| loss/alpha                         | -0.035   |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9700
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9697
num rollout transitions: 250000, reward mean: 4.9684
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.676    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -565     |
| loss/alpha                         | -0.112   |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 107000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9737
num rollout transitions: 250000, reward mean: 4.9818
num rollout transitions: 250000, reward mean: 4.9819
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.0258  |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 6.93     |
| loss/actor                         | -566     |
| loss/alpha                         | 0.0561   |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 108000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9695
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9662
num rollout transitions: 250000, reward mean: 4.9606
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 1.19     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -567     |
| loss/alpha                         | 0.0285   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 109000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9702
num rollout transitions: 250000, reward mean: 4.9614
num rollout transitions: 250000, reward mean: 4.9681
num rollout transitions: 250000, reward mean: 4.9597
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.66e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.291    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.9      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -568      |
| loss/alpha                         | -0.0173   |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 110000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9593
num rollout transitions: 250000, reward mean: 4.9634
num rollout transitions: 250000, reward mean: 4.9672
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 1.58     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -569     |
| loss/alpha                         | 0.151    |
| loss/critic1                       | 20.7     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9517
num rollout transitions: 250000, reward mean: 4.9569
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9667
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.623    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.9      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -570      |
| loss/alpha                         | 0.0189    |
| loss/critic1                       | 20        |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 112000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9675
num rollout transitions: 250000, reward mean: 4.9686
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.0947    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.7      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -571      |
| loss/alpha                         | -0.0307   |
| loss/critic1                       | 20        |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 113000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9688
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9661
num rollout transitions: 250000, reward mean: 4.9628
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.53e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.785     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -573      |
| loss/alpha                         | 0.0623    |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 114000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9615
num rollout transitions: 250000, reward mean: 4.9657
num rollout transitions: 250000, reward mean: 4.9730
num rollout transitions: 250000, reward mean: 4.9657
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.000886 |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -574      |
| loss/alpha                         | -0.122    |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 115000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9618
num rollout transitions: 250000, reward mean: 4.9676
num rollout transitions: 250000, reward mean: 4.9663
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.00539  |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -575     |
| loss/alpha                         | -0.0477  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 116000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9704
num rollout transitions: 250000, reward mean: 4.9631
num rollout transitions: 250000, reward mean: 4.9596
num rollout transitions: 250000, reward mean: 4.9530
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.66     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.6     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -575     |
| loss/alpha                         | 0.0512   |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 117000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9501
num rollout transitions: 250000, reward mean: 4.9548
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9640
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.376     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -576      |
| loss/alpha                         | -0.0683   |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 19.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 118000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9697
num rollout transitions: 250000, reward mean: 4.9632
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9676
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.78      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.3      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -577      |
| loss/alpha                         | -0.0254   |
| loss/critic1                       | 20.2      |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 119000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9695
num rollout transitions: 250000, reward mean: 4.9622
num rollout transitions: 250000, reward mean: 4.9673
num rollout transitions: 250000, reward mean: 4.9639
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.652     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -578      |
| loss/alpha                         | -0.0689   |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 120000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9652
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9712
num rollout transitions: 250000, reward mean: 4.9703
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.174    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -578     |
| loss/alpha                         | 0.044    |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 121000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9677
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-09 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1.26      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -579      |
| loss/alpha                         | -0.0137   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 122000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9564
num rollout transitions: 250000, reward mean: 4.9689
num rollout transitions: 250000, reward mean: 4.9685
num rollout transitions: 250000, reward mean: 4.9883
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.127     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 2.47      |
| loss/actor                         | -580      |
| loss/alpha                         | 0.0848    |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 123000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 4.9671
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9732
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.92e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.00119  |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.9      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -581      |
| loss/alpha                         | 0.0168    |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 124000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9719
num rollout transitions: 250000, reward mean: 4.9630
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9815
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -582     |
| loss/alpha                         | 0.0297   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 125000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9625
num rollout transitions: 250000, reward mean: 4.9671
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9598
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.221    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -583     |
| loss/alpha                         | 0.0257   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 126000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9549
num rollout transitions: 250000, reward mean: 4.9756
num rollout transitions: 250000, reward mean: 4.9718
num rollout transitions: 250000, reward mean: 4.9651
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -1.01    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -583     |
| loss/alpha                         | -0.0378  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 127000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9698
num rollout transitions: 250000, reward mean: 4.9554
num rollout transitions: 250000, reward mean: 4.9618
num rollout transitions: 250000, reward mean: 4.9659
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.78     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -584     |
| loss/alpha                         | -0.0426  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 128000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9528
num rollout transitions: 250000, reward mean: 4.9698
num rollout transitions: 250000, reward mean: 4.9637
num rollout transitions: 250000, reward mean: 4.9713
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.915     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -585      |
| loss/alpha                         | -0.0471   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 129000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9712
num rollout transitions: 250000, reward mean: 4.9687
num rollout transitions: 250000, reward mean: 4.9822
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.14     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.3     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -586     |
| loss/alpha                         | 0.0042   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9712
num rollout transitions: 250000, reward mean: 4.9710
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.76     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -586     |
| loss/alpha                         | -0.00921 |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9667
num rollout transitions: 250000, reward mean: 4.9620
num rollout transitions: 250000, reward mean: 4.9593
num rollout transitions: 250000, reward mean: 4.9720
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -587      |
| loss/alpha                         | 0.0336    |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 132000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9685
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9757
num rollout transitions: 250000, reward mean: 4.9662
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.608   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.9     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -588     |
| loss/alpha                         | 0.00673  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 133000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9606
num rollout transitions: 250000, reward mean: 4.9723
num rollout transitions: 250000, reward mean: 4.9655
num rollout transitions: 250000, reward mean: 4.9627
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.167     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 2.54      |
| loss/actor                         | -589      |
| loss/alpha                         | -0.0322   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 134000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9669
num rollout transitions: 250000, reward mean: 4.9747
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.63      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.5      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -590      |
| loss/alpha                         | 0.0908    |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 135000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9661
num rollout transitions: 250000, reward mean: 4.9558
num rollout transitions: 250000, reward mean: 4.9657
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.141   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -590     |
| loss/alpha                         | 0.0636   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9732
num rollout transitions: 250000, reward mean: 4.9535
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9840
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.5       |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.2      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -591      |
| loss/alpha                         | -0.0444   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 137000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9755
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9757
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.224    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -592     |
| loss/alpha                         | -0.0395  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 138000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
num rollout transitions: 250000, reward mean: 4.9631
num rollout transitions: 250000, reward mean: 4.9541
num rollout transitions: 250000, reward mean: 4.9765
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.499     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -593      |
| loss/alpha                         | 0.0847    |
| loss/critic1                       | 19.9      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 139000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9655
num rollout transitions: 250000, reward mean: 4.9721
num rollout transitions: 250000, reward mean: 4.9690
num rollout transitions: 250000, reward mean: 4.9690
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.402     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.3      |
| eval/normalized_episode_reward_std | 2.54      |
| loss/actor                         | -594      |
| loss/alpha                         | 0.0995    |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 140000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9653
num rollout transitions: 250000, reward mean: 4.9584
num rollout transitions: 250000, reward mean: 4.9739
num rollout transitions: 250000, reward mean: 4.9823
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.181    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -595     |
| loss/alpha                         | -0.117   |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 141000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9744
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9800
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.743     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 9.17      |
| loss/actor                         | -596      |
| loss/alpha                         | -0.0157   |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 142000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9577
num rollout transitions: 250000, reward mean: 4.9621
num rollout transitions: 250000, reward mean: 4.9648
num rollout transitions: 250000, reward mean: 4.9575
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.6e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.158    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 2.49     |
| loss/actor                         | -597     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9707
num rollout transitions: 250000, reward mean: 4.9563
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9697
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.272   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -598     |
| loss/alpha                         | -0.0666  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 144000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9747
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9743
num rollout transitions: 250000, reward mean: 4.9734
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.13     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -598     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9783
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.432   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -599     |
| loss/alpha                         | 0.0985   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 146000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9602
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9605
num rollout transitions: 250000, reward mean: 4.9619
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.451    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -599     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 147000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9525
num rollout transitions: 250000, reward mean: 4.9701
num rollout transitions: 250000, reward mean: 4.9614
num rollout transitions: 250000, reward mean: 4.9768
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.797    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -600     |
| loss/alpha                         | -0.125   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 148000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9696
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9653
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.64     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -600     |
| loss/alpha                         | -0.0337  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9640
num rollout transitions: 250000, reward mean: 4.9783
num rollout transitions: 250000, reward mean: 4.9693
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-09 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.141    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -601     |
| loss/alpha                         | 0.115    |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 150000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9689
num rollout transitions: 250000, reward mean: 4.9633
num rollout transitions: 250000, reward mean: 4.9779
num rollout transitions: 250000, reward mean: 4.9748
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.407    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 2.46     |
| loss/actor                         | -602     |
| loss/alpha                         | -0.00678 |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 151000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9697
num rollout transitions: 250000, reward mean: 4.9680
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9720
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.39    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -602     |
| loss/alpha                         | -0.0169  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 152000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9637
num rollout transitions: 250000, reward mean: 4.9777
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.799    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -603     |
| loss/alpha                         | -0.0177  |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 153000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9651
num rollout transitions: 250000, reward mean: 4.9622
num rollout transitions: 250000, reward mean: 4.9726
num rollout transitions: 250000, reward mean: 4.9665
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.18     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -603     |
| loss/alpha                         | 0.0518   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 154000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9685
num rollout transitions: 250000, reward mean: 4.9642
num rollout transitions: 250000, reward mean: 4.9648
num rollout transitions: 250000, reward mean: 4.9791
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.43     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.1      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -604      |
| loss/alpha                         | -0.00581  |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 155000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 4.9797
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.105   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -604     |
| loss/alpha                         | -0.123   |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 156000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9789
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.218    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.1     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -605     |
| loss/alpha                         | 0.0362   |
| loss/critic1                       | 20.6     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 157000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9704
num rollout transitions: 250000, reward mean: 4.9738
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.261     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -606      |
| loss/alpha                         | 0.00528   |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 158000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9631
num rollout transitions: 250000, reward mean: 4.9732
num rollout transitions: 250000, reward mean: 4.9592
num rollout transitions: 250000, reward mean: 4.9681
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.185    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.2      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -607      |
| loss/alpha                         | 0.12      |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 159000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9768
num rollout transitions: 250000, reward mean: 4.9754
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.679    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -607     |
| loss/alpha                         | 0.0932   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 160000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9628
num rollout transitions: 250000, reward mean: 4.9704
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9806
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.379   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -608     |
| loss/alpha                         | 0.0029   |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 161000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9694
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.00256   |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -608      |
| loss/alpha                         | -0.1      |
| loss/critic1                       | 20.2      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 162000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 4.9689
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9705
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.52e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.652    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -608     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 163000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9612
num rollout transitions: 250000, reward mean: 4.9779
num rollout transitions: 250000, reward mean: 4.9715
num rollout transitions: 250000, reward mean: 4.9609
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -1.05    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -609     |
| loss/alpha                         | 0.0493   |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 19.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 164000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9599
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9731
num rollout transitions: 250000, reward mean: 4.9794
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.393    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -609      |
| loss/alpha                         | -0.0404   |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 165000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9730
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0797   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -610     |
| loss/alpha                         | -0.127   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 166000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9753
num rollout transitions: 250000, reward mean: 4.9782
num rollout transitions: 250000, reward mean: 4.9703
num rollout transitions: 250000, reward mean: 4.9819
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.69      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.4      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -610      |
| loss/alpha                         | -0.0136   |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 167000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9771
num rollout transitions: 250000, reward mean: 4.9676
num rollout transitions: 250000, reward mean: 4.9817
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.215   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -611     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 19       |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 168000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9776
num rollout transitions: 250000, reward mean: 4.9747
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.112   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -611     |
| loss/alpha                         | -0.00187 |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 169000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9755
num rollout transitions: 250000, reward mean: 4.9715
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9793
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.653     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -612      |
| loss/alpha                         | 0.0564    |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 170000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9785
num rollout transitions: 250000, reward mean: 4.9737
num rollout transitions: 250000, reward mean: 4.9688
num rollout transitions: 250000, reward mean: 4.9831
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.46      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -612      |
| loss/alpha                         | 0.0219    |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 171000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9720
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9678
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.449    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -612     |
| loss/alpha                         | 0.0582   |
| loss/critic1                       | 19       |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 172000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9738
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9634
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0525    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.2      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -612      |
| loss/alpha                         | 0.0379    |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 173000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
num rollout transitions: 250000, reward mean: 4.9701
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9756
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.0161    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -613      |
| loss/alpha                         | -0.0186   |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 174000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9690
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.71    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -613     |
| loss/alpha                         | -0.133   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 175000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9633
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.83e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.511     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.4      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -613      |
| loss/alpha                         | -0.0951   |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 176000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9665
num rollout transitions: 250000, reward mean: 4.9698
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.872     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -614      |
| loss/alpha                         | 0.0334    |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 177000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9850
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.798     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -614      |
| loss/alpha                         | -0.0243   |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 178000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9704
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.31      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -615      |
| loss/alpha                         | 0.0483    |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 179000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9669
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9753
num rollout transitions: 250000, reward mean: 4.9741
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.473     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -615      |
| loss/alpha                         | -0.0066   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 180000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9694
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.78     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -616     |
| loss/alpha                         | 0.0579   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 181000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9596
num rollout transitions: 250000, reward mean: 4.9551
num rollout transitions: 250000, reward mean: 4.9688
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.472    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -616     |
| loss/alpha                         | 0.0673   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 182000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9684
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9663
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-09 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.174   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -616     |
| loss/alpha                         | 0.0569   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9730
num rollout transitions: 250000, reward mean: 4.9747
num rollout transitions: 250000, reward mean: 4.9798
num rollout transitions: 250000, reward mean: 4.9892
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.93     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -616     |
| loss/alpha                         | -0.0367  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9804
num rollout transitions: 250000, reward mean: 4.9746
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -617      |
| loss/alpha                         | -0.0518   |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 185000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9858
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0347   |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -617      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 186000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9750
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9709
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.291     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -617      |
| loss/alpha                         | -0.0726   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 187000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9724
num rollout transitions: 250000, reward mean: 4.9726
num rollout transitions: 250000, reward mean: 4.9775
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.12      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -617      |
| loss/alpha                         | 0.0177    |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 188000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9719
num rollout transitions: 250000, reward mean: 4.9771
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0648   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -618     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 189000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9753
num rollout transitions: 250000, reward mean: 4.9731
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-09 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.384    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -618     |
| loss/alpha                         | 0.0157   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9741
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.569     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -619      |
| loss/alpha                         | 0.0159    |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 191000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9690
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9624
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.714   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -619     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9693
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.518   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -620     |
| loss/alpha                         | 0.0374   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 193000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9688
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9785
num rollout transitions: 250000, reward mean: 4.9706
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.209    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -620      |
| loss/alpha                         | 0.00396   |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 194000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9674
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9731
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -620      |
| loss/alpha                         | -0.114    |
| loss/critic1                       | 19.2      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 195000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9696
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.374   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -621     |
| loss/alpha                         | -0.0286  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9753
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.45e-09 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.173   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -621     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 197000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9823
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.996    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -621      |
| loss/alpha                         | -0.0587   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 198000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9710
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9719
num rollout transitions: 250000, reward mean: 4.9776
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.572    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -622      |
| loss/alpha                         | 0.0172    |
| loss/critic1                       | 19        |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 199000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9582
num rollout transitions: 250000, reward mean: 4.9733
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.446    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -622     |
| loss/alpha                         | 0.00665  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 200000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9771
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.516    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -622     |
| loss/alpha                         | -0.0164  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 201000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9726
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9795
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0494  |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -623     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 202000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9675
num rollout transitions: 250000, reward mean: 4.9695
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.135     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -623      |
| loss/alpha                         | -0.0168   |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 203000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 4.9754
num rollout transitions: 250000, reward mean: 4.9722
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.51      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -624      |
| loss/alpha                         | -0.00366  |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 204000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9714
num rollout transitions: 250000, reward mean: 4.9791
num rollout transitions: 250000, reward mean: 4.9747
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -1.11     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -624      |
| loss/alpha                         | -0.0712   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 205000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9658
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.479    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -624     |
| loss/alpha                         | 0.0559   |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 206000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9675
num rollout transitions: 250000, reward mean: 4.9741
num rollout transitions: 250000, reward mean: 4.9797
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.321   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -625     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 207000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9829
num rollout transitions: 250000, reward mean: 4.9873
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.656    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -625     |
| loss/alpha                         | 0.0024   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 208000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9824
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9699
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.471    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -625     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9779
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9874
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.232     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -626      |
| loss/alpha                         | -0.0626   |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 210000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9913
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.48     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -626      |
| loss/alpha                         | 0.0167    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 211000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9704
num rollout transitions: 250000, reward mean: 4.9892
num rollout transitions: 250000, reward mean: 4.9772
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0983  |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -627     |
| loss/alpha                         | 0.124    |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 212000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9756
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9776
num rollout transitions: 250000, reward mean: 4.9763
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.28     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -628     |
| loss/alpha                         | 0.0608   |
| loss/critic1                       | 19       |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 213000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9779
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9907
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-09  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.193    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -628     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 214000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9701
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9763
num rollout transitions: 250000, reward mean: 4.9719
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.711    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 215000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9794
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9709
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.909    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -629      |
| loss/alpha                         | -0.138    |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 216000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9811
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9856
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.701    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.0902  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 217000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9782
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9754
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -630     |
| loss/alpha                         | -0.00799 |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 218000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9747
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9719
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.61      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -630      |
| loss/alpha                         | 0.0432    |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 219000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9848
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9782
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.356     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -631      |
| loss/alpha                         | 0.0414    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 220000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.542     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -631      |
| loss/alpha                         | -0.0481   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 221000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0454  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 222000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9836
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.598     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -632      |
| loss/alpha                         | 0.121     |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 223000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.49     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0447  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9752
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9722
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.106    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0589  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9804
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.355    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -633     |
| loss/alpha                         | -0.00394 |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 226000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9609
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9763
num rollout transitions: 250000, reward mean: 4.9827
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0999   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -633     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 227000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9767
num rollout transitions: 250000, reward mean: 4.9773
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.179    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -633      |
| loss/alpha                         | -0.0147   |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 228000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9794
num rollout transitions: 250000, reward mean: 4.9798
num rollout transitions: 250000, reward mean: 4.9627
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.25     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -633     |
| loss/alpha                         | 0.0699   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9903
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.255     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -634      |
| loss/alpha                         | 0.052     |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 230000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9576
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.853     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -634      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 231000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9726
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9907
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.04e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.124     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -634      |
| loss/alpha                         | 0.00674   |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 232000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9813
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.00388  |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.027   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 233000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9689
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9744
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0239   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0757  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9765
num rollout transitions: 250000, reward mean: 4.9761
num rollout transitions: 250000, reward mean: 4.9900
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.629   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -636     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 235000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9662
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9798
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -1.19    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -636     |
| loss/alpha                         | 0.0178   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 236000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9785
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9876
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.04e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.571    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -637      |
| loss/alpha                         | 0.0956    |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 237000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9583
num rollout transitions: 250000, reward mean: 4.9808
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9750
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.48     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -637     |
| loss/alpha                         | -0.0398  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9717
num rollout transitions: 250000, reward mean: 4.9783
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9806
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.913     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0467   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 239000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9836
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.175    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -638     |
| loss/alpha                         | -0.0709  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 240000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9754
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9693
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0752   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.0876   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 241000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9770
num rollout transitions: 250000, reward mean: 4.9827
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -638      |
| loss/alpha                         | -0.115    |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 242000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9802
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9708
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.315    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.0673   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 243000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9843
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.63      |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -638      |
| loss/alpha                         | -0.0577   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 244000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9739
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9665
num rollout transitions: 250000, reward mean: 4.9843
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -639      |
| loss/alpha                         | -0.036    |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 245000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9782
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9766
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.472   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9690
num rollout transitions: 250000, reward mean: 4.9680
num rollout transitions: 250000, reward mean: 4.9808
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.019    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -639      |
| loss/alpha                         | 0.0478    |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 247000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9720
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9687
num rollout transitions: 250000, reward mean: 4.9758
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.17     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.5      |
| loss/actor                         | -639     |
| loss/alpha                         | 0.000457 |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 248000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9922
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.382     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -640      |
| loss/alpha                         | -0.00497  |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 249000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9802
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.707    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -640     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 18       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 250000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9809
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0618    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -640      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 251000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9744
num rollout transitions: 250000, reward mean: 4.9974
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.969    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0933   |
| loss/critic1                       | 18       |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 252000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9831
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.531   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0271   |
| loss/critic1                       | 18       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 253000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9645
num rollout transitions: 250000, reward mean: 4.9761
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9752
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.0455   |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -641      |
| loss/alpha                         | -0.0523   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 254000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9877
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.7e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.577    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -641     |
| loss/alpha                         | -0.141   |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9750
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0766   |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -641      |
| loss/alpha                         | 0.0081    |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 256000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9861
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.292     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -642      |
| loss/alpha                         | 0.0365    |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 257000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9802
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 4.9800
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.581    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -642     |
| loss/alpha                         | 0.0766   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 4.9773
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9977
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.467     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 4.74      |
| loss/actor                         | -642      |
| loss/alpha                         | -0.0182   |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 259000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9719
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.662    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -642     |
| loss/alpha                         | -0.0431  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 260000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9670
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9757
num rollout transitions: 250000, reward mean: 4.9716
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -1.21     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -642      |
| loss/alpha                         | 0.11      |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 261000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9743
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9900
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.061    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -642      |
| loss/alpha                         | 0.00211   |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 262000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.17e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.329   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -642     |
| loss/alpha                         | -0.00983 |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9711
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9715
num rollout transitions: 250000, reward mean: 4.9791
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.101     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -643      |
| loss/alpha                         | 0.0382    |
| loss/critic1                       | 19        |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 264000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9851
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.333    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.0727  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 265000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9864
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.196     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -643      |
| loss/alpha                         | -0.00152  |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 266000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.286     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -644      |
| loss/alpha                         | -0.00437  |
| loss/critic1                       | 19.9      |
| loss/critic2                       | 18.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 267000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9777
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.147    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.52     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.00716  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9766
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.102   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -644     |
| loss/alpha                         | -0.0349  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 269000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9841
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.0242   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 270000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9892
num rollout transitions: 250000, reward mean: 4.9772
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.696     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -645      |
| loss/alpha                         | 0.0666    |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 271000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.587    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.0995  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 272000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9939
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.929     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -645      |
| loss/alpha                         | -0.0713   |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 273000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.44     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.159    |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 274000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9892
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.99e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.712    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.0563  |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 275000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9889
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.365     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -645      |
| loss/alpha                         | 0.0458    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 276000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9791
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -1.17    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.19    |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9764
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9815
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0659   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.00689 |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 278000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9868
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.302   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -645     |
| loss/alpha                         | 0.118    |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9855
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -646      |
| loss/alpha                         | 0.0812    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 280000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.397   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -646     |
| loss/alpha                         | 0.0239   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 281000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9882
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.944    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -646     |
| loss/alpha                         | 0.00907  |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 282000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9862
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -646      |
| loss/alpha                         | -0.0183   |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 283000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9888
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -1.09     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -647      |
| loss/alpha                         | -0.104    |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 284000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9885
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4e-10    |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.409    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -647     |
| loss/alpha                         | 0.0461   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 285000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9850
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0248    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -648      |
| loss/alpha                         | -0.0182   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 286000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9963
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.94      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -648      |
| loss/alpha                         | 0.0289    |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 287000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9890
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-10    |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.714   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -648     |
| loss/alpha                         | -0.0555  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 288000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9947
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.607    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -649     |
| loss/alpha                         | 0.0231   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 289000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9882
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.682     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -649      |
| loss/alpha                         | 0.0186    |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 290000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9835
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.454     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.51      |
| loss/actor                         | -649      |
| loss/alpha                         | 0.0342    |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 291000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9848
num rollout transitions: 250000, reward mean: 4.9763
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.83e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.00531 |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -649     |
| loss/alpha                         | -0.104   |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 292000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.663    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -649     |
| loss/alpha                         | 0.0119   |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 293000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.13      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -650      |
| loss/alpha                         | -0.14     |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 294000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 4.9863
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0347    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -650      |
| loss/alpha                         | 0.00476   |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 295000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9742
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0344    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -650      |
| loss/alpha                         | 0.085     |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 296000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9875
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.748     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -650      |
| loss/alpha                         | 0.0636    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 297000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0899   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -650     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 298000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9805
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -650     |
| loss/alpha                         | 0.037    |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9859
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -1.21     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -650      |
| loss/alpha                         | -0.112    |
| loss/critic1                       | 19        |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 300000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9873
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.632     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -651      |
| loss/alpha                         | 0.0108    |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 301000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9887
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.772   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -651     |
| loss/alpha                         | 0.0521   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 302000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.114    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.46      |
| loss/actor                         | -651      |
| loss/alpha                         | -0.0167   |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 303000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.263    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -652     |
| loss/alpha                         | -0.0247  |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 304000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9873
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.989     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -652      |
| loss/alpha                         | -0.0153   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 305000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9786
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.114     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -652      |
| loss/alpha                         | 0.0915    |
| loss/critic1                       | 18        |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 306000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9724
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.05     |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -652     |
| loss/alpha                         | 0.054    |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 307000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9871
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.208     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -652      |
| loss/alpha                         | -0.0363   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 308000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9986
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.325     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -652      |
| loss/alpha                         | 0.0351    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 309000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9883
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.761    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -653      |
| loss/alpha                         | 0.0286    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 310000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9831
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.000109 |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.031    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 311000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9924
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.412    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0545   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 312000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9759
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -653      |
| loss/alpha                         | 0.0236    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 313000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -653     |
| loss/alpha                         | -0.0715  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 314000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.43    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.00199  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 315000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9854
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.685    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.8      |
| eval/normalized_episode_reward_std | 2.48      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0326   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 316000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9951
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.336    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -654      |
| loss/alpha                         | 0.156     |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 317000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9856
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.0703   |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -654      |
| loss/alpha                         | -0.0536   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 318000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.384    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -654     |
| loss/alpha                         | -0.0192  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.424    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -654      |
| loss/alpha                         | 0.000294  |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 320000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.12    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -654     |
| loss/alpha                         | 0.00392  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 321000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9863
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.22e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.628     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.0486   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 322000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.268   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.0782   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 323000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9737
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9884
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.56e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.844     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.0392   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 324000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9962
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0411   |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.102    |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 325000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.0775   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.0813   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 326000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.781    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0318  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 327000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9947
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.675    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0443  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9767
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.33      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -655      |
| loss/alpha                         | 0.0517    |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 329000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9843
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.28     |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.0614   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.153    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -655     |
| loss/alpha                         | 0.047    |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 331000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0002
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.98e-12 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.094    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 332000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.627    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -655     |
| loss/alpha                         | -0.099   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 333000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.5      |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.0399   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.465    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -656     |
| loss/alpha                         | -0.05    |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 335000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9985
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.19     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -656      |
| loss/alpha                         | 0.0193    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 336000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9820
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.1e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.607    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.13     |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 337000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9960
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.698    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.00662 |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 338000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9875
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.417    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 339000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9941
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.571     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.1      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -657      |
| loss/alpha                         | 0.0612    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 340000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9791
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9865
num rollout transitions: 250000, reward mean: 4.9947
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.389     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 2.42      |
| loss/actor                         | -657      |
| loss/alpha                         | -0.0557   |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 341000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9775
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9805
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.322     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0528   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 342000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9927
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0909    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -658      |
| loss/alpha                         | 0.076     |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 343000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9782
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 344000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0010
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.405    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0253   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9737
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.794    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 346000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9851
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.287    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0417    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 347000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9830
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.465     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0206    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 348000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.219   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0641  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 349000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9940
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.347    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -659      |
| loss/alpha                         | -0.155    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 350000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9992
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0783    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.0108    |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 351000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.106   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -660     |
| loss/alpha                         | 0.00436  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 352000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0018
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.981    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0254   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 353000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.367     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.0928    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 354000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.65e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.235     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0868   |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 355000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9905
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.376     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.152     |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 356000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9854
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.265    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.8      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0636   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 357000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.347   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -660     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 358000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.422    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.049    |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 359000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.16      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0794    |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 360000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.564    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0217   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 361000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 4.9783
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.149    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.00503   |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 362000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.266    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 363000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9824
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9948
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0341   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.113   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 364000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9912
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.773    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0586  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 365000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9844
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9963
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.27     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0595   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9813
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.83      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0417   |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 367000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.196   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -660     |
| loss/alpha                         | -0.0355  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 368000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.47      |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 369000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 1.13      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.00928   |
| loss/critic1                       | 18        |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 370000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9931
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.59      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0574   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 371000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.5       |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0615   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 372000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.66     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.00724  |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 373000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9826
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.185    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -660     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 374000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9899
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.328    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0627   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 375000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9753
num rollout transitions: 250000, reward mean: 4.9856
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.801     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.0605   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 376000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9873
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.797    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0571   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 377000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9961
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.83     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.106     |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 378000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9793
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.755    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0313    |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 379000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0010
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.05     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 380000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0003
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.475    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -660     |
| loss/alpha                         | 0.0218   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9892
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0409   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -660     |
| loss/alpha                         | 0.0178   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 382000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0881   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0667  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 383000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9888
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.326     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0428   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 384000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9899
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.15      |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.017    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 385000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9863
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.207    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0212  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9883
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7e-10   |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.323   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.785    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0953   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 388000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9879
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.278     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.00154   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 389000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3e-10    |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0249  |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.00735  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 390000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.39e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.252   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0356   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 391000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9837
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.645    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.0545  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 392000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.673     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0128    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 393000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9819
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.509    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0361    |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 394000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9810
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9966
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.32     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -661      |
| loss/alpha                         | 0.0632    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 395000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9881
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.17      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.125    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 396000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.163   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.126   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 397000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9849
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.681   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0574   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 398000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.409    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9899
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.01      |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.0181   |
| loss/critic1                       | 17        |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 400000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.84     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0344  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9900
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.239    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 402000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.546   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.084    |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 403000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9980
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.205    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0362  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 404000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.47     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -664     |
| loss/alpha                         | -0.0033  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.073     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -664      |
| loss/alpha                         | -0.0531   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 406000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.1e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.274    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.0516   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 407000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -1.19     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0576    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 408000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.65e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.588    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -665     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9829
num rollout transitions: 250000, reward mean: 4.9896
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.984    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.00257  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 410000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9909
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.232    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0217    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 411000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.159   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0472  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 412000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9948
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.79     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0341  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9980
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.222    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.103     |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 414000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-09 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0046  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.00458  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 415000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9911
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.13e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0309    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.0121   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 416000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.24    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0619   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 417000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.2e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0454  |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0435  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 418000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.186     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.115    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 419000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.95e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0162   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 420000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9922
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.76e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.456   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0698   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.38e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.13      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -666      |
| loss/alpha                         | 0.00394   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 422000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9922
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0362  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 423000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0502  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 424000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9941
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.231    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0283    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 425000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.937     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0552   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 426000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9933
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.218    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.092    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9841
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.161    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0248   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -1.17     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0998   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 429000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.329    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0338   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 430000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.135   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0921   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 431000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0828   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0429  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 432000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.322    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.48     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0194  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 433000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9960
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.795    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 434000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.368   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.0304   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 435000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9808
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.181   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.00394 |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -1.58    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -666     |
| loss/alpha                         | 0.066    |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 437000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.513    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0822   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 438000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.58     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0762  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.992    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.47     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.00735 |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 440000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9883
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.97e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.837     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.1      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -666      |
| loss/alpha                         | -0.0681   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 441000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.16     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0247    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 442000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9791
num rollout transitions: 250000, reward mean: 4.9930
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.144     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0479    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 443000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9939
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.516   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 444000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9950
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.108    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -667      |
| loss/alpha                         | 0.0562    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 445000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.954    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -667     |
| loss/alpha                         | 6.95e-05 |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 446000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0832  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0538   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 447000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.407   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0753   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 448000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.236    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0468   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 449000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9777
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.766    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.014    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 450000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9907
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.58      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -667      |
| loss/alpha                         | -0.0723   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 451000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0043
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.804     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -667      |
| loss/alpha                         | -0.0342   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 452000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.822     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -667      |
| loss/alpha                         | -0.043    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 453000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.572   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0617  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 454000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9823
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.62     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.00648  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 455000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.25    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.023    |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 456000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -668     |
| loss/alpha                         | 0.0798   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 457000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 5.0072
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.71      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.00041   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 458000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.0182  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0043
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.72     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -668      |
| loss/alpha                         | -0.0135   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 460000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.623     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.0428    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 461000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9948
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.179    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.0454    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 462000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.103    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.032   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 463000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.303   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0273  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 464000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.881   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 9.89     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0164   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 465000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.654    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0696  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9808
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9931
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.502    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0225   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 467000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.364    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.104    |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 468000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.431     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0169   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 469000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.0585    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0554   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 470000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.972   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 471000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -1.18     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.014     |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 472000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0276  |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 473000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9993
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.694     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0554    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 474000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.683   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 475000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.945     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.119    |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 476000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.119   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0484  |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 477000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.45     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.00961 |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 478000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.136     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0591    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 479000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0028
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.523    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.0216    |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 480000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.113     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -669      |
| loss/alpha                         | -0.1      |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 481000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.464    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 482000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9933
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.863    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.105     |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 483000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.471    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 484000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.453    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0627  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 485000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.185    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.0276   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 486000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9962
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.631     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0273    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 487000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9889
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.667    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.052    |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 488000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9964
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.108    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0588   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 489000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.109    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.0663   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 490000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9916
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.0782   |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0483    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 491000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.689   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.5      |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0926   |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.636     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.0752    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 493000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0428   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9940
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.241     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.0115   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 495000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.442    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 496000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.681    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.099     |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 497000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.635    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0216   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 498000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.06e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.0658   |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0386   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 499000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.808     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.078    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 500000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.183    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0379  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 501000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9768
num rollout transitions: 250000, reward mean: 4.9925
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.659   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0011
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -1.28     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0754    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 503000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9846
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.92e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.242     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.123     |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 504000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.063    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.00123  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.0646   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0886  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 506000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.15e-09 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.0778    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0111    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 507000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9921
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.018    |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 508000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9971
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.875    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 509000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0736  |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.127   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 510000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0971   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 511000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9947
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.532     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.00737   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 512000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.365    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0811   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.83e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.383     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.045     |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 514000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.168    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 515000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.878   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0719   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 516000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -1.55     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -672      |
| loss/alpha                         | -0.129    |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 517000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.643     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0362    |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 518000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.275   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.136    |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 519000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.562    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.49     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.00479  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 520000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.268    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.113    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 521000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0627  |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.109   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 522000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9993
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.288     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.0575    |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 523000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9901
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.435    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0077
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.842    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -673      |
| loss/alpha                         | 0.033     |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 525000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.00605 |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 526000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.674     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0252   |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 527000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.28     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 528000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.28    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0247   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0027
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.26      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0509   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 530000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9959
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.432    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.15      |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 531000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.227    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.41     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 532000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9914
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.144    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.00734   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 533000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9905
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.411    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0337   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 534000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.2      |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0423  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9946
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.401    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.0488    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 536000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.191    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0249  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 537000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9919
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.16      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -674      |
| loss/alpha                         | -0.00359  |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 538000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -1.04    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0189  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 539000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9979
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.0261   |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0488   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 540000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9866
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.59e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.753    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0376   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 541000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.00739 |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 542000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0043
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.62e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.274     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0892   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 543000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.312    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0355   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 544000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.459   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0182   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 545000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.778   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.00363  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 546000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9904
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.166     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0865    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 547000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.831    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0366  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 548000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9962
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.648     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -676      |
| loss/alpha                         | 0.00984   |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 549000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.00712 |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 550000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.15    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.175    |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 551000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.181    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.122   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 552000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.469    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0411  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 553000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.698    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0197   |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 554000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9966
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -677      |
| loss/alpha                         | 0.0752    |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 555000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9985
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.541   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0423  |
| loss/critic1                       | 15       |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 556000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.154    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0677   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 557000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9948
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5e-10    |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0379   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.173   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0104   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9937
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.723     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.113    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 560000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.945   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0513  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 561000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 1.39      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.172     |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 562000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0216  |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0435  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 563000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.58     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0616  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 564000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.98e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.14     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0223   |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 565000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.657    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.0234    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 566000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.05      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 567000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0473   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 568000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.558    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0305   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 4.9946
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.499    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.104     |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 570000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9967
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-12 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.52      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0778    |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 571000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.382    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.0578  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0056
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0114   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 573000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0069
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.03      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0741   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 574000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.44    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0895  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.48e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.028     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0184   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 576000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-12 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.305    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0968   |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 577000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0821   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0025  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 578000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.577   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.00826 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 579000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.675    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0821   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.5e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.399    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 581000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9963
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.643    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0667  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 582000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9863
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.216   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0288  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 583000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9958
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.379    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0224    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 584000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9988
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.332     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0486   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 585000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9871
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.635    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.016   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 586000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0413   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 587000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.137     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.023    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 588000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9998
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.305     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0527    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 589000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9883
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.81    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.12     |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 590000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9796
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.485   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.107   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 591000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0069
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.81     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0681    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 592000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.168    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.09    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 593000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.718    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0485   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 594000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.771    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0373   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.066     |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 596000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.388     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.137    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 597000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.498    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.132     |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 598000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.733   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0421   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 599000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.132    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0199  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 600000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.85e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.0853   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0635  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 601000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9850
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.457    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.054     |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 602000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9970
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0548   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 603000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.28     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.00696  |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 604000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0303   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0517  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 605000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9901
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.00822  |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.00835  |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 606000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9948
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.544    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0676    |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 607000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.953    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0532   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 608000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9886
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.438   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0129  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 609000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0114
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0477  |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0989  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 610000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9979
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.601     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0589    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 611000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.168    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0658   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.141    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.157    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 613000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.92e-12 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.425    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.000199 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 614000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.953    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0255  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 615000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.554    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0569   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 616000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.283   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0749   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 617000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.921     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0121    |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 618000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0003
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.684    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0732   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 619000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9856
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.214   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0902  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 620000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.107   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0541  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 621000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.307    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.136     |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 622000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0722   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.104   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 623000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9940
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.44     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0692   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.0237   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.024   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 625000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.875     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0342    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 626000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-10   |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.16    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 9.77     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0788   |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 627000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9986
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.461   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0534  |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 628000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.68e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.201    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0406   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 629000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9911
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0097
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0251    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0341   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 630000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.423     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.11     |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 631000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0355    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0028    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 632000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.372    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.172     |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 633000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.354   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0149  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.118   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.955    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0501   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 636000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.765    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.125   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.621    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.012    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 638000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9967
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.52      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0991    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 639000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0039
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.734    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0177   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 640000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0561   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0784   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 641000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.548    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0557   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 642000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.892   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0771   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9986
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.123    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.11     |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 644000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.309    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.042   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.8e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.242    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.00756 |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.546     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.0494    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 647000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.135    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0871   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 648000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.484   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0701  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9961
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.371     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.00853  |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 650000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0341   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 651000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9996
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.818     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.053     |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 652000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.357     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0345    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 653000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9897
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.439     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.00965  |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 654000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9966
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.414    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0739  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 655000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.41e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.0796   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0276   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 656000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9931
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.8e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.515    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0252  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 657000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.19e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.727    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0858   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9850
num rollout transitions: 250000, reward mean: 4.9958
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-09 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.234     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0309   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 659000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0035
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.649     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.00173   |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 660000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.31e-13 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.226   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.05     |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9929
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.92e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.691    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.00306   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 662000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9979
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.232     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0733   |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 663000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0027
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.121     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0696   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 664000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0739    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0247   |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 665000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.103   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0827   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 666000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0675  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.00342  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 667000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0031
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.926     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0572   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 668000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0013
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.631    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0447    |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 669000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0031
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.559     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0719   |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 670000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0805   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0618    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 671000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0013
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.864     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0118   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 672000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.613    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 673000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.669    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0267   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 674000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9883
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.882    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0603  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 675000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.889   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.115    |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 676000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.378     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0037    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 677000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.997    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0541   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 678000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.383   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0309   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 679000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.6       |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.102    |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 680000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.133     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0555    |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 681000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.0509   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.101   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 682000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.38e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0455   |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 683000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.314   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0674  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 684000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.283     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 4.62      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.00299  |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 685000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9977
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.521    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.087     |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 686000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9960
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.187    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00895   |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 687000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.217    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.047   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 688000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -1.39    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0627   |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 689000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.774    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0762  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.438     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 691000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.386   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 18.6     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 692000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9884
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.906   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0441   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 693000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9907
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0321    |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 694000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.082    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0313  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 695000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9964
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0549    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0253   |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 696000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9808
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9951
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.187    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0692   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 697000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.151    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00632  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 698000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.24      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0273    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 699000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0005
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.644    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0567   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 700000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.291    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0241   |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 701000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.875   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0445   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 702000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.717    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.095    |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 703000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.466    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.058   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 704000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9966
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.33      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.051    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 705000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.588   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0901   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 706000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0017
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.536    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0376    |
| loss/critic1                       | 17.8      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 707000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0293  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.04    |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 708000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-10   |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.00177 |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00372  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 709000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0201   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 710000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0831    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.121    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 711000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0534  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 712000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9985
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.155    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 713000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0003
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.274    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.119     |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 714000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.275    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 13.1     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0436  |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 715000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.89     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00527 |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 716000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.531    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0173   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 717000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0827   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.104     |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 718000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0581  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0787  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 719000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.384   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0381  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 720000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.194    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0765   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 721000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.413     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0173    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 722000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0037
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.78      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0471    |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 723000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.225     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0376   |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 724000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.17     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00315  |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 725000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.397    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.109     |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 726000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9892
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 4.9921
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.214     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0106    |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 727000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.662    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0877   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 728000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0925  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0289  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 729000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0592    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 730000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.716    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0755  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.1e-09 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.267    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.00657 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.364   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.106   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.3      |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.127    |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 734000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.141   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 735000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0353    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0495    |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 736000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.183    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.086   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 737000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0372  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0683   |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 738000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.49     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.122     |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 739000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9986
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.319     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -686      |
| loss/alpha                         | -0.159    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 740000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.349    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.058   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.025    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.046    |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 742000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.281    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.186    |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 743000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.319   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.00951  |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 744000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.67     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0541   |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.383     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0214   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 746000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.135    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0242    |
| loss/critic1                       | 19.3      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 747000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.9e-12  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.279    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0909  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.737    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0554  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0985   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0733   |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9986
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.38e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.122    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.6      |
| eval/normalized_episode_reward_std | 18.4      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0146   |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 751000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.24     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0879  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 752000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.72      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00353   |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 753000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.269   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0232   |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 754000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.77e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.04     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.063    |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 755000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.518    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0707   |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 756000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -1.13    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0282  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 757000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.263   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.15    |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.02     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0717   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 759000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0048
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.235    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.09     |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 760000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.132   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 18.7     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 761000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.196    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0227   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 762000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0040
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.778    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0642   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 763000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 2.17     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 764000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0002
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.283     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0311    |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 765000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.136     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.00879   |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 766000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0195
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-12 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.653     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0233    |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 767000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9973
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.307    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.114     |
| loss/critic1                       | 20.6      |
| loss/critic2                       | 19.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 768000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0693  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 769000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.483    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0116   |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 20        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 770000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.398    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0295    |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 771000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.28e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.501    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0776   |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 772000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.406   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.248    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -684     |
| loss/alpha                         | -0.109   |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9979
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.11      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 15.7      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0608   |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 775000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.43     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0116   |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 776000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.272     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0976    |
| loss/critic1                       | 22.4      |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 777000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.106     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0933   |
| loss/critic1                       | 21.1      |
| loss/critic2                       | 20.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 778000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.154   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0932   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 779000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0351    |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 780000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.333   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 18.8     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0016
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.537     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0571    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 782000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.693    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0122  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.64      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0402    |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 17.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 784000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.114   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -685     |
| loss/alpha                         | 0.124    |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 785000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1.98      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0456    |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 786000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0021
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.0995   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0714  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 787000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0158
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.158    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.126    |
| loss/critic1                       | 24.3      |
| loss/critic2                       | 23.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 788000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.387   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0161   |
| loss/critic1                       | 22.7     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9909
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.455   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0586   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 790000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0076
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.709     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0536   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 791000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.819    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.051   |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 792000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.441     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0255    |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 19.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 793000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.596   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0776   |
| loss/critic1                       | 19.8     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.146    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 795000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.00424  |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 796000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.285    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0285   |
| loss/critic1                       | 20.8      |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 797000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.329    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0868  |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0006
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 4.55      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0405   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 799000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.0382   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0394    |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 800000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.138    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.159     |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 801000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.578    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.104    |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 802000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.184   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.045   |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 24.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 1.14     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0957  |
| loss/critic1                       | 26       |
| loss/critic2                       | 25.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 804000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.8e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0961   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0298   |
| loss/critic1                       | 24       |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 805000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.882   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.12    |
| loss/critic1                       | 23.3     |
| loss/critic2                       | 22.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 806000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.709    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.037    |
| loss/critic1                       | 25.9      |
| loss/critic2                       | 25.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 807000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.867    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00207 |
| loss/critic1                       | 27.4     |
| loss/critic2                       | 26.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.00783   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.13      |
| loss/critic1                       | 26.6      |
| loss/critic2                       | 25.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 809000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.336    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0405    |
| loss/critic1                       | 31.5      |
| loss/critic2                       | 30.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 810000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.329    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.089   |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 24.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.338   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0648  |
| loss/critic1                       | 21.9     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 812000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-12 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.93      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0638    |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 19.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 813000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.68     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00986  |
| loss/critic1                       | 21.3     |
| loss/critic2                       | 20.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 814000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9881
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.735    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0466  |
| loss/critic1                       | 21.8     |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 815000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.91e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.928    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 27.2      |
| loss/critic2                       | 26.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 816000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.05e-12 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.599   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0646  |
| loss/critic1                       | 25.3     |
| loss/critic2                       | 24.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 817000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 4.9975
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.289     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.011    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 818000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.801    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.112   |
| loss/critic1                       | 25.5     |
| loss/critic2                       | 24.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0022
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.781     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0298   |
| loss/critic1                       | 22        |
| loss/critic2                       | 21.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 820000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.643   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 821000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.53      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0502    |
| loss/critic1                       | 21.4      |
| loss/critic2                       | 20.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 822000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.226   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00301  |
| loss/critic1                       | 22       |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.371    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-11  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.512   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 9.56     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0609  |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.263   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0575  |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.0378   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0192   |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 25.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 827000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.998    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0978   |
| loss/critic1                       | 28.3     |
| loss/critic2                       | 27.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.452   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0663  |
| loss/critic1                       | 29.5     |
| loss/critic2                       | 29       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0371  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0527   |
| loss/critic1                       | 23       |
| loss/critic2                       | 22.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 830000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.336   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.127    |
| loss/critic1                       | 25.5     |
| loss/critic2                       | 25.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 831000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.71e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.292    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0729   |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 22.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 832000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.0538   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.132    |
| loss/critic1                       | 27.3      |
| loss/critic2                       | 26.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 833000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9895
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.38      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.214    |
| loss/critic1                       | 25.8      |
| loss/critic2                       | 25        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 834000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.0668  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0716  |
| loss/critic1                       | 36.1     |
| loss/critic2                       | 35.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 835000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.445     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.229     |
| loss/critic1                       | 34.8      |
| loss/critic2                       | 34        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 836000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0347   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 4.49     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.12     |
| loss/critic1                       | 29.2     |
| loss/critic2                       | 28.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.145   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0789  |
| loss/critic1                       | 21.2     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 838000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0084
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0323    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0242    |
| loss/critic1                       | 20.1      |
| loss/critic2                       | 19.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 839000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -1.01    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0917  |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0952   |
| loss/critic1                       | 20.3     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 841000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.474    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00925 |
| loss/critic1                       | 24.1     |
| loss/critic2                       | 23.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 842000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.0631    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -684      |
| loss/alpha                         | -0.00204  |
| loss/critic1                       | 26        |
| loss/critic2                       | 25.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 843000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 4.9969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -1.11     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0111    |
| loss/critic1                       | 22.7      |
| loss/critic2                       | 21.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 844000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.112   |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 845000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0072
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.367     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0759    |
| loss/critic1                       | 25        |
| loss/critic2                       | 24.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 846000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.298     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.069     |
| loss/critic1                       | 21.4      |
| loss/critic2                       | 20.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 847000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9943
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.938    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0853    |
| loss/critic1                       | 23.3      |
| loss/critic2                       | 22.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 848000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0143
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0398    |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 20        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 849000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.0554  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0956  |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 850000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.526   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.149   |
| loss/critic1                       | 22.9     |
| loss/critic2                       | 22.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0574  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0724  |
| loss/critic1                       | 21.8     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 852000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.221     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.13      |
| loss/critic1                       | 24.1      |
| loss/critic2                       | 23.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 853000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.603   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.006    |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 21.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 854000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.52e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.194   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 29       |
| loss/critic2                       | 27.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 855000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0112
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.31e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 1.1       |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0441    |
| loss/critic1                       | 29.7      |
| loss/critic2                       | 28.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 856000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0007
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.984     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.144     |
| loss/critic1                       | 27.8      |
| loss/critic2                       | 27        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 857000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.852     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0572   |
| loss/critic1                       | 25.5      |
| loss/critic2                       | 24.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 858000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.787    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0755  |
| loss/critic1                       | 20.2     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 859000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9910
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.886     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0735   |
| loss/critic1                       | 25        |
| loss/critic2                       | 23.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 860000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9874
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.827     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0464    |
| loss/critic1                       | 25.9      |
| loss/critic2                       | 25.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 861000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.0976   |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0733   |
| loss/critic1                       | 25.8      |
| loss/critic2                       | 25.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 862000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.556    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.131    |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 863000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.306     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.124    |
| loss/critic1                       | 23        |
| loss/critic2                       | 22.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 864000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.475    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.00715 |
| loss/critic1                       | 22       |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 865000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.378   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0332   |
| loss/critic1                       | 22       |
| loss/critic2                       | 21.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 866000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.46e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.689    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0392    |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 867000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.189   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.00918 |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 868000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9e-10    |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.664   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.03    |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 869000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.04     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.613    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0484   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 871000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0082
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.238     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0411   |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 872000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.735    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00408  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 21       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 873000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -1.17    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.156    |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 25.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 874000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.0936   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 21.9     |
| loss/critic2                       | 21.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 875000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0343    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0383   |
| loss/critic1                       | 26.1      |
| loss/critic2                       | 25.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 876000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0051
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.319   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0553  |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 877000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.626    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 3.57      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0214   |
| loss/critic1                       | 20        |
| loss/critic2                       | 19.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 878000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.706    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0159  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.192   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 880000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.905     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0485    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 881000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.244     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.67      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0155   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 882000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.414    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0507  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 883000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9983
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.704     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.138    |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 18.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 884000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.911     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.171     |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 885000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9921
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.67e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.417     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.23      |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 886000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.309   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0922  |
| loss/critic1                       | 19.5     |
| loss/critic2                       | 18.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9965
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.709    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0679   |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 20.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 888000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -1.33     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.00219  |
| loss/critic1                       | 21.5      |
| loss/critic2                       | 20.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 889000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.156     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.108    |
| loss/critic1                       | 26.2      |
| loss/critic2                       | 25.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 890000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.574   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.129   |
| loss/critic1                       | 22       |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 891000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0261
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.704     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.00882   |
| loss/critic1                       | 23.5      |
| loss/critic2                       | 22.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 892000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.206    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 23.1     |
| loss/critic2                       | 22.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -1.37    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 894000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.0342   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.12    |
| loss/critic1                       | 25.4     |
| loss/critic2                       | 24.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 895000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.512    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0326   |
| loss/critic1                       | 22.8      |
| loss/critic2                       | 22.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 896000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 1.57     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.152    |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 21.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 897000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.911     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0243   |
| loss/critic1                       | 23.5      |
| loss/critic2                       | 22.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 898000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9910
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.758   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0656  |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 25       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 899000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.159   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.212   |
| loss/critic1                       | 23.7     |
| loss/critic2                       | 23.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 900000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9954
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.114     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0458   |
| loss/critic1                       | 22.3      |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 901000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0008
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.672     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0537    |
| loss/critic1                       | 21.3      |
| loss/critic2                       | 20.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 902000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-10   |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.279    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.00405 |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9918
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.22     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0982   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.305    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.021     |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 905000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.349    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0516  |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 906000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9905
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.645    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 907000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.0277  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 908000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.111    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.123   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 909000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.404   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.173    |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9953
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.827     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0285   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 911000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.475     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.13      |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 17.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 912000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.648    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0535  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 913000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.753     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0646   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 914000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.128   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 18.5     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.446    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.41      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0254    |
| loss/critic1                       | 22.9      |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 916000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.295   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0187  |
| loss/critic1                       | 19.7     |
| loss/critic2                       | 19.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-11    |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.288   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0725   |
| loss/critic1                       | 25.7     |
| loss/critic2                       | 24.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0077
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.791     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.00821  |
| loss/critic1                       | 23        |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 919000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.911    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.03     |
| loss/critic1                       | 21.9      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 920000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.669    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0234   |
| loss/critic1                       | 24.2      |
| loss/critic2                       | 23.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 921000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9897
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.21    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0359   |
| loss/critic1                       | 26.3     |
| loss/critic2                       | 25.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 922000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 1.48      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0159    |
| loss/critic1                       | 33.9      |
| loss/critic2                       | 33.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 923000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.435   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.194    |
| loss/critic1                       | 31       |
| loss/critic2                       | 30.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0042
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.202    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0206    |
| loss/critic1                       | 33.4      |
| loss/critic2                       | 32.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 925000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 5.0030
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.668     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.164     |
| loss/critic1                       | 30.2      |
| loss/critic2                       | 28.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 926000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.375   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.168   |
| loss/critic1                       | 32.6     |
| loss/critic2                       | 32.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 927000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10   |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.838    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.228   |
| loss/critic1                       | 33.6     |
| loss/critic2                       | 33.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 928000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9873
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.13     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0628   |
| loss/critic1                       | 40.1     |
| loss/critic2                       | 39.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 929000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.209   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 38.8     |
| loss/critic2                       | 38.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 930000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 5.0020
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.334    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.217    |
| loss/critic1                       | 39.8      |
| loss/critic2                       | 38.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 931000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.0595   |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.00293  |
| loss/critic1                       | 41.3      |
| loss/critic2                       | 40.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 932000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.149   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.037    |
| loss/critic1                       | 40.5     |
| loss/critic2                       | 39.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 933000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9982
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.766   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.256    |
| loss/critic1                       | 36.4     |
| loss/critic2                       | 35.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.583    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 29       |
| loss/critic2                       | 28.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.584     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0563    |
| loss/critic1                       | 34.2      |
| loss/critic2                       | 33.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 936000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 4.9973
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-11  |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.000821 |
| loss/critic1                       | 30.5      |
| loss/critic2                       | 29.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 937000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.5       |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0363   |
| loss/critic1                       | 31.8      |
| loss/critic2                       | 31.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 938000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0084
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.412     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81        |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.113    |
| loss/critic1                       | 31.4      |
| loss/critic2                       | 31.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 939000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.08e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.0831  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0795  |
| loss/critic1                       | 41.6     |
| loss/critic2                       | 40.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 940000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.248    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.00814  |
| loss/critic1                       | 32.4      |
| loss/critic2                       | 31.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 941000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.972    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.116    |
| loss/critic1                       | 28.7     |
| loss/critic2                       | 28.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 942000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0005
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.179   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0527   |
| loss/critic1                       | 35.6     |
| loss/critic2                       | 34.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 943000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.273    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0465   |
| loss/critic1                       | 34.6      |
| loss/critic2                       | 33.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 944000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.498   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.127   |
| loss/critic1                       | 29.7     |
| loss/critic2                       | 29.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0076
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.435     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.1       |
| loss/critic1                       | 45.1      |
| loss/critic2                       | 43.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 946000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.519   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.138    |
| loss/critic1                       | 35.9     |
| loss/critic2                       | 35.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 947000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.527     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0657   |
| loss/critic1                       | 36.3      |
| loss/critic2                       | 35.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 948000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.343    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 41.2     |
| loss/critic2                       | 40.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 949000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.225    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0464   |
| loss/critic1                       | 37.8      |
| loss/critic2                       | 37.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 950000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9860
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.0765   |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.016     |
| loss/critic1                       | 32.7      |
| loss/critic2                       | 32.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 951000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.128     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0704   |
| loss/critic1                       | 44.1      |
| loss/critic2                       | 42.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 952000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9864
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.468     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0377    |
| loss/critic1                       | 40.6      |
| loss/critic2                       | 39.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 953000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-11  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.515   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0636   |
| loss/critic1                       | 38       |
| loss/critic2                       | 37.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 954000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.03e-12  |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.64      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.000915 |
| loss/critic1                       | 43.7      |
| loss/critic2                       | 43.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 955000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9992
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.16e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.0143    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 12.5      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.108    |
| loss/critic1                       | 50.1      |
| loss/critic2                       | 49.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 956000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.442   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 25.1     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0124  |
| loss/critic1                       | 52       |
| loss/critic2                       | 50.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 957000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9799
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.92    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.185    |
| loss/critic1                       | 62.5     |
| loss/critic2                       | 62.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.68     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0422   |
| loss/critic1                       | 41.9     |
| loss/critic2                       | 41       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 959000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.45e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.0912   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 67.1     |
| loss/critic2                       | 65.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 960000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.572   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -682     |
| loss/alpha                         | 0.113    |
| loss/critic1                       | 58.3     |
| loss/critic2                       | 56.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.676    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.227     |
| loss/critic1                       | 54.5      |
| loss/critic2                       | 53.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 962000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.192    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 4.4       |
| loss/actor                         | -682      |
| loss/alpha                         | -0.265    |
| loss/critic1                       | 51.5      |
| loss/critic2                       | 49.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 963000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.318    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.224   |
| loss/critic1                       | 49.1     |
| loss/critic2                       | 47.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 964000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0127
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.582    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.105    |
| loss/critic1                       | 45.9      |
| loss/critic2                       | 45.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 965000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4e-10    |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -683     |
| loss/alpha                         | 0.141    |
| loss/critic1                       | 67.5     |
| loss/critic2                       | 66.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 966000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.62     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0242  |
| loss/critic1                       | 40.9     |
| loss/critic2                       | 39.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 967000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.77    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0562   |
| loss/critic1                       | 53.6     |
| loss/critic2                       | 53.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9743
num rollout transitions: 250000, reward mean: 4.9951
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.15      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.141     |
| loss/critic1                       | 57.8      |
| loss/critic2                       | 57        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 969000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 4.9978
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-09 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.709     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0133    |
| loss/critic1                       | 69.8      |
| loss/critic2                       | 68.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 970000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0022
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.117     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.123     |
| loss/critic1                       | 79.5      |
| loss/critic2                       | 78.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 971000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.255     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.025    |
| loss/critic1                       | 51.7      |
| loss/critic2                       | 51.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 972000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 4.9901
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.558     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0754   |
| loss/critic1                       | 49.5      |
| loss/critic2                       | 48.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 973000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.761   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.00177 |
| loss/critic1                       | 61.8     |
| loss/critic2                       | 59.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 974000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9932
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.133     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 67.1      |
| loss/critic2                       | 64.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 975000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.186   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.14    |
| loss/critic1                       | 80.7     |
| loss/critic2                       | 80.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.685    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.17    |
| loss/critic1                       | 103      |
| loss/critic2                       | 102      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 977000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.23e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.11     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.8      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0875    |
| loss/critic1                       | 98.8      |
| loss/critic2                       | 97.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 978000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.39     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 70.8     |
| loss/critic2                       | 70.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.424     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0264    |
| loss/critic1                       | 73.8      |
| loss/critic2                       | 73.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 980000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.0191  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0117  |
| loss/critic1                       | 87       |
| loss/critic2                       | 86.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.447   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.181    |
| loss/critic1                       | 71       |
| loss/critic2                       | 70.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 982000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9971
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.233    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.033    |
| loss/critic1                       | 69.4      |
| loss/critic2                       | 70        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 983000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.549   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0949  |
| loss/critic1                       | 50       |
| loss/critic2                       | 49.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 984000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.194     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0172    |
| loss/critic1                       | 42.2      |
| loss/critic2                       | 42.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 985000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0156
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.098     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0356   |
| loss/critic1                       | 50.4      |
| loss/critic2                       | 50.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 986000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 4.9964
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.0885    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.135    |
| loss/critic1                       | 54.5      |
| loss/critic2                       | 53.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 987000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.432   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.00716  |
| loss/critic1                       | 49.8     |
| loss/critic2                       | 48.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 988000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.278    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.7      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0175    |
| loss/critic1                       | 52.9      |
| loss/critic2                       | 52.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 989000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.233    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 49.5     |
| loss/critic2                       | 48.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 990000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0196
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.21     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.00719   |
| loss/critic1                       | 54.5      |
| loss/critic2                       | 53.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 991000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0972   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 7.78     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.127    |
| loss/critic1                       | 65.1     |
| loss/critic2                       | 65.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.491     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.2      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0831    |
| loss/critic1                       | 51.4      |
| loss/critic2                       | 50.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 993000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.525    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.192   |
| loss/critic1                       | 54.1     |
| loss/critic2                       | 52.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0031
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.65e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.434     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.6      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.121    |
| loss/critic1                       | 45.7      |
| loss/critic2                       | 43.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 995000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.879    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0377   |
| loss/critic1                       | 45.1     |
| loss/critic2                       | 44       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 996000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9882
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.441     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.7      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.329     |
| loss/critic1                       | 46        |
| loss/critic2                       | 46        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 997000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9811
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.291    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0568    |
| loss/critic1                       | 50.1      |
| loss/critic2                       | 48.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 998000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.678   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.111   |
| loss/critic1                       | 52.1     |
| loss/critic2                       | 51.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 999000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.297    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0313   |
| loss/critic1                       | 73.7      |
| loss/critic2                       | 73.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1000000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.209     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0921   |
| loss/critic1                       | 55.1      |
| loss/critic2                       | 54.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1001000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9985
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.45     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -677      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 73.2      |
| loss/critic2                       | 71.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1002000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9988
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.0476    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.172     |
| loss/critic1                       | 78.2      |
| loss/critic2                       | 78.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1003000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 1.04      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0552   |
| loss/critic1                       | 61.9      |
| loss/critic2                       | 61.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1004000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9624
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.444     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0114   |
| loss/critic1                       | 48.7      |
| loss/critic2                       | 48.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1005000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.12e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -1.26    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.146   |
| loss/critic1                       | 63.6     |
| loss/critic2                       | 62.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1006000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0127   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 16       |
| loss/actor                         | -677     |
| loss/alpha                         | 0.00677  |
| loss/critic1                       | 54.9     |
| loss/critic2                       | 53.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1007000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9767
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.422     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0826   |
| loss/critic1                       | 58.4      |
| loss/critic2                       | 58.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1008000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.165    |
| loss/critic1                       | 78.2     |
| loss/critic2                       | 78       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1009000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.31     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.148    |
| loss/critic1                       | 64        |
| loss/critic2                       | 64        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1010000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.471     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.202     |
| loss/critic1                       | 79.7      |
| loss/critic2                       | 78.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1011000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9e-10    |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.13     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -677     |
| loss/alpha                         | 0.114    |
| loss/critic1                       | 75.8     |
| loss/critic2                       | 75.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1012000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.658    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0446  |
| loss/critic1                       | 45.1     |
| loss/critic2                       | 44.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1013000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.675    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.0686  |
| loss/critic1                       | 52.3     |
| loss/critic2                       | 51.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1014000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.369    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0743   |
| loss/critic1                       | 45.5     |
| loss/critic2                       | 45.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1015000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 5.0011
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.208     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0604   |
| loss/critic1                       | 33.5      |
| loss/critic2                       | 32.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1016000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.619    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0973  |
| loss/critic1                       | 39.3     |
| loss/critic2                       | 38.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1017000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9926
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -1.46    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.207    |
| loss/critic1                       | 47.1     |
| loss/critic2                       | 46.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1018000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0052
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.188     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.17     |
| loss/critic1                       | 37.1      |
| loss/critic2                       | 36.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1019000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.83      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 44        |
| loss/critic2                       | 42.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1020000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.306    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.04     |
| loss/critic1                       | 52.1      |
| loss/critic2                       | 52.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1021000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.95e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.582     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0188   |
| loss/critic1                       | 56.6      |
| loss/critic2                       | 57.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1022000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.556    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.434    |
| loss/critic1                       | 80       |
| loss/critic2                       | 80.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1023000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.177   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0957   |
| loss/critic1                       | 86.6     |
| loss/critic2                       | 86.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1024000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.461   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.171   |
| loss/critic1                       | 74.6     |
| loss/critic2                       | 74.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1025000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.73e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.62    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 79.7     |
| loss/critic2                       | 78.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1026000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.827   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0676   |
| loss/critic1                       | 122      |
| loss/critic2                       | 120      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1027000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9986
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.33e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.35     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0213   |
| loss/critic1                       | 90.9      |
| loss/critic2                       | 90.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1028000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.0412   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.284   |
| loss/critic1                       | 96.8     |
| loss/critic2                       | 95.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1029000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0005
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.0186   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.13    |
| loss/critic1                       | 81.9     |
| loss/critic2                       | 82.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1030000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.31e-11 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.855   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.134    |
| loss/critic1                       | 69.7     |
| loss/critic2                       | 69.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1031000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0174
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.381   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.284    |
| loss/critic1                       | 88.2     |
| loss/critic2                       | 88.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1032000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.77e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.1       |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0997   |
| loss/critic1                       | 76        |
| loss/critic2                       | 75.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1033000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.15    |
| loss/critic1                       | 57.8     |
| loss/critic2                       | 56.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1034000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 4.9887
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.644    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 78.4     |
| loss/critic2                       | 78.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1035000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.515    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.143    |
| loss/critic1                       | 72.8     |
| loss/critic2                       | 71.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1036000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.852    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 20.9      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.126    |
| loss/critic1                       | 67.4      |
| loss/critic2                       | 67.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1037000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0011
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.606    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 8.89      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.267     |
| loss/critic1                       | 73.4      |
| loss/critic2                       | 74        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1038000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.000129  |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0065    |
| loss/critic1                       | 40.3      |
| loss/critic2                       | 39.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1039000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.951     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.163     |
| loss/critic1                       | 42.8      |
| loss/critic2                       | 41.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1040000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.19e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.277    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.2      |
| loss/critic1                       | 63.8      |
| loss/critic2                       | 65.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1041000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.486    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.329   |
| loss/critic1                       | 87.3     |
| loss/critic2                       | 87.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1042000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.328    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.00214 |
| loss/critic1                       | 82.5     |
| loss/critic2                       | 81.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1043000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.0804    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0312    |
| loss/critic1                       | 95.6      |
| loss/critic2                       | 96.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1044000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.193    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 23.1      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.00554  |
| loss/critic1                       | 108       |
| loss/critic2                       | 107       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1045000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.41e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.0248  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.481    |
| loss/critic1                       | 117      |
| loss/critic2                       | 116      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1046000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.118    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.094    |
| loss/critic1                       | 72.8     |
| loss/critic2                       | 70.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1047000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9844
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.574     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.147     |
| loss/critic1                       | 123       |
| loss/critic2                       | 122       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1048000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9693
num rollout transitions: 250000, reward mean: 4.9806
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.403    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.141     |
| loss/critic1                       | 84.2      |
| loss/critic2                       | 82.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 1049000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9756
num rollout transitions: 250000, reward mean: 4.9741
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9815
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 1.44     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.188    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0945   |
| loss/critic1                       | 92.4     |
| loss/critic2                       | 91.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 1050000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9876
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 5.13     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.191   |
| loss/critic1                       | 78.1     |
| loss/critic2                       | 78.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1051000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9948
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.227    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.377   |
| loss/critic1                       | 92.4     |
| loss/critic2                       | 91.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1052000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.176    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.326   |
| loss/critic1                       | 73.8     |
| loss/critic2                       | 75.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1053000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.288     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.109    |
| loss/critic1                       | 111       |
| loss/critic2                       | 110       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1054000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.464   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 109      |
| loss/critic2                       | 108      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1055000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.537    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.00236  |
| loss/critic1                       | 102       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1056000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.0848   |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.334     |
| loss/critic1                       | 132       |
| loss/critic2                       | 133       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1057000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0017
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -1.08     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.64      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.065     |
| loss/critic1                       | 132       |
| loss/critic2                       | 134       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1058000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.21e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -1.17    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.00905  |
| loss/critic1                       | 135      |
| loss/critic2                       | 135      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1059000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9996
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.133     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.41      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0852    |
| loss/critic1                       | 103       |
| loss/critic2                       | 102       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1060000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.427    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0598   |
| loss/critic1                       | 79.7      |
| loss/critic2                       | 79.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1061000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9978
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-09 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.169    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0206   |
| loss/critic1                       | 85.2      |
| loss/critic2                       | 84.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1062000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.799     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 20.9      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.189    |
| loss/critic1                       | 111       |
| loss/critic2                       | 109       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1063000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -1.87    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.047    |
| loss/critic1                       | 76.4     |
| loss/critic2                       | 76.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1064000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.24e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.214     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.73      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.12      |
| loss/critic1                       | 138       |
| loss/critic2                       | 137       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1065000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9784
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 5.0021
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.15e-09 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.0347    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -674      |
| loss/alpha                         | 0.104     |
| loss/critic1                       | 107       |
| loss/critic2                       | 107       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1066000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9784
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.235     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.00522   |
| loss/critic1                       | 104       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1067000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.608    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.026   |
| loss/critic1                       | 93.1     |
| loss/critic2                       | 90.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1068000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9915
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.3e-10  |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.196    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0234   |
| loss/critic1                       | 109      |
| loss/critic2                       | 108      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1069000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9822
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.203    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.134   |
| loss/critic1                       | 117      |
| loss/critic2                       | 117      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1070000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9998
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.7      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.7       |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0554   |
| loss/critic1                       | 100       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1071000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.8e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.235   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0371  |
| loss/critic1                       | 83.4     |
| loss/critic2                       | 83.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1072000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0010
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.708    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.11     |
| loss/critic1                       | 105      |
| loss/critic2                       | 106      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1073000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9922
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | -0.43     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0405   |
| loss/critic1                       | 123       |
| loss/critic2                       | 119       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1074000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9936
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-11  |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.528   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.084    |
| loss/critic1                       | 129      |
| loss/critic2                       | 123      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1075000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 4.9945
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.741    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -671     |
| loss/alpha                         | -0.188   |
| loss/critic1                       | 81.9     |
| loss/critic2                       | 80.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1076000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 4.9824
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.143     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0267    |
| loss/critic1                       | 79.1      |
| loss/critic2                       | 78.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1077000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.913    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0405   |
| loss/critic1                       | 75.7     |
| loss/critic2                       | 75.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1078000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.84e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -1.03     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.00641  |
| loss/critic1                       | 60        |
| loss/critic2                       | 58.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1079000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0187
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.9e-10  |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.872    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.119   |
| loss/critic1                       | 50.2     |
| loss/critic2                       | 48.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1080000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0182
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.632   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0714  |
| loss/critic1                       | 82.6     |
| loss/critic2                       | 82.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1081000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.726    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0938   |
| loss/critic1                       | 87.5     |
| loss/critic2                       | 87.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1082000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.44e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.391     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.2      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.169     |
| loss/critic1                       | 57.2      |
| loss/critic2                       | 56.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1083000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.853   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 65.1     |
| loss/critic2                       | 64       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1084000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.63      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.218     |
| loss/critic1                       | 78.9      |
| loss/critic2                       | 79.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1085000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-09 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.346     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0667    |
| loss/critic1                       | 72.5      |
| loss/critic2                       | 72.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1086000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.218   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 96.3     |
| loss/critic2                       | 95.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1087000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.146    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.205   |
| loss/critic1                       | 68.7     |
| loss/critic2                       | 69.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1088000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9933
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.1e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.0576  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -672     |
| loss/alpha                         | 0.141    |
| loss/critic1                       | 104      |
| loss/critic2                       | 105      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1089000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.469    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.056     |
| loss/critic1                       | 86.9      |
| loss/critic2                       | 86.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1090000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9901
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.25e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.113    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.0261    |
| loss/critic1                       | 111       |
| loss/critic2                       | 111       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1091000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9848
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 1.37     |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0698   |
| loss/critic1                       | 124      |
| loss/critic2                       | 122      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1092000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.639     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.00988   |
| loss/critic1                       | 107       |
| loss/critic2                       | 106       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1093000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0016
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.722    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 10.6      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.194    |
| loss/critic1                       | 77.7      |
| loss/critic2                       | 77.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1094000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9960
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.83e-11 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | -0.269    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 15.9      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0126   |
| loss/critic1                       | 107       |
| loss/critic2                       | 107       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1095000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.45e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.911    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.57     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0632  |
| loss/critic1                       | 75.7     |
| loss/critic2                       | 76.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1096000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9924
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.225    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -675      |
| loss/alpha                         | 0.0402    |
| loss/critic1                       | 84.4      |
| loss/critic2                       | 86.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1097000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.616    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0981   |
| loss/critic1                       | 71.4      |
| loss/critic2                       | 69.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1098000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.68e-12 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.76     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.054    |
| loss/critic1                       | 44.3     |
| loss/critic2                       | 43.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1099000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.3      |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 11.9     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 66       |
| loss/critic2                       | 64       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1100000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.17e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.0273  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0481  |
| loss/critic1                       | 51.3     |
| loss/critic2                       | 48.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1101000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.736    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.148    |
| loss/critic1                       | 82.2     |
| loss/critic2                       | 81       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1102000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9894
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9865
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.168    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 15.2      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0296    |
| loss/critic1                       | 91.5      |
| loss/critic2                       | 90.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1103000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.561    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.72      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.00732  |
| loss/critic1                       | 98.2      |
| loss/critic2                       | 99.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1104000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.57e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.738   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0961   |
| loss/critic1                       | 75.6     |
| loss/critic2                       | 73.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1105000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-11  |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.204   |
| loss/critic1                       | 56.7     |
| loss/critic2                       | 55.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1106000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-10  |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.204    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0375  |
| loss/critic1                       | 62.7     |
| loss/critic2                       | 61       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1107000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.269     |
| loss/critic1                       | 93.3      |
| loss/critic2                       | 91.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1108000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.27e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.332    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.207    |
| loss/critic1                       | 118      |
| loss/critic2                       | 117      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1109000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9962
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.44e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.516    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0799   |
| loss/critic1                       | 109       |
| loss/critic2                       | 111       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1110000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0024
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.59e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.122     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -675      |
| loss/alpha                         | 0.0611    |
| loss/critic1                       | 135       |
| loss/critic2                       | 134       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1111000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9925
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.225    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0407   |
| loss/critic1                       | 170       |
| loss/critic2                       | 166       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1112000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9703
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9773
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.2e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.0962   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0718  |
| loss/critic1                       | 130      |
| loss/critic2                       | 131      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 1113000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9934
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.356     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.205     |
| loss/critic1                       | 121       |
| loss/critic2                       | 123       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1114000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.475   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.159   |
| loss/critic1                       | 129      |
| loss/critic2                       | 128      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1115000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.0735  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0181  |
| loss/critic1                       | 124      |
| loss/critic2                       | 127      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1116000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.38      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 10.3      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0506   |
| loss/critic1                       | 133       |
| loss/critic2                       | 131       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1117000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.251    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0986  |
| loss/critic1                       | 125      |
| loss/critic2                       | 126      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1118000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.68e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.944   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.134    |
| loss/critic1                       | 123      |
| loss/critic2                       | 124      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1119000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.389   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 10.1     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0675  |
| loss/critic1                       | 129      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1120000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.67    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0833   |
| loss/critic1                       | 160      |
| loss/critic2                       | 168      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1121000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.0663  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0987  |
| loss/critic1                       | 175      |
| loss/critic2                       | 177      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1122000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.0671    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.0275    |
| loss/critic1                       | 131       |
| loss/critic2                       | 132       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1123000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0157
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.34e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.338     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.00379  |
| loss/critic1                       | 198       |
| loss/critic2                       | 200       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1124000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.61e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.135   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 9.86     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0293   |
| loss/critic1                       | 168      |
| loss/critic2                       | 169      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1125000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.719    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.168   |
| loss/critic1                       | 86.2     |
| loss/critic2                       | 82.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1126000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.0115  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0328  |
| loss/critic1                       | 86.1     |
| loss/critic2                       | 88.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1127000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9981
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0429   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.23     |
| loss/critic1                       | 88.3     |
| loss/critic2                       | 85.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1128000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 4.9958
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.344    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.268    |
| loss/critic1                       | 129      |
| loss/critic2                       | 125      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1129000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9893
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.289    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.316    |
| loss/critic1                       | 75.3      |
| loss/critic2                       | 74.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1130000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.737   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0268  |
| loss/critic1                       | 125      |
| loss/critic2                       | 128      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1131000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9733
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-10  |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.328    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0681   |
| loss/critic1                       | 113      |
| loss/critic2                       | 113      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1132000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9910
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.671    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.131     |
| loss/critic1                       | 142       |
| loss/critic2                       | 141       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1133000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.5e-10  |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.306    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0501   |
| loss/critic1                       | 102      |
| loss/critic2                       | 100      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1134000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0152
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.334     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.207    |
| loss/critic1                       | 149       |
| loss/critic2                       | 151       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1135000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.85e-12 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.13     |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0163   |
| loss/critic1                       | 77.4     |
| loss/critic2                       | 78.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1136000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -1.01    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0817  |
| loss/critic1                       | 90.7     |
| loss/critic2                       | 92       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1137000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.166    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0401  |
| loss/critic1                       | 94.5     |
| loss/critic2                       | 93.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1138000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0091
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.67e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.502   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0394   |
| loss/critic1                       | 87.4     |
| loss/critic2                       | 88.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1139000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.6e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0149   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0203   |
| loss/critic1                       | 79.5     |
| loss/critic2                       | 80.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1140000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.371    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.48     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.031   |
| loss/critic1                       | 78       |
| loss/critic2                       | 77.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1141000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.56      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0338   |
| loss/critic1                       | 68.2      |
| loss/critic2                       | 66.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1142000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.0781   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.015   |
| loss/critic1                       | 68.4     |
| loss/critic2                       | 67.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1143000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9898
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.706   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.292    |
| loss/critic1                       | 102      |
| loss/critic2                       | 101      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1144000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.113    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0308   |
| loss/critic1                       | 86.7     |
| loss/critic2                       | 86.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1145000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.24    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.193   |
| loss/critic1                       | 77.1     |
| loss/critic2                       | 75.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1146000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9996
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.04e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.133    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 11.8      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.00478  |
| loss/critic1                       | 65.8      |
| loss/critic2                       | 65.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1147000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-11 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | -0.16     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.149    |
| loss/critic1                       | 90        |
| loss/critic2                       | 91.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1148000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.0158   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0982   |
| loss/critic1                       | 76.3     |
| loss/critic2                       | 74.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1149000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0171
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | -0.318    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0945   |
| loss/critic1                       | 105       |
| loss/critic2                       | 104       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1150000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.0174  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0239   |
| loss/critic1                       | 75.7     |
| loss/critic2                       | 75       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1151000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.643     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.0418   |
| loss/critic1                       | 82.7      |
| loss/critic2                       | 84.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1152000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-10  |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.119    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0598   |
| loss/critic1                       | 93.8     |
| loss/critic2                       | 92.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1153000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.75e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.147   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.111    |
| loss/critic1                       | 78.6     |
| loss/critic2                       | 77.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1154000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9964
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.322     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -674      |
| loss/alpha                         | 0.132     |
| loss/critic1                       | 76.7      |
| loss/critic2                       | 75.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1155000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 4.9965
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.899    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0956  |
| loss/critic1                       | 77.2     |
| loss/critic2                       | 77.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1156000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.98e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.322     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.04     |
| loss/critic1                       | 96.3      |
| loss/critic2                       | 97.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1157000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.0332   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0517  |
| loss/critic1                       | 83.1     |
| loss/critic2                       | 83       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1158000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.173   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.233   |
| loss/critic1                       | 84.4     |
| loss/critic2                       | 85.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1159000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.442   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 92.1     |
| loss/critic2                       | 92.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1160000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.35e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.701   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -676     |
| loss/alpha                         | 0.0396   |
| loss/critic1                       | 73.9     |
| loss/critic2                       | 70.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1161000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0816  |
| loss/critic1                       | 71.1     |
| loss/critic2                       | 69.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1162000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-11  |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.495    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.188    |
| loss/critic1                       | 89.2     |
| loss/critic2                       | 89.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1163000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0043
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.288    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.127     |
| loss/critic1                       | 72.1      |
| loss/critic2                       | 72.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1164000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0018
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.31e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.0796   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -678      |
| loss/alpha                         | -0.088    |
| loss/critic1                       | 61.4      |
| loss/critic2                       | 61.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1165000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.0925   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -677      |
| loss/alpha                         | 0.131     |
| loss/critic1                       | 65.8      |
| loss/critic2                       | 64.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1166000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.47e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.236    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.9      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.026     |
| loss/critic1                       | 53.8      |
| loss/critic2                       | 54.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1167000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.414   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 57.2     |
| loss/critic2                       | 57.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1168000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.42e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.353     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.107     |
| loss/critic1                       | 54        |
| loss/critic2                       | 53.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1169000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.0761    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0846   |
| loss/critic1                       | 35.4      |
| loss/critic2                       | 33.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1170000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.109   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.157   |
| loss/critic1                       | 41.1     |
| loss/critic2                       | 39.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1171000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0142
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.3       |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 55.4      |
| loss/critic2                       | 54.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1172000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.43     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0386   |
| loss/critic1                       | 47.6      |
| loss/critic2                       | 47.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1173000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0159
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.52e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.275    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0406   |
| loss/critic1                       | 44.8     |
| loss/critic2                       | 43.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1174000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.52e-12 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -1.04    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.131   |
| loss/critic1                       | 41.3     |
| loss/critic2                       | 41.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1175000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.886    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.45     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0949  |
| loss/critic1                       | 47.6     |
| loss/critic2                       | 47.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1176000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.744    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0727   |
| loss/critic1                       | 42.5     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1177000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.00732  |
| loss/critic1                       | 50.6     |
| loss/critic2                       | 49.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1178000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.61e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.433    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.0922    |
| loss/critic1                       | 37        |
| loss/critic2                       | 36.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1179000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.00454  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 45       |
| loss/critic2                       | 43.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1180000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.914   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -677     |
| loss/alpha                         | 0.16     |
| loss/critic1                       | 49.7     |
| loss/critic2                       | 49.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1181000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0136
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | -0.255    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0313   |
| loss/critic1                       | 39.8      |
| loss/critic2                       | 38.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1182000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.5e-10  |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.529   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.114   |
| loss/critic1                       | 34.8     |
| loss/critic2                       | 34.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1183000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.65     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.00784   |
| loss/critic1                       | 43.3      |
| loss/critic2                       | 44.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1184000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -1.11    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.47     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0225   |
| loss/critic1                       | 36.6     |
| loss/critic2                       | 34.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1185000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.23     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0579   |
| loss/critic1                       | 27.2      |
| loss/critic2                       | 26.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1186000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.88e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0742   |
| loss/critic1                       | 41.5     |
| loss/critic2                       | 41.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1187000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 4.9984
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.122   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.176   |
| loss/critic1                       | 37       |
| loss/critic2                       | 36.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1188000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9874
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.141    |
| loss/critic1                       | 59       |
| loss/critic2                       | 56.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1189000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.18e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.282    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0298  |
| loss/critic1                       | 50.4     |
| loss/critic2                       | 50.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1190000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.376    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.127   |
| loss/critic1                       | 48.1     |
| loss/critic2                       | 48       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1191000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -1.19    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0709  |
| loss/critic1                       | 38.9     |
| loss/critic2                       | 37.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1192000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | -0.224    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -681      |
| loss/alpha                         | 0.155     |
| loss/critic1                       | 62.8      |
| loss/critic2                       | 64.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1193000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9952
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.68      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.148     |
| loss/critic1                       | 61.4      |
| loss/critic2                       | 61.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1194000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.48    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0532   |
| loss/critic1                       | 46.5     |
| loss/critic2                       | 46       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1195000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.724    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0116  |
| loss/critic1                       | 59.7     |
| loss/critic2                       | 58.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1196000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.304    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.104   |
| loss/critic1                       | 44.4     |
| loss/critic2                       | 42.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1197000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.0177    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0165   |
| loss/critic1                       | 45        |
| loss/critic2                       | 45        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1198000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.89    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0564  |
| loss/critic1                       | 35.6     |
| loss/critic2                       | 34.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1199000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.119     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0231   |
| loss/critic1                       | 41.4      |
| loss/critic2                       | 41.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1200000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.8e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.239   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0798  |
| loss/critic1                       | 40.1     |
| loss/critic2                       | 39.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1201000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.9e-10  |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.733    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0393   |
| loss/critic1                       | 47.6     |
| loss/critic2                       | 47.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1202000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.1e-11  |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.833    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.49     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0644   |
| loss/critic1                       | 45.4     |
| loss/critic2                       | 44.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1203000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.5e-10  |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.374   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0393   |
| loss/critic1                       | 50.2     |
| loss/critic2                       | 49.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1204000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.374    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0411   |
| loss/critic1                       | 36.9      |
| loss/critic2                       | 36.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1205000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0131
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0407   |
| loss/critic1                       | 48.3      |
| loss/critic2                       | 47        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1206000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0222
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.711     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.000697 |
| loss/critic1                       | 39        |
| loss/critic2                       | 37.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1207000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.45e-11 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 1.2       |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0821   |
| loss/critic1                       | 31.4      |
| loss/critic2                       | 31        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1208000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.7e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.878    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.07     |
| loss/critic1                       | 47.2     |
| loss/critic2                       | 46.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1209000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.698     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.234     |
| loss/critic1                       | 34        |
| loss/critic2                       | 33.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1210000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.106    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0791    |
| loss/critic1                       | 29.2      |
| loss/critic2                       | 28.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1211000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.785    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0831  |
| loss/critic1                       | 27.6     |
| loss/critic2                       | 26.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1212000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.25     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.239    |
| loss/critic1                       | 29.3      |
| loss/critic2                       | 27.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1213000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0267
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.827    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00204   |
| loss/critic1                       | 38.4      |
| loss/critic2                       | 37.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1214000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.233     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.048    |
| loss/critic1                       | 43.7      |
| loss/critic2                       | 42.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1215000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.132     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.45      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0319    |
| loss/critic1                       | 37        |
| loss/critic2                       | 35.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1216000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.3e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.631    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.284    |
| loss/critic1                       | 46.9     |
| loss/critic2                       | 45.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1217000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.9e-10  |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.231   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 36.8     |
| loss/critic2                       | 35.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1218000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.59e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.309    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0585   |
| loss/critic1                       | 37.4      |
| loss/critic2                       | 36.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1219000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.324    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0169  |
| loss/critic1                       | 40.6     |
| loss/critic2                       | 40.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1220000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.506   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0436  |
| loss/critic1                       | 31.1     |
| loss/critic2                       | 30.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1221000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.147    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0227   |
| loss/critic1                       | 38.4      |
| loss/critic2                       | 38.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1222000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9977
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.439     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0298   |
| loss/critic1                       | 39.7      |
| loss/critic2                       | 38.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1223000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.2e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.148    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0519   |
| loss/critic1                       | 44.6     |
| loss/critic2                       | 42.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1224000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.0588   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0971   |
| loss/critic1                       | 38.4     |
| loss/critic2                       | 37.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1225000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.0528   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0709   |
| loss/critic1                       | 53       |
| loss/critic2                       | 51.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1226000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.934    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0637  |
| loss/critic1                       | 45.4     |
| loss/critic2                       | 45.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1227000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.55e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | -0.102    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.104    |
| loss/critic1                       | 67.2      |
| loss/critic2                       | 67.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1228000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | -0.296    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0924    |
| loss/critic1                       | 104       |
| loss/critic2                       | 103       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1229000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0031
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.485    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.19     |
| loss/critic1                       | 90.8      |
| loss/critic2                       | 91.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1230000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.0606  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0552   |
| loss/critic1                       | 60.5     |
| loss/critic2                       | 60.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1231000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.00747   |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.145    |
| loss/critic1                       | 55.4      |
| loss/critic2                       | 54.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1232000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.36e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.871   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0787  |
| loss/critic1                       | 59.9     |
| loss/critic2                       | 59.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1233000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 5.0046
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.92e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 1.57      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.035     |
| loss/critic1                       | 50        |
| loss/critic2                       | 50.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1234000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9868
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.166    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0723    |
| loss/critic1                       | 72        |
| loss/critic2                       | 71.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1235000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9967
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.983    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0635   |
| loss/critic1                       | 50.8     |
| loss/critic2                       | 50.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1236000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.14e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.129     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0559   |
| loss/critic1                       | 55.5      |
| loss/critic2                       | 54.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1237000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.434   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0341   |
| loss/critic1                       | 72.6     |
| loss/critic2                       | 72.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1238000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9944
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.206    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.148     |
| loss/critic1                       | 66        |
| loss/critic2                       | 65.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1239000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9969
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-10  |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.499   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.143    |
| loss/critic1                       | 74.6     |
| loss/critic2                       | 73.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1240000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.96e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -1.26    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.00357  |
| loss/critic1                       | 81       |
| loss/critic2                       | 80.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1241000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.71e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.315   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0768  |
| loss/critic1                       | 79.6     |
| loss/critic2                       | 81.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1242000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.408     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.187    |
| loss/critic1                       | 118       |
| loss/critic2                       | 119       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1243000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.0617  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.018   |
| loss/critic1                       | 105      |
| loss/critic2                       | 103      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1244000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.499    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0385    |
| loss/critic1                       | 89.8      |
| loss/critic2                       | 92        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1245000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.576     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.00743   |
| loss/critic1                       | 95.7      |
| loss/critic2                       | 93.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1246000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.396   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -685     |
| loss/alpha                         | 0.146    |
| loss/critic1                       | 119      |
| loss/critic2                       | 118      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1247000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.633    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0382  |
| loss/critic1                       | 101      |
| loss/critic2                       | 99.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1248000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.33e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.208    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0313    |
| loss/critic1                       | 128       |
| loss/critic2                       | 128       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1249000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0016
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.54e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.854   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.115    |
| loss/critic1                       | 101      |
| loss/critic2                       | 101      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1250000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0158
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.0447   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0436    |
| loss/critic1                       | 125       |
| loss/critic2                       | 123       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1251000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 5.0040
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.93e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.0806    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0543   |
| loss/critic1                       | 113       |
| loss/critic2                       | 113       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1252000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.11e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.147    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.18     |
| loss/critic1                       | 55.2      |
| loss/critic2                       | 53.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1253000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0053
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.711   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.113   |
| loss/critic1                       | 88.7     |
| loss/critic2                       | 88.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1254000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0194
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.204     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0825   |
| loss/critic1                       | 81.1      |
| loss/critic2                       | 82.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1255000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.26e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.237   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 78.3     |
| loss/critic2                       | 76.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1256000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.874     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0708    |
| loss/critic1                       | 150       |
| loss/critic2                       | 154       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1257000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9764
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.165    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0277   |
| loss/critic1                       | 119       |
| loss/critic2                       | 120       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1258000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.494   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 90.9     |
| loss/critic2                       | 92.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1259000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.364    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.083    |
| loss/critic1                       | 113      |
| loss/critic2                       | 114      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1260000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.5e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.275   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0957   |
| loss/critic1                       | 133      |
| loss/critic2                       | 134      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1261000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0088
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.963    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.179     |
| loss/critic1                       | 128       |
| loss/critic2                       | 125       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1262000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.34e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.258    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0805    |
| loss/critic1                       | 150       |
| loss/critic2                       | 149       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1263000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.494    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.00341   |
| loss/critic1                       | 150       |
| loss/critic2                       | 149       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1264000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0788  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0703   |
| loss/critic1                       | 75       |
| loss/critic2                       | 75       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1265000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.73     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0165  |
| loss/critic1                       | 78.3     |
| loss/critic2                       | 77.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1266000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.43e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.152     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 15.1      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0572   |
| loss/critic1                       | 73.7      |
| loss/critic2                       | 73.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1267000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.361   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.109   |
| loss/critic1                       | 135      |
| loss/critic2                       | 127      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1268000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.295     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0943   |
| loss/critic1                       | 107       |
| loss/critic2                       | 107       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1269000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9913
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.049    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0959   |
| loss/critic1                       | 110      |
| loss/critic2                       | 109      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1270000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0094
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.36e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.166   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.084    |
| loss/critic1                       | 90.6     |
| loss/critic2                       | 89.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1271000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.978   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.107   |
| loss/critic1                       | 76       |
| loss/critic2                       | 76.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1272000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.33e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.148   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -679     |
| loss/alpha                         | -0.00888 |
| loss/critic1                       | 106      |
| loss/critic2                       | 104      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1273000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.48e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.6     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0212   |
| loss/critic1                       | 86.2     |
| loss/critic2                       | 85       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1274000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.64e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.5      |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.119   |
| loss/critic1                       | 64.8     |
| loss/critic2                       | 62.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1275000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9964
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.5e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.988   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0176  |
| loss/critic1                       | 80.8     |
| loss/critic2                       | 81.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1276000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9928
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.865    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0241   |
| loss/critic1                       | 70.4      |
| loss/critic2                       | 68.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1277000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0042
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.873     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0587   |
| loss/critic1                       | 62.3      |
| loss/critic2                       | 61.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1278000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.78e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.595   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0488  |
| loss/critic1                       | 42.7     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1279000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.292    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0774  |
| loss/critic1                       | 51.6     |
| loss/critic2                       | 48.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1280000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0174
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -1.66    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.00951 |
| loss/critic1                       | 70.7     |
| loss/critic2                       | 69.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1281000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.809     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.253     |
| loss/critic1                       | 102       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1282000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.53e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.552   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.185    |
| loss/critic1                       | 112      |
| loss/critic2                       | 113      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1283000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0016
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.355    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.00479   |
| loss/critic1                       | 92.6      |
| loss/critic2                       | 94.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1284000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 4.9994
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.453   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0776   |
| loss/critic1                       | 81.1     |
| loss/critic2                       | 82.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1285000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.13     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.128    |
| loss/critic1                       | 63.9      |
| loss/critic2                       | 63.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1286000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0036
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.85e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.807     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0915   |
| loss/critic1                       | 56.1      |
| loss/critic2                       | 56.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1287000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0106
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.24e-11 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.143     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.218    |
| loss/critic1                       | 70.1      |
| loss/critic2                       | 70.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1288000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.258    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.121    |
| loss/critic1                       | 103      |
| loss/critic2                       | 102      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1289000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.622    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.101   |
| loss/critic1                       | 107      |
| loss/critic2                       | 109      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1290000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9874
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.17e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.693    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0488   |
| loss/critic1                       | 122       |
| loss/critic2                       | 123       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1291000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0033
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.376   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.265    |
| loss/critic1                       | 83.8     |
| loss/critic2                       | 82.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1292000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9818
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.745    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0752  |
| loss/critic1                       | 87.5     |
| loss/critic2                       | 86.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1293000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.543    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0517    |
| loss/critic1                       | 150       |
| loss/critic2                       | 149       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1294000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9858
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.2e-10  |
| adv_dynamics_update/adv_log_prob   | 49.1     |
| adv_dynamics_update/adv_loss       | -0.703   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.165    |
| loss/critic1                       | 227      |
| loss/critic2                       | 225      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1295000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9724
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -0.136   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.225    |
| loss/critic1                       | 237      |
| loss/critic2                       | 242      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1296000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9921
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.341     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.023     |
| loss/critic1                       | 174       |
| loss/critic2                       | 172       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1297000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 4.9955
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-11 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.316    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0888   |
| loss/critic1                       | 169       |
| loss/critic2                       | 168       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1298000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.4e-11  |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.568   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0679   |
| loss/critic1                       | 124      |
| loss/critic2                       | 125      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1299000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.21e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.378    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.182   |
| loss/critic1                       | 97       |
| loss/critic2                       | 98.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1300000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.555    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0616   |
| loss/critic1                       | 99.4      |
| loss/critic2                       | 97.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1301000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.91e-11 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.0664   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.152    |
| loss/critic1                       | 69.6      |
| loss/critic2                       | 67.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1302000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.0252    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0844   |
| loss/critic1                       | 73.3      |
| loss/critic2                       | 71.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1303000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.0927   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -680     |
| loss/alpha                         | 0.00812  |
| loss/critic1                       | 104      |
| loss/critic2                       | 105      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1304000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9955
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.17e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.59     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0989   |
| loss/critic1                       | 75.7     |
| loss/critic2                       | 75.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1305000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0112
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.0764    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.174     |
| loss/critic1                       | 47.7      |
| loss/critic2                       | 46        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1306000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.0878  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0863   |
| loss/critic1                       | 56.4     |
| loss/critic2                       | 55.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1307000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.08e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.436    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -680      |
| loss/alpha                         | 0.00534   |
| loss/critic1                       | 64.9      |
| loss/critic2                       | 65.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1308000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.9e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.00553 |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0825   |
| loss/critic1                       | 62.8     |
| loss/critic2                       | 62.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1309000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.00722  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 54.4     |
| loss/critic2                       | 52.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1310000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.76e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.0822    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.00983  |
| loss/critic1                       | 90.1      |
| loss/critic2                       | 88.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1311000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.0382  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0942  |
| loss/critic1                       | 54.4     |
| loss/critic2                       | 54.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1312000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0108
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.459     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -680      |
| loss/alpha                         | -0.0337   |
| loss/critic1                       | 77.9      |
| loss/critic2                       | 75.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1313000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -680     |
| loss/alpha                         | -0.118   |
| loss/critic1                       | 87.7     |
| loss/critic2                       | 87       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1314000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.263    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.017    |
| loss/critic1                       | 80.2     |
| loss/critic2                       | 78.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1315000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0090
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.386    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.167    |
| loss/critic1                       | 77.4     |
| loss/critic2                       | 76.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1316000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.48      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.125     |
| loss/critic1                       | 47.2      |
| loss/critic2                       | 46.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1317000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0006
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.37e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.657     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.0805    |
| loss/critic1                       | 30.1      |
| loss/critic2                       | 28.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1318000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9975
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.282    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0417  |
| loss/critic1                       | 32.3     |
| loss/critic2                       | 31.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1319000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.703     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.135    |
| loss/critic1                       | 29.8      |
| loss/critic2                       | 28.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1320000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.0358   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0321  |
| loss/critic1                       | 26.8     |
| loss/critic2                       | 26       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1321000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.252    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0847  |
| loss/critic1                       | 26.5     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1322000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0324  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 23.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1323000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.41e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.648    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.04      |
| loss/critic1                       | 33        |
| loss/critic2                       | 32.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1324000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.1e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.0802  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 9.84     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0818   |
| loss/critic1                       | 32       |
| loss/critic2                       | 30.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1325000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.0437   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0108   |
| loss/critic1                       | 22.4      |
| loss/critic2                       | 21.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1326000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0082
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.753     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0769   |
| loss/critic1                       | 22.2      |
| loss/critic2                       | 21.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1327000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.5e-10  |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.0884   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.052   |
| loss/critic1                       | 22.1     |
| loss/critic2                       | 20.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1328000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.737     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0205   |
| loss/critic1                       | 21.7      |
| loss/critic2                       | 21        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1329000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.531    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0269   |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 21.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1330000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.22      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0171    |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 20.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1331000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.872   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0911  |
| loss/critic1                       | 21.4     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1332000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.179    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0825  |
| loss/critic1                       | 23.9     |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1333000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9896
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-11 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -1.42     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0394   |
| loss/critic1                       | 25.4      |
| loss/critic2                       | 24        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1334000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.00232  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0342   |
| loss/critic1                       | 26.2     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1335000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9966
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.626   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0681   |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 22       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1336000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.3e-11  |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.7     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 7.58     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0402   |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 25.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1337000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.622    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 12.4      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0523   |
| loss/critic1                       | 21.8      |
| loss/critic2                       | 21.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1338000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.605    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0441   |
| loss/critic1                       | 25        |
| loss/critic2                       | 24.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1339000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9927
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.89e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.687    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 15.3     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0242   |
| loss/critic1                       | 22.9     |
| loss/critic2                       | 22.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1340000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.59     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0618   |
| loss/critic1                       | 24       |
| loss/critic2                       | 23.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1341000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.464   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0995   |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1342000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0071
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.48e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.939   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0831  |
| loss/critic1                       | 24.9     |
| loss/critic2                       | 23.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1343000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.5      |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00905 |
| loss/critic1                       | 25.8     |
| loss/critic2                       | 25.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1344000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.626   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 30.4     |
| loss/critic2                       | 29.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1345000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 1.43     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00152 |
| loss/critic1                       | 33.9     |
| loss/critic2                       | 33.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1346000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9944
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.218   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0704  |
| loss/critic1                       | 35       |
| loss/critic2                       | 34       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1347000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0111
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.195     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.164     |
| loss/critic1                       | 26.8      |
| loss/critic2                       | 26.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1348000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.00469  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0708   |
| loss/critic1                       | 24.7     |
| loss/critic2                       | 24.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1349000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.8e-10  |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.439    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.154   |
| loss/critic1                       | 23.6     |
| loss/critic2                       | 22.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1350000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-11 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.0262  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0605  |
| loss/critic1                       | 30.5     |
| loss/critic2                       | 30.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1351000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.89e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.478   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0143   |
| loss/critic1                       | 37.1     |
| loss/critic2                       | 35.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1352000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-09    |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.458    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 17.1     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0856   |
| loss/critic1                       | 50.7     |
| loss/critic2                       | 50.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1353000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.874    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0595  |
| loss/critic1                       | 40.4     |
| loss/critic2                       | 39.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1354000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.6e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0725   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0932  |
| loss/critic1                       | 29.5     |
| loss/critic2                       | 29.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1355000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0006
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.72e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.334   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0461   |
| loss/critic1                       | 64       |
| loss/critic2                       | 62.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1356000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.255    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0424   |
| loss/critic1                       | 46.1      |
| loss/critic2                       | 46.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1357000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9910
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.246    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0953  |
| loss/critic1                       | 36.9     |
| loss/critic2                       | 35.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1358000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9996
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.401    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0367    |
| loss/critic1                       | 52.1      |
| loss/critic2                       | 53        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1359000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.348    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0657   |
| loss/critic1                       | 43.1     |
| loss/critic2                       | 41.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1360000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | -0.0847   |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0936    |
| loss/critic1                       | 48.8      |
| loss/critic2                       | 49.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1361000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0094
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.82e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.791     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0266   |
| loss/critic1                       | 35.4      |
| loss/critic2                       | 34.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1362000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.302   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0154  |
| loss/critic1                       | 37.9     |
| loss/critic2                       | 36.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1363000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.171    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0433   |
| loss/critic1                       | 50.4      |
| loss/critic2                       | 49.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1364000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.0812   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0142  |
| loss/critic1                       | 51.8     |
| loss/critic2                       | 51.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1365000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -1.88     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0792    |
| loss/critic1                       | 52.8      |
| loss/critic2                       | 53        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1366000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.0577    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.023     |
| loss/critic1                       | 42        |
| loss/critic2                       | 40.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1367000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.53e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.402   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 42.4     |
| loss/critic2                       | 41.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1368000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.96e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.442    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 32.8     |
| loss/critic2                       | 32       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1369000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.0705    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.08     |
| loss/critic1                       | 32        |
| loss/critic2                       | 30.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1370000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.402     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0461    |
| loss/critic1                       | 34.1      |
| loss/critic2                       | 33.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1371000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.414    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0148    |
| loss/critic1                       | 29.4      |
| loss/critic2                       | 28.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1372000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.0828   |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.102    |
| loss/critic1                       | 29.3      |
| loss/critic2                       | 28.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1373000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.244     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0494    |
| loss/critic1                       | 37.3      |
| loss/critic2                       | 36.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1374000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0046
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.358    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 34.1     |
| loss/critic2                       | 32.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1375000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.8e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.758    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0936   |
| loss/critic1                       | 34.1     |
| loss/critic2                       | 33.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1376000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0069
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.34e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.707   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0918   |
| loss/critic1                       | 32.1     |
| loss/critic2                       | 31.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1377000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.8e-11  |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.97     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0833  |
| loss/critic1                       | 41.1     |
| loss/critic2                       | 40.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1378000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.251    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0361   |
| loss/critic1                       | 41.7     |
| loss/critic2                       | 41.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1379000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0449    |
| loss/critic1                       | 37.8      |
| loss/critic2                       | 37.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1380000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.879    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0941  |
| loss/critic1                       | 31.7     |
| loss/critic2                       | 31.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1381000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.175   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0113   |
| loss/critic1                       | 25.4     |
| loss/critic2                       | 24.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1382000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.7e-10  |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.732    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.00544  |
| loss/critic1                       | 28.5     |
| loss/critic2                       | 27.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1383000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.186    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -691      |
| loss/alpha                         | -0.106    |
| loss/critic1                       | 23.2      |
| loss/critic2                       | 22.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1384000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.188    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0139   |
| loss/critic1                       | 25.6      |
| loss/critic2                       | 24.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1385000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9968
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.172   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0679   |
| loss/critic1                       | 22.5     |
| loss/critic2                       | 22.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1386000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 4.9976
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.31e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.0324    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0318   |
| loss/critic1                       | 28.1      |
| loss/critic2                       | 27        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1387000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.731   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0274  |
| loss/critic1                       | 33       |
| loss/critic2                       | 32.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1388000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9962
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.939     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.169     |
| loss/critic1                       | 26.1      |
| loss/critic2                       | 24.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1389000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.798    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0262   |
| loss/critic1                       | 31.9     |
| loss/critic2                       | 31.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1390000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.7e-10  |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.195    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0163   |
| loss/critic1                       | 30.5     |
| loss/critic2                       | 30.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1391000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.205     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0581   |
| loss/critic1                       | 33.2      |
| loss/critic2                       | 32        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1392000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.17e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.915   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.00838 |
| loss/critic1                       | 32.2     |
| loss/critic2                       | 32.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1393000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 1.21     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 29.1     |
| loss/critic2                       | 28.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1394000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.97e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | -0.706   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0931  |
| loss/critic1                       | 29.2     |
| loss/critic2                       | 28.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1395000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.87e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.271    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0313   |
| loss/critic1                       | 31.8     |
| loss/critic2                       | 31.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1396000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 1.2      |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 6.67     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0332  |
| loss/critic1                       | 26.6     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1397000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.57     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0337   |
| loss/critic1                       | 23       |
| loss/critic2                       | 22.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1398000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0023
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.378     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.115    |
| loss/critic1                       | 22.9      |
| loss/critic2                       | 22        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1399000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.0952   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0213  |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1400000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0112
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.049     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.155     |
| loss/critic1                       | 21.1      |
| loss/critic2                       | 20.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1401000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-11  |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.458   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0854  |
| loss/critic1                       | 19.3     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1402000  |
----------------------------------------------------------------------------------
num rollout transitions: 249999, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.0235  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.052   |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1403000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.26e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.921   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0211  |
| loss/critic1                       | 22.2     |
| loss/critic2                       | 21.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1404000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.99e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.164    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.00942  |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1405000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.625   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.035   |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1406000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.546     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0614    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1407000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.6e-10  |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.214    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0442   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1408000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.215   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0515   |
| loss/critic1                       | 20.1     |
| loss/critic2                       | 19.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1409000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.842    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00817  |
| loss/critic1                       | 19.9     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1410000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.61e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.607     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.00412   |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 18.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1411000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.71     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1412000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0025
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.911    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0226  |
| loss/critic1                       | 20       |
| loss/critic2                       | 19.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1413000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.37e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.32     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.105    |
| loss/critic1                       | 26.2      |
| loss/critic2                       | 25.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1414000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.305    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0594   |
| loss/critic1                       | 23.4     |
| loss/critic2                       | 22.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1415000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0026
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.31e-10  |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.862    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -691      |
| loss/alpha                         | -3.73e-05 |
| loss/critic1                       | 22.7      |
| loss/critic2                       | 22.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1416000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 4.9997
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 1        |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0504   |
| loss/critic1                       | 27.2     |
| loss/critic2                       | 27.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1417000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0282
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -1.02     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 8.96      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0705   |
| loss/critic1                       | 22.2      |
| loss/critic2                       | 21.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1418000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.21      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0303    |
| loss/critic1                       | 33.3      |
| loss/critic2                       | 32.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1419000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.139   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 29.6     |
| loss/critic2                       | 28.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1420000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0158
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -1.1      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0571   |
| loss/critic1                       | 24.2      |
| loss/critic2                       | 23.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1421000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.82    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0183   |
| loss/critic1                       | 27.2     |
| loss/critic2                       | 27.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1422000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9987
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.599   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.014   |
| loss/critic1                       | 21.5     |
| loss/critic2                       | 20.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1423000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.226    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00104  |
| loss/critic1                       | 24.1     |
| loss/critic2                       | 23.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1424000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0091
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.00848   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.1      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0998    |
| loss/critic1                       | 28        |
| loss/critic2                       | 27.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1425000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0142
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.11     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 9.35      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0033    |
| loss/critic1                       | 29.3      |
| loss/critic2                       | 28.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1426000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0565  |
| loss/critic1                       | 29.9     |
| loss/critic2                       | 29.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1427000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -1.2     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00571  |
| loss/critic1                       | 31.9     |
| loss/critic2                       | 30.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1428000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8e-10   |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.0183   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.00455  |
| loss/critic1                       | 26.3     |
| loss/critic2                       | 25.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1429000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.45e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.00974 |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0913  |
| loss/critic1                       | 21.7     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1430000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.361   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0782   |
| loss/critic1                       | 22.4     |
| loss/critic2                       | 21.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1431000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.71e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.736     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.136    |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1432000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 1.27     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 24.3     |
| loss/critic2                       | 23.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1433000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.361    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0506    |
| loss/critic1                       | 23.7      |
| loss/critic2                       | 23.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1434000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.484     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0251    |
| loss/critic1                       | 26.2      |
| loss/critic2                       | 26        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1435000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.82e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.584    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0144   |
| loss/critic1                       | 28.3      |
| loss/critic2                       | 28.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1436000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.0048  |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0581   |
| loss/critic1                       | 26.7     |
| loss/critic2                       | 26.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1437000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.89e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.246   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -692     |
| loss/alpha                         | 0.0813   |
| loss/critic1                       | 25.9     |
| loss/critic2                       | 25.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1438000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | -0.3      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.0173    |
| loss/critic1                       | 24.7      |
| loss/critic2                       | 23.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1439000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.466   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 21.2     |
| loss/critic2                       | 20.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1440000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0369    |
| loss/critic1                       | 22.7      |
| loss/critic2                       | 22.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1441000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.907   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0279   |
| loss/critic1                       | 19.1     |
| loss/critic2                       | 18.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1442000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.73e-11 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.0849  |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 22       |
| loss/critic2                       | 21.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1443000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.658     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0109   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1444000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.00904 |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1445000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.726   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0647   |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1446000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.68e-12 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.775    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0591  |
| loss/critic1                       | 20.9     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1447000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.55e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.263   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0163  |
| loss/critic1                       | 21.6     |
| loss/critic2                       | 21.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1448000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.97     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0449   |
| loss/critic1                       | 21.2     |
| loss/critic2                       | 20       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1449000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0007
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.52      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0371   |
| loss/critic1                       | 28.4      |
| loss/critic2                       | 27.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1450000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.469     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0141   |
| loss/critic1                       | 29.3      |
| loss/critic2                       | 28.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1451000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.361     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0522    |
| loss/critic1                       | 37.6      |
| loss/critic2                       | 36.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1452000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.301    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.022    |
| loss/critic1                       | 38.6     |
| loss/critic2                       | 38.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1453000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.61e-11 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.0829  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0913  |
| loss/critic1                       | 39.4     |
| loss/critic2                       | 37.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1454000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0072
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.0879    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 9.25      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0716    |
| loss/critic1                       | 44.6      |
| loss/critic2                       | 44.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1455000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.064    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.00474  |
| loss/critic1                       | 41        |
| loss/critic2                       | 39.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1456000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 1.1      |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0567  |
| loss/critic1                       | 47.5     |
| loss/critic2                       | 46       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1457000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.48e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.0891   |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0246   |
| loss/critic1                       | 41.1      |
| loss/critic2                       | 39.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1458000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.646    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -695     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 41       |
| loss/critic2                       | 39.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1459000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.0712   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0922   |
| loss/critic1                       | 36.7     |
| loss/critic2                       | 36.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1460000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.116    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0195  |
| loss/critic1                       | 41.4     |
| loss/critic2                       | 39.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1461000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10  |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.351   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0491   |
| loss/critic1                       | 39.6     |
| loss/critic2                       | 39.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1462000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.82e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0266  |
| loss/critic1                       | 33.6     |
| loss/critic2                       | 33.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1463000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9e-10   |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.538    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0098  |
| loss/critic1                       | 30.9     |
| loss/critic2                       | 29.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1464000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0100
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.466    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0072  |
| loss/critic1                       | 33.2     |
| loss/critic2                       | 32.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1465000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.31e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.216   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0935   |
| loss/critic1                       | 41.4     |
| loss/critic2                       | 41.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1466000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0054
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.171    |
| loss/critic1                       | 34.5     |
| loss/critic2                       | 34.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1467000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.297     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.2      |
| loss/critic1                       | 29.4      |
| loss/critic2                       | 29.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1468000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.24e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.403    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0409  |
| loss/critic1                       | 35.1     |
| loss/critic2                       | 34.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1469000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.25e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.284    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0598  |
| loss/critic1                       | 34.8     |
| loss/critic2                       | 34.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1470000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.221     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0836    |
| loss/critic1                       | 41.5      |
| loss/critic2                       | 40.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1471000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.127     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.00423  |
| loss/critic1                       | 33.4      |
| loss/critic2                       | 32.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1472000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.9e-10  |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.266   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0775  |
| loss/critic1                       | 34.2     |
| loss/critic2                       | 33.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1473000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0354
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.86e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.178    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 17.6      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.114     |
| loss/critic1                       | 33.7      |
| loss/critic2                       | 33.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1474000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0220
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.247     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.133     |
| loss/critic1                       | 33.5      |
| loss/critic2                       | 32.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1475000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0310
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.71e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.936    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0407  |
| loss/critic1                       | 26.7     |
| loss/critic2                       | 25.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1476000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.0921    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0201   |
| loss/critic1                       | 31.9      |
| loss/critic2                       | 31.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1477000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.733    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0452   |
| loss/critic1                       | 35.7     |
| loss/critic2                       | 36       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1478000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.159    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0898  |
| loss/critic1                       | 36.6     |
| loss/critic2                       | 35.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1479000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.83e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | -0.897    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0165   |
| loss/critic1                       | 51.4      |
| loss/critic2                       | 51.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1480000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.0313  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 5.02     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.0605   |
| loss/critic1                       | 37       |
| loss/critic2                       | 36.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1481000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 4.9958
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.44     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.00372 |
| loss/critic1                       | 36       |
| loss/critic2                       | 34.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1482000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.69e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.0104  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0647  |
| loss/critic1                       | 34.4     |
| loss/critic2                       | 34       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1483000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 1.03      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0203    |
| loss/critic1                       | 34.8      |
| loss/critic2                       | 34.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1484000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.66e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.162   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0743  |
| loss/critic1                       | 30.9     |
| loss/critic2                       | 29.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1485000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0171
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.291     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0752   |
| loss/critic1                       | 40.6      |
| loss/critic2                       | 40        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1486000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.91e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.319   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 12.6     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.182    |
| loss/critic1                       | 38       |
| loss/critic2                       | 37.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1487000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0172
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.537    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.12     |
| loss/critic1                       | 34.3      |
| loss/critic2                       | 34        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1488000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 0.825     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.124    |
| loss/critic1                       | 33.1      |
| loss/critic2                       | 32.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1489000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0136
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.278   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0403  |
| loss/critic1                       | 43.3     |
| loss/critic2                       | 42.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1490000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0049
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.73e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -1.41     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.065    |
| loss/critic1                       | 46.7      |
| loss/critic2                       | 46.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1491000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.129    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.111     |
| loss/critic1                       | 53.7      |
| loss/critic2                       | 53.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1492000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0254
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.1e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.256   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -696     |
| loss/alpha                         | 0.25     |
| loss/critic1                       | 53.3     |
| loss/critic2                       | 52.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1493000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | -0.121    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -695      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 56.4      |
| loss/critic2                       | 55.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1494000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.709   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.057    |
| loss/critic1                       | 64.5     |
| loss/critic2                       | 64.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1495000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.31e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.184    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.108    |
| loss/critic1                       | 42.8      |
| loss/critic2                       | 43.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1496000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.44      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0551   |
| loss/critic1                       | 40.5      |
| loss/critic2                       | 39.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1497000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.61e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.135    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0579  |
| loss/critic1                       | 61.6     |
| loss/critic2                       | 60       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1498000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-11  |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.905   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.138   |
| loss/critic1                       | 66.4     |
| loss/critic2                       | 65.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1499000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.317   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0326   |
| loss/critic1                       | 45       |
| loss/critic2                       | 45.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1500000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.94e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.127   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0097   |
| loss/critic1                       | 54.8     |
| loss/critic2                       | 54       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1501000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.8      |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -694     |
| loss/alpha                         | -0.0409  |
| loss/critic1                       | 59.9     |
| loss/critic2                       | 59       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1502000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9884
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.33e-11 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.124    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0285   |
| loss/critic1                       | 62.5      |
| loss/critic2                       | 63.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1503000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.675    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0313   |
| loss/critic1                       | 73.7     |
| loss/critic2                       | 71.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1504000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | -1.18    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0894   |
| loss/critic1                       | 78.7     |
| loss/critic2                       | 78       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1505000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 0.523    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0548  |
| loss/critic1                       | 61.3     |
| loss/critic2                       | 61.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1506000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0253
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.237   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0312  |
| loss/critic1                       | 90.2     |
| loss/critic2                       | 86.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1507000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.77e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.146    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0295  |
| loss/critic1                       | 79.9     |
| loss/critic2                       | 79.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1508000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.54e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.308    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0126   |
| loss/critic1                       | 77       |
| loss/critic2                       | 78.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1509000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0947   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.017    |
| loss/critic1                       | 66.1     |
| loss/critic2                       | 65.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1510000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.36     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0172   |
| loss/critic1                       | 71.4     |
| loss/critic2                       | 72       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1511000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0048
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.186    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0301  |
| loss/critic1                       | 52.2     |
| loss/critic2                       | 52.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1512000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.307    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0529    |
| loss/critic1                       | 61.4      |
| loss/critic2                       | 60.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1513000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9908
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.0836  |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.145    |
| loss/critic1                       | 54.1     |
| loss/critic2                       | 53.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1514000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0165
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.481    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0252   |
| loss/critic1                       | 53.6      |
| loss/critic2                       | 54.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1515000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0053
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.035    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.00798   |
| loss/critic1                       | 63.1      |
| loss/critic2                       | 64        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1516000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-12 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.672    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.44     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0294  |
| loss/critic1                       | 43.1     |
| loss/critic2                       | 41.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1517000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.523    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0452    |
| loss/critic1                       | 60        |
| loss/critic2                       | 58.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1518000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.276   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.3     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0507  |
| loss/critic1                       | 47       |
| loss/critic2                       | 45.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1519000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0196
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.212     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0478    |
| loss/critic1                       | 59.1      |
| loss/critic2                       | 58.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1520000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.775    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.159    |
| loss/critic1                       | 86.7     |
| loss/critic2                       | 85.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1521000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | -0.4      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0105   |
| loss/critic1                       | 49.6      |
| loss/critic2                       | 48.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1522000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.306    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0197    |
| loss/critic1                       | 51        |
| loss/critic2                       | 50.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1523000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 4.9958
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.346   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.000725 |
| loss/critic1                       | 50.2     |
| loss/critic2                       | 48.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1524000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.628   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.00932  |
| loss/critic1                       | 78.1     |
| loss/critic2                       | 76.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1525000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.00319  |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.0728    |
| loss/critic1                       | 52.4      |
| loss/critic2                       | 51        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1526000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.59e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | 0.231    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0958  |
| loss/critic1                       | 83.9     |
| loss/critic2                       | 83.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1527000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.11     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0234   |
| loss/critic1                       | 56.1     |
| loss/critic2                       | 55.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1528000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 4.9966
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.692   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 107      |
| loss/critic2                       | 107      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1529000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.391   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0878  |
| loss/critic1                       | 90       |
| loss/critic2                       | 89.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1530000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.62    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0347  |
| loss/critic1                       | 119      |
| loss/critic2                       | 120      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1531000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -1.18    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0039   |
| loss/critic1                       | 65.2     |
| loss/critic2                       | 62.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1532000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.554    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -690      |
| loss/alpha                         | -0.0723   |
| loss/critic1                       | 69.5      |
| loss/critic2                       | 67.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1533000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.57e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.801     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.00242   |
| loss/critic1                       | 84.1      |
| loss/critic2                       | 81.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1534000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.37e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -0.0714  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0472   |
| loss/critic1                       | 82.2     |
| loss/critic2                       | 80.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1535000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.94e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.4      |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0031   |
| loss/critic1                       | 69.4     |
| loss/critic2                       | 68.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1536000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.0202   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0531  |
| loss/critic1                       | 82.5     |
| loss/critic2                       | 82.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1537000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.61e-11 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.216   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.148    |
| loss/critic1                       | 52.8     |
| loss/critic2                       | 52.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1538000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.124    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0178    |
| loss/critic1                       | 46.8      |
| loss/critic2                       | 47        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1539000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.00845  |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0424   |
| loss/critic1                       | 54.8      |
| loss/critic2                       | 54.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1540000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0241
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.829    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.000253 |
| loss/critic1                       | 62.4      |
| loss/critic2                       | 61.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1541000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 4.9984
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.113     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0106   |
| loss/critic1                       | 72.3      |
| loss/critic2                       | 72.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1542000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.513     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0278    |
| loss/critic1                       | 76.9      |
| loss/critic2                       | 76.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1543000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.15e-11 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | -0.366    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.123    |
| loss/critic1                       | 55.6      |
| loss/critic2                       | 52.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1544000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0016
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.00976  |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0609   |
| loss/critic1                       | 92.8      |
| loss/critic2                       | 90.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1545000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.162     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.00347  |
| loss/critic1                       | 81.3      |
| loss/critic2                       | 80.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1546000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.55e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.052    |
| loss/critic1                       | 61.4      |
| loss/critic2                       | 60.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1547000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-12 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | 0.144    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0266   |
| loss/critic1                       | 73.7     |
| loss/critic2                       | 76.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1548000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.79e-10 |
| adv_dynamics_update/adv_log_prob   | 49       |
| adv_dynamics_update/adv_loss       | -0.933   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.15     |
| loss/critic1                       | 88       |
| loss/critic2                       | 89.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1549000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0174
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-11 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | 0.497    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 67.6     |
| loss/critic2                       | 68.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1550000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.83e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | 0.388     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0146   |
| loss/critic1                       | 55.2      |
| loss/critic2                       | 55.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1551000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.17e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.187     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.00735   |
| loss/critic1                       | 64.9      |
| loss/critic2                       | 65.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1552000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0114
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.95e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.308     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0742    |
| loss/critic1                       | 61.9      |
| loss/critic2                       | 63.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1553000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.00405  |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0986   |
| loss/critic1                       | 61.7      |
| loss/critic2                       | 60.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1554000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.82e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.881    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0731   |
| loss/critic1                       | 52.7      |
| loss/critic2                       | 49.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1555000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0128
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0083   |
| loss/critic1                       | 43       |
| loss/critic2                       | 41.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1556000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.5e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.579   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.00779  |
| loss/critic1                       | 45.1     |
| loss/critic2                       | 45.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1557000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.4e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.0968   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.172    |
| loss/critic1                       | 68.6     |
| loss/critic2                       | 67.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1558000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.235    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0982    |
| loss/critic1                       | 77.5      |
| loss/critic2                       | 75.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1559000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.758    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0693  |
| loss/critic1                       | 69.4     |
| loss/critic2                       | 69.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1560000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9973
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.51e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.239    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0693   |
| loss/critic1                       | 61.1      |
| loss/critic2                       | 59.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1561000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.717   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0331  |
| loss/critic1                       | 57.8     |
| loss/critic2                       | 56.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1562000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0282
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.593   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 51.4     |
| loss/critic2                       | 49.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1563000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.779    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0783   |
| loss/critic1                       | 58.7     |
| loss/critic2                       | 58.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1564000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.26e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.501   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.037   |
| loss/critic1                       | 41.7     |
| loss/critic2                       | 41.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1565000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-11 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.952    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0747  |
| loss/critic1                       | 47.5     |
| loss/critic2                       | 46.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1566000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9921
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.04e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.681    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0549    |
| loss/critic1                       | 47.4      |
| loss/critic2                       | 48        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1567000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -1.31    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 55.3     |
| loss/critic2                       | 56.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1568000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 49.1      |
| adv_dynamics_update/adv_loss       | -0.281    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0796   |
| loss/critic1                       | 62.1      |
| loss/critic2                       | 61.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1569000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | -0.0658  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0441   |
| loss/critic1                       | 54.1     |
| loss/critic2                       | 54.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1570000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9878
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.466     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0751    |
| loss/critic1                       | 111       |
| loss/critic2                       | 113       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1571000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9872
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.41e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.541   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0979   |
| loss/critic1                       | 106      |
| loss/critic2                       | 104      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1572000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0200
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.04e-10 |
| adv_dynamics_update/adv_log_prob   | 49        |
| adv_dynamics_update/adv_loss       | -0.424    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.39      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.298    |
| loss/critic1                       | 137       |
| loss/critic2                       | 135       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1573000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.98e-10 |
| adv_dynamics_update/adv_log_prob   | 49.7      |
| adv_dynamics_update/adv_loss       | 0.0594    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.12     |
| loss/critic1                       | 135       |
| loss/critic2                       | 128       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1574000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0248
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | 0.0846    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0423    |
| loss/critic1                       | 134       |
| loss/critic2                       | 135       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1575000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0005
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.69e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.823    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.24      |
| loss/critic1                       | 137       |
| loss/critic2                       | 136       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1576000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.36e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.238   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0523   |
| loss/critic1                       | 176      |
| loss/critic2                       | 176      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1577000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.551   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.123   |
| loss/critic1                       | 138      |
| loss/critic2                       | 140      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1578000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.73e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.135    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.076     |
| loss/critic1                       | 90.3      |
| loss/critic2                       | 85.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1579000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | 0.518    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0296   |
| loss/critic1                       | 133      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1580000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.45e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.637    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.171     |
| loss/critic1                       | 154       |
| loss/critic2                       | 151       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1581000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.328   |
| loss/critic1                       | 126      |
| loss/critic2                       | 122      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1582000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | 0.43      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.344    |
| loss/critic1                       | 123       |
| loss/critic2                       | 120       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1583000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0107
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.5e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.291    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00181  |
| loss/critic1                       | 74.9     |
| loss/critic2                       | 74.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1584000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-11  |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.217    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 15.8     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0836   |
| loss/critic1                       | 143      |
| loss/critic2                       | 144      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1585000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.642    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0207    |
| loss/critic1                       | 106       |
| loss/critic2                       | 108       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1586000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9976
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -0.0213  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.144    |
| loss/critic1                       | 141      |
| loss/critic2                       | 142      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1587000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.0959    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.181     |
| loss/critic1                       | 146       |
| loss/critic2                       | 143       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1588000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 249999, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.84e-10 |
| adv_dynamics_update/adv_log_prob   | 49.6     |
| adv_dynamics_update/adv_loss       | 0.294    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 91.9     |
| loss/critic2                       | 92.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1589000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.14e-11 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.548   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -686     |
| loss/alpha                         | -0.226   |
| loss/critic1                       | 130      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1590000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.36e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | -0.322   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.176   |
| loss/critic1                       | 136      |
| loss/critic2                       | 140      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1591000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.124    |
| loss/critic1                       | 138      |
| loss/critic2                       | 135      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1592000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.11      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0621   |
| loss/critic1                       | 138       |
| loss/critic2                       | 139       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1593000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.867     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.19      |
| loss/critic1                       | 182       |
| loss/critic2                       | 187       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1594000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.83e-11 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -1.03    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.236    |
| loss/critic1                       | 208      |
| loss/critic2                       | 203      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1595000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -2.31    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 133      |
| loss/critic2                       | 132      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1596000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.05e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | -0.374    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.016    |
| loss/critic1                       | 156       |
| loss/critic2                       | 162       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1597000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | -0.668   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 195      |
| loss/critic2                       | 191      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1598000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.943    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 115       |
| loss/critic2                       | 114       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1599000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | 0.366     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 47.6      |
| loss/critic2                       | 47.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1600000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | 0.616    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 95.8     |
| loss/critic2                       | 94.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1601000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | -0.861    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0141   |
| loss/critic1                       | 87.7      |
| loss/critic2                       | 90.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1602000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.173    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0679  |
| loss/critic1                       | 60.5     |
| loss/critic2                       | 57.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1603000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.168    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0794    |
| loss/critic1                       | 96.2      |
| loss/critic2                       | 95.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1604000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 4.9970
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | -1.4      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.165     |
| loss/critic1                       | 95.2      |
| loss/critic2                       | 93.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1605000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 49       |
| adv_dynamics_update/adv_loss       | 0.19     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.104    |
| loss/critic1                       | 78.2     |
| loss/critic2                       | 77.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1606000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 249999, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | 0.439    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0426   |
| loss/critic1                       | 103      |
| loss/critic2                       | 106      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1607000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | 0.628     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0784   |
| loss/critic1                       | 165       |
| loss/critic2                       | 164       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1608000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.73e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.176   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.166   |
| loss/critic1                       | 144      |
| loss/critic2                       | 143      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1609000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0267
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.393     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.079    |
| loss/critic1                       | 102       |
| loss/critic2                       | 100       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1610000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.43e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.511     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 12.8      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0442    |
| loss/critic1                       | 129       |
| loss/critic2                       | 127       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1611000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3e-11   |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.698    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0138  |
| loss/critic1                       | 111      |
| loss/critic2                       | 111      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1612000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.985   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.113    |
| loss/critic1                       | 80.3     |
| loss/critic2                       | 79.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1613000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | 0.137    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 180      |
| loss/critic2                       | 183      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1614000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0058
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | 0.54     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.167    |
| loss/critic1                       | 133      |
| loss/critic2                       | 131      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1615000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-11 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.221    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 16.1      |
| loss/actor                         | -681      |
| loss/alpha                         | 0.0112    |
| loss/critic1                       | 140       |
| loss/critic2                       | 139       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1616000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0088
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 0.774    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 15.4     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.118   |
| loss/critic1                       | 221      |
| loss/critic2                       | 228      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1617000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.73e-11 |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | -0.389   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0633   |
| loss/critic1                       | 187      |
| loss/critic2                       | 187      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1618000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0006
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 49.3      |
| adv_dynamics_update/adv_loss       | 0.726     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.115     |
| loss/critic1                       | 206       |
| loss/critic2                       | 207       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1619000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -0.582   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 10.6     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0161  |
| loss/critic1                       | 245      |
| loss/critic2                       | 243      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1620000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.62e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | 0.0667    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.166    |
| loss/critic1                       | 142       |
| loss/critic2                       | 139       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1621000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 49.9     |
| adv_dynamics_update/adv_loss       | 0.674    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0191  |
| loss/critic1                       | 204      |
| loss/critic2                       | 200      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1622000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9832
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -0.458    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.153     |
| loss/critic1                       | 200       |
| loss/critic2                       | 198       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1623000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.88e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.394   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.0651   |
| loss/critic1                       | 158      |
| loss/critic2                       | 162      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1624000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0037
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 49.4      |
| adv_dynamics_update/adv_loss       | 0.144     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0435   |
| loss/critic1                       | 154       |
| loss/critic2                       | 158       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1625000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0071
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 49.3      |
| adv_dynamics_update/adv_loss       | 0.277     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 17.4      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.179    |
| loss/critic1                       | 229       |
| loss/critic2                       | 233       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1626000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.2e-11 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -0.968   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -678     |
| loss/alpha                         | -0.213   |
| loss/critic1                       | 216      |
| loss/critic2                       | 219      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1627000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -1.24     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -677      |
| loss/alpha                         | 0.0041    |
| loss/critic1                       | 125       |
| loss/critic2                       | 123       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1628000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.501   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.131    |
| loss/critic1                       | 140      |
| loss/critic2                       | 140      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1629000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.19e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.474     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0328   |
| loss/critic1                       | 132       |
| loss/critic2                       | 129       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1630000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.75e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.0714   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 7.94     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0038  |
| loss/critic1                       | 111      |
| loss/critic2                       | 117      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1631000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0023
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | -0.243    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0431   |
| loss/critic1                       | 121       |
| loss/critic2                       | 118       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1632000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.43e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.0379   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0378   |
| loss/critic1                       | 150      |
| loss/critic2                       | 158      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1633000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9893
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | -2.23     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.134    |
| loss/critic1                       | 80.9      |
| loss/critic2                       | 81.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1634000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.155   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.141   |
| loss/critic1                       | 143      |
| loss/critic2                       | 144      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1635000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4e-11    |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.481   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.00716 |
| loss/critic1                       | 154      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1636000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | -0.0974  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.56     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0624   |
| loss/critic1                       | 114      |
| loss/critic2                       | 115      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1637000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.2e-11  |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.306    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0174   |
| loss/critic1                       | 92.4     |
| loss/critic2                       | 94.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1638000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0048
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | 0.0353    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0922   |
| loss/critic1                       | 92.6      |
| loss/critic2                       | 91.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1639000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.6e-10  |
| adv_dynamics_update/adv_log_prob   | 49.4     |
| adv_dynamics_update/adv_loss       | 0.387    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0885   |
| loss/critic1                       | 166      |
| loss/critic2                       | 168      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1640000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9976
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.78e-11 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | 0.267     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0941    |
| loss/critic1                       | 216       |
| loss/critic2                       | 215       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1641000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9910
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9      |
| adv_dynamics_update/adv_loss       | -0.76     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.109     |
| loss/critic1                       | 281       |
| loss/critic2                       | 284       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1642000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.78e-10  |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | -0.18     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.000238 |
| loss/critic1                       | 136       |
| loss/critic2                       | 134       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1643000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 49.8      |
| adv_dynamics_update/adv_loss       | -0.18     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.111     |
| loss/critic1                       | 395       |
| loss/critic2                       | 402       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1644000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9      |
| adv_dynamics_update/adv_loss       | -0.717    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.125     |
| loss/critic1                       | 311       |
| loss/critic2                       | 311       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1645000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.17e-11 |
| adv_dynamics_update/adv_log_prob   | 50.4     |
| adv_dynamics_update/adv_loss       | 0.244    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.279   |
| loss/critic1                       | 318      |
| loss/critic2                       | 317      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1646000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.94e-11 |
| adv_dynamics_update/adv_log_prob   | 49.1      |
| adv_dynamics_update/adv_loss       | -0.415    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.126    |
| loss/critic1                       | 254       |
| loss/critic2                       | 254       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1647000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.41e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | -0.385   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0835  |
| loss/critic1                       | 258      |
| loss/critic2                       | 260      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1648000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 49       |
| adv_dynamics_update/adv_loss       | -0.296   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0766   |
| loss/critic1                       | 246      |
| loss/critic2                       | 243      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1649000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | -0.669   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0455   |
| loss/critic1                       | 270      |
| loss/critic2                       | 277      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1650000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | -0.582   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.184    |
| loss/critic1                       | 381      |
| loss/critic2                       | 387      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1651000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.43e-12 |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | 0.781    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0586  |
| loss/critic1                       | 331      |
| loss/critic2                       | 327      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1652000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0135
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.14    |
| loss/critic1                       | 177      |
| loss/critic2                       | 178      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1653000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0169
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.0792  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.2     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0715   |
| loss/critic1                       | 182      |
| loss/critic2                       | 178      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1654000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.116   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.9     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0425  |
| loss/critic1                       | 181      |
| loss/critic2                       | 177      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1655000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.43e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | 0.111    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.0915  |
| loss/critic1                       | 201      |
| loss/critic2                       | 203      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1656000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 49.6     |
| adv_dynamics_update/adv_loss       | -0.507   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.155    |
| loss/critic1                       | 491      |
| loss/critic2                       | 497      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1657000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9957
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 49.2      |
| adv_dynamics_update/adv_loss       | 0.239     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.0103    |
| loss/critic1                       | 346       |
| loss/critic2                       | 338       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1658000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | 0.203    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.0225  |
| loss/critic1                       | 357      |
| loss/critic2                       | 353      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1659000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -668     |
| loss/alpha                         | 0.19     |
| loss/critic1                       | 326      |
| loss/critic2                       | 339      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1660000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | -0.736   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.183   |
| loss/critic1                       | 342      |
| loss/critic2                       | 349      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1661000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 49.3     |
| adv_dynamics_update/adv_loss       | -0.437   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -667     |
| loss/alpha                         | -0.02    |
| loss/critic1                       | 403      |
| loss/critic2                       | 394      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1662000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 49.2      |
| adv_dynamics_update/adv_loss       | -1.05     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -667      |
| loss/alpha                         | 0.00223   |
| loss/critic1                       | 266       |
| loss/critic2                       | 264       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1663000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 49.5     |
| adv_dynamics_update/adv_loss       | -0.491   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.251    |
| loss/critic1                       | 356      |
| loss/critic2                       | 356      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1664000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 49.2     |
| adv_dynamics_update/adv_loss       | -0.567   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.046    |
| loss/critic1                       | 214      |
| loss/critic2                       | 213      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1665000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.637   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 216      |
| loss/critic2                       | 209      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1666000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -0.649   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 6.32     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.184   |
| loss/critic1                       | 325      |
| loss/critic2                       | 321      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1667000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0077
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.553    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0746  |
| loss/critic1                       | 285      |
| loss/critic2                       | 287      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1668000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 49.9      |
| adv_dynamics_update/adv_loss       | -0.147    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0553    |
| loss/critic1                       | 287       |
| loss/critic2                       | 293       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1669000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-11 |
| adv_dynamics_update/adv_log_prob   | 49        |
| adv_dynamics_update/adv_loss       | 0.0423    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.00588  |
| loss/critic1                       | 298       |
| loss/critic2                       | 299       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1670000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.52e-11 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -0.505   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0562  |
| loss/critic1                       | 352      |
| loss/critic2                       | 353      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1671000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9976
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 49.2      |
| adv_dynamics_update/adv_loss       | -0.627    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0754    |
| loss/critic1                       | 387       |
| loss/critic2                       | 385       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1672000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.28e-10 |
| adv_dynamics_update/adv_log_prob   | 49.3      |
| adv_dynamics_update/adv_loss       | -0.107    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 9.96      |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0654    |
| loss/critic1                       | 494       |
| loss/critic2                       | 488       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1673000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.26e-09 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.203   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0587   |
| loss/critic1                       | 323      |
| loss/critic2                       | 318      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1674000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9994
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.65e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | -0.476   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.00353  |
| loss/critic1                       | 317      |
| loss/critic2                       | 314      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1675000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-11 |
| adv_dynamics_update/adv_log_prob   | 49.4     |
| adv_dynamics_update/adv_loss       | -0.32    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 219      |
| loss/critic2                       | 219      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1676000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.61e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | 0.308    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0558  |
| loss/critic1                       | 200      |
| loss/critic2                       | 191      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1677000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.36e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.0741   |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.0206   |
| loss/critic1                       | 171       |
| loss/critic2                       | 172       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1678000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.222    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0505  |
| loss/critic1                       | 145      |
| loss/critic2                       | 145      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1679000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.534     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.0896   |
| loss/critic1                       | 143       |
| loss/critic2                       | 143       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1680000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.49e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0601   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0623  |
| loss/critic1                       | 90.3     |
| loss/critic2                       | 83.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1681000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.273     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 6.71      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.157     |
| loss/critic1                       | 200       |
| loss/critic2                       | 202       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1682000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 4.9948
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.729    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0855    |
| loss/critic1                       | 252       |
| loss/critic2                       | 250       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1683000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0152
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 0.312     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 12.4      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0455    |
| loss/critic1                       | 250       |
| loss/critic2                       | 243       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1684000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9978
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | 1.67      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.1      |
| eval/normalized_episode_reward_std | 3.62      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.168     |
| loss/critic1                       | 307       |
| loss/critic2                       | 310       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1685000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.539    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.11    |
| loss/critic1                       | 119      |
| loss/critic2                       | 114      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1686000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.52e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.227    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.00379 |
| loss/critic1                       | 182      |
| loss/critic2                       | 179      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1687000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0021
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.52     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.168     |
| loss/critic1                       | 187       |
| loss/critic2                       | 190       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1688000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.15e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | 0.235    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 215      |
| loss/critic2                       | 216      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1689000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-09 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.415   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -663     |
| loss/alpha                         | -0.126   |
| loss/critic1                       | 221      |
| loss/critic2                       | 216      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1690000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9954
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.431   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.00108 |
| loss/critic1                       | 265      |
| loss/critic2                       | 268      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1691000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9539
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9793
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -0.36     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.301     |
| loss/critic1                       | 263       |
| loss/critic2                       | 263       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 1692000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9780
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9840
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.43e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.564     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0394    |
| loss/critic1                       | 169       |
| loss/critic2                       | 174       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1693000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.8      |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0775  |
| loss/critic1                       | 94.4     |
| loss/critic2                       | 93.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1694000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.224    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 18.2      |
| loss/actor                         | -662      |
| loss/alpha                         | -0.0812   |
| loss/critic1                       | 143       |
| loss/critic2                       | 142       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1695000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.211    |
| loss/critic1                       | 94.4      |
| loss/critic2                       | 102       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1696000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | -0.738    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 5.43      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.402    |
| loss/critic1                       | 129       |
| loss/critic2                       | 125       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1697000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9993
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.149   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.048   |
| loss/critic1                       | 135      |
| loss/critic2                       | 129      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1698000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9937
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.766   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.213    |
| loss/critic1                       | 153      |
| loss/critic2                       | 151      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1699000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.254     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.0963    |
| loss/critic1                       | 156       |
| loss/critic2                       | 153       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1700000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.61e-11 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 0.368    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 14.4     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0611   |
| loss/critic1                       | 248      |
| loss/critic2                       | 242      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1701000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.211   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 148      |
| loss/critic2                       | 148      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1702000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 4.9880
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.0183    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.176     |
| loss/critic1                       | 123       |
| loss/critic2                       | 120       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1703000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.382    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.07    |
| loss/critic1                       | 89       |
| loss/critic2                       | 91.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1704000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.4e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.285    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 6.61     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.181   |
| loss/critic1                       | 104      |
| loss/critic2                       | 97.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1705000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.792    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 19.7      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.00857   |
| loss/critic1                       | 158       |
| loss/critic2                       | 158       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1706000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 4.9923
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.04e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.154     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 11.2      |
| loss/actor                         | -664      |
| loss/alpha                         | 0.124     |
| loss/critic1                       | 124       |
| loss/critic2                       | 128       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1707000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0076
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.216     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0347    |
| loss/critic1                       | 116       |
| loss/critic2                       | 116       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1708000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0048
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0653   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -664     |
| loss/alpha                         | -0.13    |
| loss/critic1                       | 84.1     |
| loss/critic2                       | 83.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1709000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9930
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.261    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0939    |
| loss/critic1                       | 57.5      |
| loss/critic2                       | 58.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1710000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -2.08    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0865   |
| loss/critic1                       | 61       |
| loss/critic2                       | 59.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1711000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 4.9891
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.948     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 19.2      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0341    |
| loss/critic1                       | 71.5      |
| loss/critic2                       | 72.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1712000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.86e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.101    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.0469    |
| loss/critic1                       | 64.5      |
| loss/critic2                       | 64.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1713000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9896
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.539     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 24.4      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.092     |
| loss/critic1                       | 86.4      |
| loss/critic2                       | 87.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1714000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.57e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -1.35    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0582   |
| loss/critic1                       | 149      |
| loss/critic2                       | 155      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1715000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.718   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0743   |
| loss/critic1                       | 75.1     |
| loss/critic2                       | 73.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1716000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | 0.0156    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.0444   |
| loss/critic1                       | 77.9      |
| loss/critic2                       | 73.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1717000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 5.0000
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 49.9      |
| adv_dynamics_update/adv_loss       | -0.442    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -665      |
| loss/alpha                         | 0.172     |
| loss/critic1                       | 157       |
| loss/critic2                       | 162       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1718000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0072
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 49.3     |
| adv_dynamics_update/adv_loss       | -0.574   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0382   |
| loss/critic1                       | 165      |
| loss/critic2                       | 173      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1719000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0080
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.302     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -664      |
| loss/alpha                         | -0.201    |
| loss/critic1                       | 140       |
| loss/critic2                       | 142       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1720000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -0.663   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -665     |
| loss/alpha                         | -0.0376  |
| loss/critic1                       | 138      |
| loss/critic2                       | 132      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1721000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 0.0106   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.00766 |
| loss/critic1                       | 136      |
| loss/critic2                       | 132      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1722000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9898
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.175    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 18.4      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0735    |
| loss/critic1                       | 116       |
| loss/critic2                       | 114       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1723000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-09 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.371    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0983   |
| loss/critic1                       | 134      |
| loss/critic2                       | 132      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1724000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.7e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.495    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0718  |
| loss/critic1                       | 146      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1725000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 5.0060
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.01e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.385    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -668      |
| loss/alpha                         | -0.104    |
| loss/critic1                       | 110       |
| loss/critic2                       | 109       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1726000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.188   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 227      |
| loss/critic2                       | 226      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1727000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.624   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0505  |
| loss/critic1                       | 123      |
| loss/critic2                       | 120      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1728000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.622     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0142   |
| loss/critic1                       | 165       |
| loss/critic2                       | 163       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1729000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9937
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-10  |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.0203   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 95.9     |
| loss/critic2                       | 97.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1730000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0075
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.06e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.879     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0537   |
| loss/critic1                       | 89.9      |
| loss/critic2                       | 89.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1731000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.503    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.8      |
| eval/normalized_episode_reward_std | 4.15      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.00638  |
| loss/critic1                       | 62.8      |
| loss/critic2                       | 59.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1732000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-09 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | -0.0878   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 21        |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0227   |
| loss/critic1                       | 52.4      |
| loss/critic2                       | 50.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1733000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.432     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.00209   |
| loss/critic1                       | 86        |
| loss/critic2                       | 85.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1734000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9984
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.69e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.242    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -673      |
| loss/alpha                         | 0.0543    |
| loss/critic1                       | 79.3      |
| loss/critic2                       | 79.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1735000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.695    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0324  |
| loss/critic1                       | 76.7     |
| loss/critic2                       | 78.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1736000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0161
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.91      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.0604   |
| loss/critic1                       | 55.1      |
| loss/critic2                       | 55.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1737000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | 0.385    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0506  |
| loss/critic1                       | 63.5     |
| loss/critic2                       | 60.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1738000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.03e-11 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.865    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 9.24     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.0231  |
| loss/critic1                       | 76.1     |
| loss/critic2                       | 77.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1739000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | 0.454     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -675      |
| loss/alpha                         | 0.0875    |
| loss/critic1                       | 61.6      |
| loss/critic2                       | 62.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1740000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.22e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | 0.829    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -675     |
| loss/alpha                         | 0.183    |
| loss/critic1                       | 82.6     |
| loss/critic2                       | 85.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1741000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.127    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -675     |
| loss/alpha                         | -0.0384  |
| loss/critic1                       | 83.5     |
| loss/critic2                       | 84.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1742000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0072
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-09 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.0644   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.1      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.00745   |
| loss/critic1                       | 82.1      |
| loss/critic2                       | 79.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1743000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | 1.17      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -676      |
| loss/alpha                         | -0.0657   |
| loss/critic1                       | 60.7      |
| loss/critic2                       | 60.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1744000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0079
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.34e-09 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | -0.563    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.0704   |
| loss/critic1                       | 62.4      |
| loss/critic2                       | 60.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1745000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.875    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -677     |
| loss/alpha                         | 0.0294   |
| loss/critic1                       | 45.2     |
| loss/critic2                       | 43.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1746000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0165
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.59e-11 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.103     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.57      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0558    |
| loss/critic1                       | 85.3      |
| loss/critic2                       | 84.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1747000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.56e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.343    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.7      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0609    |
| loss/critic1                       | 55.3      |
| loss/critic2                       | 54.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1748000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.03e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.983    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.9      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.161    |
| loss/critic1                       | 106       |
| loss/critic2                       | 103       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1749000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.576    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.00994 |
| loss/critic1                       | 72       |
| loss/critic2                       | 70.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1750000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.73e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | 0.409     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -680      |
| loss/alpha                         | 0.0245    |
| loss/critic1                       | 144       |
| loss/critic2                       | 144       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1751000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.891   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.155    |
| loss/critic1                       | 130      |
| loss/critic2                       | 129      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1752000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.9e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 0.147    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.0305   |
| loss/critic1                       | 171      |
| loss/critic2                       | 165      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1753000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.113   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -682     |
| loss/alpha                         | -0.112   |
| loss/critic1                       | 73.3     |
| loss/critic2                       | 72       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1754000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0189
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.384    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0215   |
| loss/critic1                       | 148       |
| loss/critic2                       | 149       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1755000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0085
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | 0.288     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0295    |
| loss/critic1                       | 92.1      |
| loss/critic2                       | 90.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1756000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0104
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.62e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | 0.422     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.00653  |
| loss/critic1                       | 139       |
| loss/critic2                       | 140       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1757000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.76e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | 1.49      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.173     |
| loss/critic1                       | 153       |
| loss/critic2                       | 152       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1758000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-09 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.241     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 13.5      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0249   |
| loss/critic1                       | 200       |
| loss/critic2                       | 202       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1759000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.76e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | -0.679    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 4.61      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.025    |
| loss/critic1                       | 181       |
| loss/critic2                       | 174       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1760000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -1.05    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0755  |
| loss/critic1                       | 145      |
| loss/critic2                       | 136      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1761000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0205
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.82e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | -0.0508   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 17.7      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0234   |
| loss/critic1                       | 133       |
| loss/critic2                       | 136       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1762000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.54e-11 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -1.12     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 20.3      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.166    |
| loss/critic1                       | 162       |
| loss/critic2                       | 161       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1763000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | 0.888     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00174   |
| loss/critic1                       | 234       |
| loss/critic2                       | 234       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1764000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9947
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-09  |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | 0.27     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0339   |
| loss/critic1                       | 126      |
| loss/critic2                       | 124      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1765000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9956
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.49e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | -0.706    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0511    |
| loss/critic1                       | 134       |
| loss/critic2                       | 131       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1766000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9821
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.44e-09 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 1.13     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.1     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0382   |
| loss/critic1                       | 97.5     |
| loss/critic2                       | 100      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1767000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9909
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.95e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.352    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0036    |
| loss/critic1                       | 119       |
| loss/critic2                       | 119       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1768000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0034
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.423    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0473  |
| loss/critic1                       | 137      |
| loss/critic2                       | 141      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1769000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.0788  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83.1     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0149   |
| loss/critic1                       | 151      |
| loss/critic2                       | 153      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1770000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.78e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.281    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.00567 |
| loss/critic1                       | 142      |
| loss/critic2                       | 143      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1771000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.423    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0824   |
| loss/critic1                       | 202      |
| loss/critic2                       | 206      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1772000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.55e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.589    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0752   |
| loss/critic1                       | 140      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1773000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0096
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.49e-11 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | -0.31     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0475   |
| loss/critic1                       | 124       |
| loss/critic2                       | 122       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1774000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0170
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.83e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.939    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0751    |
| loss/critic1                       | 160       |
| loss/critic2                       | 157       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1775000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-11 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -1.26     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -684      |
| loss/alpha                         | -0.135    |
| loss/critic1                       | 146       |
| loss/critic2                       | 147       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1776000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.747   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0201   |
| loss/critic1                       | 144      |
| loss/critic2                       | 147      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1777000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9847
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.58e-10 |
| adv_dynamics_update/adv_log_prob   | 49       |
| adv_dynamics_update/adv_loss       | -0.307   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.223    |
| loss/critic1                       | 218      |
| loss/critic2                       | 217      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1778000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.41e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.618     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 10.4      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.225    |
| loss/critic1                       | 159       |
| loss/critic2                       | 160       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1779000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.123     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.102    |
| loss/critic1                       | 150       |
| loss/critic2                       | 144       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1780000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.0302   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 8.94      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.146    |
| loss/critic1                       | 164       |
| loss/critic2                       | 161       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1781000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.231    |
| loss/critic1                       | 273      |
| loss/critic2                       | 268      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1782000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.752    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 212      |
| loss/critic2                       | 212      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1783000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0065
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.35e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | 0.0242   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0399  |
| loss/critic1                       | 248      |
| loss/critic2                       | 241      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1784000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9947
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -0.22     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 11.5      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.186     |
| loss/critic1                       | 416       |
| loss/critic2                       | 410       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1785000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 48.9     |
| adv_dynamics_update/adv_loss       | -0.144   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.184    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.00843 |
| loss/critic1                       | 321      |
| loss/critic2                       | 322      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1786000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0073
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.06e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.5       |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.145    |
| loss/critic1                       | 338       |
| loss/critic2                       | 321       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1787000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | 1.05     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.9      |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0916  |
| loss/critic1                       | 153      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1788000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.358    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.036    |
| loss/critic1                       | 347       |
| loss/critic2                       | 340       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1789000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.14e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.705   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.161   |
| loss/critic1                       | 258      |
| loss/critic2                       | 255      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1790000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.866    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.183   |
| loss/critic1                       | 305      |
| loss/critic2                       | 301      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1791000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.49e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.195   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -685     |
| loss/alpha                         | 0.143    |
| loss/critic1                       | 219      |
| loss/critic2                       | 216      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1792000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.431   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.128    |
| loss/critic1                       | 115      |
| loss/critic2                       | 117      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1793000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.729   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.9     |
| eval/normalized_episode_reward_std | 6.19     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.15    |
| loss/critic1                       | 170      |
| loss/critic2                       | 168      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1794000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0002
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.242     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 10.1      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.193     |
| loss/critic1                       | 154       |
| loss/critic2                       | 151       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1795000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.9e-10  |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.276    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 17.9     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.18     |
| loss/critic1                       | 120      |
| loss/critic2                       | 118      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1796000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9820
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.62e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.208    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0611    |
| loss/critic1                       | 185       |
| loss/critic2                       | 186       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 1797000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0034
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.107    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.9      |
| eval/normalized_episode_reward_std | 3.55      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.107    |
| loss/critic1                       | 116       |
| loss/critic2                       | 119       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1798000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.5e-10  |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.768   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0202  |
| loss/critic1                       | 231      |
| loss/critic2                       | 234      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1799000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.714   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.7     |
| eval/normalized_episode_reward_std | 4.09     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0486   |
| loss/critic1                       | 121      |
| loss/critic2                       | 125      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1800000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 4.9934
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.22    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 24.5     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0226   |
| loss/critic1                       | 120      |
| loss/critic2                       | 123      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1801000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.252    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 7.82      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.18      |
| loss/critic1                       | 211       |
| loss/critic2                       | 211       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1802000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.56e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.89    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0478  |
| loss/critic1                       | 191      |
| loss/critic2                       | 188      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1803000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.0855  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.128   |
| loss/critic1                       | 180      |
| loss/critic2                       | 183      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1804000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | 0.913    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0686   |
| loss/critic1                       | 257      |
| loss/critic2                       | 266      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1805000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9971
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | 0.479     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0713   |
| loss/critic1                       | 179       |
| loss/critic2                       | 179       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1806000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.992    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.161    |
| loss/critic1                       | 144       |
| loss/critic2                       | 142       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1807000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.547    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 24.6     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.00461  |
| loss/critic1                       | 234      |
| loss/critic2                       | 240      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1808000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.662     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 20.2      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0332   |
| loss/critic1                       | 161       |
| loss/critic2                       | 161       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1809000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0199
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.42e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.936    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.1     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00218  |
| loss/critic1                       | 128      |
| loss/critic2                       | 125      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1810000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0028
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.325    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 14.9      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.154     |
| loss/critic1                       | 167       |
| loss/critic2                       | 163       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1811000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.12e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.279    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 23.9      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0907    |
| loss/critic1                       | 106       |
| loss/critic2                       | 106       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1812000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.251    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 16.9     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0371   |
| loss/critic1                       | 117      |
| loss/critic2                       | 120      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1813000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9998
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.8e-11 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.377    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0832   |
| loss/critic1                       | 92.9     |
| loss/critic2                       | 93.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1814000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.531   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 156      |
| loss/critic2                       | 156      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1815000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.13     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.184     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 25        |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0332    |
| loss/critic1                       | 159       |
| loss/critic2                       | 164       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1816000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0413
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.38e-11 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.57    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.184    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 21.3     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0562  |
| loss/critic1                       | 111      |
| loss/critic2                       | 112      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1817000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0261
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.143    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 25.2     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.143   |
| loss/critic1                       | 132      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1818000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.544    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.3      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0606    |
| loss/critic1                       | 119       |
| loss/critic2                       | 114       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1819000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.65e-11 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.693    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.3      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.031    |
| loss/critic1                       | 93.7      |
| loss/critic2                       | 92.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1820000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.89e-12 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 83       |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 148      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1821000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 4.9949
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.25e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.864     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.1      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0946    |
| loss/critic1                       | 123       |
| loss/critic2                       | 124       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1822000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.99e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.466    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0314   |
| loss/critic1                       | 249      |
| loss/critic2                       | 246      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1823000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-10  |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.549    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0632   |
| loss/critic1                       | 185      |
| loss/critic2                       | 183      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1824000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.975   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0464  |
| loss/critic1                       | 135      |
| loss/critic2                       | 139      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1825000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9766
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -1.02    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.184    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.149    |
| loss/critic1                       | 266      |
| loss/critic2                       | 265      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1826000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 4.9940
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.46     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.244   |
| loss/critic1                       | 133      |
| loss/critic2                       | 136      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1827000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9971
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.287   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0546  |
| loss/critic1                       | 291      |
| loss/critic2                       | 291      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1828000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0098
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.29e-11 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.466     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.108    |
| loss/critic1                       | 249       |
| loss/critic2                       | 246       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1829000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0028
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.09e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.141     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.000104  |
| loss/critic1                       | 252       |
| loss/critic2                       | 247       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1830000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9891
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | -0.337    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 188       |
| loss/critic2                       | 186       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1831000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.555    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 16.1     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0899   |
| loss/critic1                       | 214      |
| loss/critic2                       | 212      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1832000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.583   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.157   |
| loss/critic1                       | 234      |
| loss/critic2                       | 237      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1833000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.23e-09 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.515   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0363   |
| loss/critic1                       | 200      |
| loss/critic2                       | 194      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1834000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-11  |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | -0.471   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0244  |
| loss/critic1                       | 177      |
| loss/critic2                       | 180      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1835000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | 0.229     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0379    |
| loss/critic1                       | 210       |
| loss/critic2                       | 202       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1836000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.8e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0757   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.316    |
| loss/critic1                       | 180      |
| loss/critic2                       | 176      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1837000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0032
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.303    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.188    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 185      |
| loss/critic2                       | 182      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1838000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.11e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.883    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 157      |
| loss/critic2                       | 158      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1839000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0121
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | -0.54     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.3      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.117    |
| loss/critic1                       | 102       |
| loss/critic2                       | 100       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1840000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0012
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.24e-11 |
| adv_dynamics_update/adv_log_prob   | 48.6     |
| adv_dynamics_update/adv_loss       | -0.0187  |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82       |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0856  |
| loss/critic1                       | 181      |
| loss/critic2                       | 179      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1841000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0067
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.125    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.4     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0572  |
| loss/critic1                       | 97.4     |
| loss/critic2                       | 97       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1842000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.31e-13 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.363    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0683  |
| loss/critic1                       | 89.8     |
| loss/critic2                       | 90.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1843000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9967
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.0761   |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 14.1      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.00259   |
| loss/critic1                       | 55.4      |
| loss/critic2                       | 52.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1844000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.92e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.622    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 82.3      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0383    |
| loss/critic1                       | 111       |
| loss/critic2                       | 111       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1845000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.86e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.551    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 12.5     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0709  |
| loss/critic1                       | 50.9     |
| loss/critic2                       | 50.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1846000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0027
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.441   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 82.3     |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0593  |
| loss/critic1                       | 67.5     |
| loss/critic2                       | 65.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1847000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | -0.24    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0699   |
| loss/critic1                       | 87.3     |
| loss/critic2                       | 86.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1848000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-10  |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.172   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0746   |
| loss/critic1                       | 120      |
| loss/critic2                       | 117      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1849000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.555   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00686  |
| loss/critic1                       | 67.6     |
| loss/critic2                       | 65.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1850000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | 0.213     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.00474   |
| loss/critic1                       | 89.3      |
| loss/critic2                       | 86.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1851000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.11e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.26     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0834    |
| loss/critic1                       | 69.1      |
| loss/critic2                       | 67.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1852000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.715   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0661  |
| loss/critic1                       | 99.1     |
| loss/critic2                       | 98.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1853000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.49e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.696   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.00914  |
| loss/critic1                       | 81.8     |
| loss/critic2                       | 80       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1854000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.16e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.179    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 110      |
| loss/critic2                       | 106      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1855000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.04e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.422     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.4      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0768   |
| loss/critic1                       | 114       |
| loss/critic2                       | 115       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1856000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.5e-11  |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.307   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0589  |
| loss/critic1                       | 162      |
| loss/critic2                       | 164      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1857000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.31     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0249  |
| loss/critic1                       | 159      |
| loss/critic2                       | 157      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1858000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | -0.499    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0927    |
| loss/critic1                       | 144       |
| loss/critic2                       | 145       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1859000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -0.865    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.123     |
| loss/critic1                       | 130       |
| loss/critic2                       | 129       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1860000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.804   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00801  |
| loss/critic1                       | 142      |
| loss/critic2                       | 143      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1861000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.3e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.0887  |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -685     |
| loss/alpha                         | -0.254   |
| loss/critic1                       | 192      |
| loss/critic2                       | 191      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1862000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.373    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0523  |
| loss/critic1                       | 154      |
| loss/critic2                       | 152      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1863000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0060
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.35e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.247   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -685     |
| loss/alpha                         | 0.123    |
| loss/critic1                       | 199      |
| loss/critic2                       | 196      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1864000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | -0.311   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0554  |
| loss/critic1                       | 157      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1865000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.343    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.00115 |
| loss/critic1                       | 188      |
| loss/critic2                       | 190      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1866000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.23e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.521     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 10.3      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0667    |
| loss/critic1                       | 179       |
| loss/critic2                       | 179       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1867000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 119      |
| loss/critic2                       | 120      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1868000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.807   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -686     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 184      |
| loss/critic2                       | 185      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1869000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.618   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.15     |
| loss/critic1                       | 166      |
| loss/critic2                       | 163      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1870000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.672    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 16.6     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0792   |
| loss/critic1                       | 126      |
| loss/critic2                       | 125      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1871000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.365   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0609  |
| loss/critic1                       | 154      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1872000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.4e-09 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.0341   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0803  |
| loss/critic1                       | 82.1     |
| loss/critic2                       | 80.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1873000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.84e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.24     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.136    |
| loss/critic1                       | 59        |
| loss/critic2                       | 60        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1874000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.593     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.00188  |
| loss/critic1                       | 96.1      |
| loss/critic2                       | 97.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1875000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0170
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.08e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 1.38      |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 8.81      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0397   |
| loss/critic1                       | 89        |
| loss/critic2                       | 83.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1876000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1      |
| adv_dynamics_update/adv_loss       | -0.636    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.009     |
| loss/critic1                       | 65.5      |
| loss/critic2                       | 63.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1877000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 1.31     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.00619 |
| loss/critic1                       | 64.9     |
| loss/critic2                       | 64.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1878000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.435   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 15.1     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0752  |
| loss/critic1                       | 86.7     |
| loss/critic2                       | 86.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1879000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 0.242     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 7.1       |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0967    |
| loss/critic1                       | 76        |
| loss/critic2                       | 79.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1880000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0060
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.71e-11 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.446     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0344    |
| loss/critic1                       | 113       |
| loss/critic2                       | 111       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1881000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | -0.564   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0762   |
| loss/critic1                       | 93.6     |
| loss/critic2                       | 93.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1882000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | -0.00395 |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.12     |
| loss/critic1                       | 135      |
| loss/critic2                       | 139      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1883000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.79     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0368   |
| loss/critic1                       | 92.4     |
| loss/critic2                       | 91.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1884000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.6e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.964   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 5.72     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.00901 |
| loss/critic1                       | 103      |
| loss/critic2                       | 105      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1885000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0110
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | -0.0539   |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0329   |
| loss/critic1                       | 141       |
| loss/critic2                       | 140       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1886000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.73e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.46     |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 23.7     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.00366  |
| loss/critic1                       | 192      |
| loss/critic2                       | 195      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1887000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0022
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.81e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.428    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 18.1      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0413    |
| loss/critic1                       | 224       |
| loss/critic2                       | 227       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1888000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.0797   |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 7.34      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0189   |
| loss/critic1                       | 139       |
| loss/critic2                       | 140       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1889000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-09 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.141   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.005   |
| loss/critic1                       | 170      |
| loss/critic2                       | 167      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1890000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.017   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0333   |
| loss/critic1                       | 99       |
| loss/critic2                       | 98       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1891000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0303
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.24e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.0156    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 13.7      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.044    |
| loss/critic1                       | 82.6      |
| loss/critic2                       | 85.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1892000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5      |
| adv_dynamics_update/adv_loss       | -0.347    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.152    |
| loss/critic1                       | 87        |
| loss/critic2                       | 87.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1893000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9953
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7      |
| adv_dynamics_update/adv_loss       | -0.607    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0117   |
| loss/critic1                       | 145       |
| loss/critic2                       | 143       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1894000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0047
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0724   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.125   |
| loss/critic1                       | 108      |
| loss/critic2                       | 109      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1895000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.806    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 4.75     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0378   |
| loss/critic1                       | 115      |
| loss/critic2                       | 116      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1896000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.69e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 0.0119   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.152    |
| loss/critic1                       | 79.3     |
| loss/critic2                       | 81       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1897000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.471   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0331   |
| loss/critic1                       | 124      |
| loss/critic2                       | 127      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1898000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.27e-10 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | 0.251     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.0663    |
| loss/critic1                       | 104       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1899000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 4.9985
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.35e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.292     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 9.12      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.124     |
| loss/critic1                       | 176       |
| loss/critic2                       | 180       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1900000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.5e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.95     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.114    |
| loss/critic1                       | 186      |
| loss/critic2                       | 188      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1901000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 249999, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9936
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | -0.576    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.186     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.9      |
| eval/normalized_episode_reward_std | 21.4      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0692   |
| loss/critic1                       | 175       |
| loss/critic2                       | 169       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1902000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | -0.427    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56.8      |
| eval/normalized_episode_reward_std | 30.1      |
| loss/actor                         | -690      |
| loss/alpha                         | -0.035    |
| loss/critic1                       | 193       |
| loss/critic2                       | 188       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1903000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.43e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | 0.0407   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.114   |
| loss/critic1                       | 176      |
| loss/critic2                       | 180      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1904000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0045
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | 0.586     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.8      |
| eval/normalized_episode_reward_std | 24.5      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.103    |
| loss/critic1                       | 273       |
| loss/critic2                       | 258       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1905000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.253     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.175     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 23.7      |
| loss/actor                         | -689      |
| loss/alpha                         | 0.037     |
| loss/critic1                       | 262       |
| loss/critic2                       | 262       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1906000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64       |
| eval/normalized_episode_reward_std | 26.1     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.155    |
| loss/critic1                       | 245      |
| loss/critic2                       | 258      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1907000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0354   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.186    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 255      |
| loss/critic2                       | 259      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1908000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0052
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.1e-10  |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.663   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 4.04     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.00924  |
| loss/critic1                       | 208      |
| loss/critic2                       | 202      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1909000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.47e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -0.938   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.198   |
| loss/critic1                       | 247      |
| loss/critic2                       | 239      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1910000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0036
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.69e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | -0.713   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0947  |
| loss/critic1                       | 183      |
| loss/critic2                       | 186      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1911000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0094
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.51e-11 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.349    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -688      |
| loss/alpha                         | -0.094    |
| loss/critic1                       | 221       |
| loss/critic2                       | 220       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1912000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0017
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.307   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 12.6     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0928   |
| loss/critic1                       | 236      |
| loss/critic2                       | 235      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1913000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0011
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | 0.119     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.067     |
| loss/critic1                       | 147       |
| loss/critic2                       | 144       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1914000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0008
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.98e-10 |
| adv_dynamics_update/adv_log_prob   | 48.4      |
| adv_dynamics_update/adv_loss       | 0.0732    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.6      |
| eval/normalized_episode_reward_std | 20.9      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0206   |
| loss/critic1                       | 238       |
| loss/critic2                       | 240       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1915000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 48.9      |
| adv_dynamics_update/adv_loss       | -0.416    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 20.1      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.00244  |
| loss/critic1                       | 319       |
| loss/critic2                       | 325       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1916000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | -0.0769  |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -687     |
| loss/alpha                         | 0.107    |
| loss/critic1                       | 403      |
| loss/critic2                       | 410      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1917000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0124
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.82e-11 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.351     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0223    |
| loss/critic1                       | 231       |
| loss/critic2                       | 229       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1918000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.24e-11 |
| adv_dynamics_update/adv_log_prob   | 48.7      |
| adv_dynamics_update/adv_loss       | 0.584     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.53      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0856    |
| loss/critic1                       | 268       |
| loss/critic2                       | 270       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1919000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0047
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3     |
| adv_dynamics_update/adv_loss       | 1.69     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.187    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0397   |
| loss/critic1                       | 462      |
| loss/critic2                       | 448      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1920000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.48e-11 |
| adv_dynamics_update/adv_log_prob   | 48.7     |
| adv_dynamics_update/adv_loss       | -0.0434  |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0361  |
| loss/critic1                       | 260      |
| loss/critic2                       | 262      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1921000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9985
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5e-10   |
| adv_dynamics_update/adv_log_prob   | 48.4     |
| adv_dynamics_update/adv_loss       | 0.204    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 282      |
| loss/critic2                       | 274      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1922000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 48.8      |
| adv_dynamics_update/adv_loss       | -1.2      |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -685      |
| loss/alpha                         | -0.124    |
| loss/critic1                       | 333       |
| loss/critic2                       | 328       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1923000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0180
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.9e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.12     |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.172    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.332   |
| loss/critic1                       | 296      |
| loss/critic2                       | 302      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1924000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 4.9972
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.24e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.133    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.152    |
| loss/critic1                       | 407       |
| loss/critic2                       | 406       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1925000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.532   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.167    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.194    |
| loss/critic1                       | 297      |
| loss/critic2                       | 292      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1926000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0211
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.247    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.167     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0638   |
| loss/critic1                       | 275       |
| loss/critic2                       | 276       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1927000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9929
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-09 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.195     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.1      |
| eval/normalized_episode_reward_std | 3.47      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.439     |
| loss/critic1                       | 383       |
| loss/critic2                       | 382       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1928000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -1.77    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0953   |
| loss/critic1                       | 284      |
| loss/critic2                       | 287      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 1929000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.33e-11 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.356   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.7      |
| loss/actor                         | -685     |
| loss/alpha                         | 0.133    |
| loss/critic1                       | 239      |
| loss/critic2                       | 235      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1930000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 4.9995
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.0841   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.185    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 24.9     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.12    |
| loss/critic1                       | 237      |
| loss/critic2                       | 222      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1931000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0109
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.84e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.243    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.045   |
| loss/critic1                       | 399      |
| loss/critic2                       | 395      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1932000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | 0.576    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.124   |
| loss/critic1                       | 321      |
| loss/critic2                       | 311      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1933000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.535   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.000319 |
| loss/critic1                       | 339      |
| loss/critic2                       | 346      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1934000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.31e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.369     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.5      |
| eval/normalized_episode_reward_std | 3.97      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0329    |
| loss/critic1                       | 227       |
| loss/critic2                       | 223       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1935000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 46.8      |
| adv_dynamics_update/adv_loss       | 0.295     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 23.3      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0234    |
| loss/critic1                       | 110       |
| loss/critic2                       | 110       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1936000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0108
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.73e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | 0.435    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0287  |
| loss/critic1                       | 130      |
| loss/critic2                       | 130      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1937000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.3e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.627   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 18.8     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0675   |
| loss/critic1                       | 203      |
| loss/critic2                       | 200      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1938000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0174
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.96e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.0536   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 8.12     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0812  |
| loss/critic1                       | 178      |
| loss/critic2                       | 178      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1939000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | 0.107     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 144       |
| loss/critic2                       | 140       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1940000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0181
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.104     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52        |
| eval/normalized_episode_reward_std | 31.2      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0188    |
| loss/critic1                       | 136       |
| loss/critic2                       | 135       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1941000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-09 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.293     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.8      |
| eval/normalized_episode_reward_std | 22.2      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00431   |
| loss/critic1                       | 88.8      |
| loss/critic2                       | 86.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1942000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-09 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.69     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0942  |
| loss/critic1                       | 135      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1943000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1      |
| adv_dynamics_update/adv_loss       | -0.11     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 9.68      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0319   |
| loss/critic1                       | 106       |
| loss/critic2                       | 103       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1944000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.5e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | 0.703    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 10.9     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.0429   |
| loss/critic1                       | 82       |
| loss/critic2                       | 84       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1945000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.1e-10  |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | -1.23    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.16    |
| loss/critic1                       | 113      |
| loss/critic2                       | 105      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1946000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.46e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.76     |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 4.58     |
| loss/actor                         | -685     |
| loss/alpha                         | 0.00954  |
| loss/critic1                       | 116      |
| loss/critic2                       | 121      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1947000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.605    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 20.7      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.00239   |
| loss/critic1                       | 92.1      |
| loss/critic2                       | 90.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1948000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0073
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-09 |
| adv_dynamics_update/adv_log_prob   | 48.8     |
| adv_dynamics_update/adv_loss       | 0.194    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 5.98     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0971   |
| loss/critic1                       | 127      |
| loss/critic2                       | 118      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1949000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0035
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | -0.308   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 5.76     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0952   |
| loss/critic1                       | 129      |
| loss/critic2                       | 127      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1950000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 249999, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 48.3      |
| adv_dynamics_update/adv_loss       | -0.234    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.57      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0475   |
| loss/critic1                       | 119       |
| loss/critic2                       | 116       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1951000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.99e-11 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | 0.0615    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.0111   |
| loss/critic1                       | 181       |
| loss/critic2                       | 180       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1952000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0024
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8     |
| adv_dynamics_update/adv_loss       | -0.0282  |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0979   |
| loss/critic1                       | 133      |
| loss/critic2                       | 133      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1953000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.544    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0185   |
| loss/critic1                       | 172      |
| loss/critic2                       | 169      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1954000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0061
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.75e-11 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | -0.432    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 23        |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0513    |
| loss/critic1                       | 129       |
| loss/critic2                       | 132       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1955000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0009
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.95e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.452    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.8     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0609   |
| loss/critic1                       | 97.9     |
| loss/critic2                       | 95       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1956000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9835
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | 0.449     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 4.23      |
| loss/actor                         | -687      |
| loss/alpha                         | 0.0612    |
| loss/critic1                       | 134       |
| loss/critic2                       | 137       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1957000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 5.0010
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3      |
| adv_dynamics_update/adv_loss       | -0.434    |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 12.9      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.123    |
| loss/critic1                       | 80.1      |
| loss/critic2                       | 83.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1958000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | -0.119   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0337  |
| loss/critic1                       | 84       |
| loss/critic2                       | 80.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1959000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.52e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.456    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.2      |
| eval/normalized_episode_reward_std | 34.5      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0402   |
| loss/critic1                       | 58.7      |
| loss/critic2                       | 58.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1960000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0172
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.22e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.0697    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 15.6      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0264    |
| loss/critic1                       | 64.2      |
| loss/critic2                       | 62.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1961000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 5.0041
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.57e-10 |
| adv_dynamics_update/adv_log_prob   | 47.8      |
| adv_dynamics_update/adv_loss       | -0.895    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0303    |
| loss/critic1                       | 54.7      |
| loss/critic2                       | 53.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1962000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0282
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.78e-12 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.306    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0258   |
| loss/critic1                       | 46        |
| loss/critic2                       | 43        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1963000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.76e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | 0.674     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.00898  |
| loss/critic1                       | 60.4      |
| loss/critic2                       | 60.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1964000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.07e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2      |
| adv_dynamics_update/adv_loss       | -0.211    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0482    |
| loss/critic1                       | 40.5      |
| loss/critic2                       | 40.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1965000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0068
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.693    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0124   |
| loss/critic1                       | 115      |
| loss/critic2                       | 116      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1966000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0120
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | 0.229     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 11.6      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0277   |
| loss/critic1                       | 109       |
| loss/critic2                       | 109       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1967000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9987
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.6e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | 0.179    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 86.4     |
| loss/critic2                       | 88.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1968000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0020
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.301     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.157    |
| loss/critic1                       | 116       |
| loss/critic2                       | 112       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1969000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0050
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.65e-10 |
| adv_dynamics_update/adv_log_prob   | 47.9      |
| adv_dynamics_update/adv_loss       | 0.288     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.173     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 4.36      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0133    |
| loss/critic1                       | 101       |
| loss/critic2                       | 101       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1970000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9979
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 49.1      |
| adv_dynamics_update/adv_loss       | -0.542    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0964    |
| loss/critic1                       | 167       |
| loss/critic2                       | 174       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1971000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0023
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 48.1     |
| adv_dynamics_update/adv_loss       | -0.773   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 147      |
| loss/critic2                       | 147      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1972000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-09  |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.256   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0998  |
| loss/critic1                       | 81       |
| loss/critic2                       | 78.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1973000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.46e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2      |
| adv_dynamics_update/adv_loss       | -0.0896   |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.174     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0496    |
| loss/critic1                       | 81.1      |
| loss/critic2                       | 83.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 1974000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | 0.188    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -688     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 107      |
| loss/critic2                       | 107      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1975000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0063
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.85e-11 |
| adv_dynamics_update/adv_log_prob   | 47.9     |
| adv_dynamics_update/adv_loss       | -0.937   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.00764  |
| loss/critic1                       | 120      |
| loss/critic2                       | 121      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1976000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.0921   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 18.3     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 113      |
| loss/critic2                       | 111      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1977000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9937
num rollout transitions: 250000, reward mean: 4.9954
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.79e-10 |
| adv_dynamics_update/adv_log_prob   | 47.2     |
| adv_dynamics_update/adv_loss       | -0.269   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 12.1     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.158    |
| loss/critic1                       | 150      |
| loss/critic2                       | 150      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1978000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9996
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | -0.757   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.186    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.165    |
| loss/critic1                       | 104      |
| loss/critic2                       | 102      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1979000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0086
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4      |
| adv_dynamics_update/adv_loss       | -0.901    |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 18.1      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.282    |
| loss/critic1                       | 111       |
| loss/critic2                       | 109       |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1980000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.0439  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 21.8     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0352   |
| loss/critic1                       | 73.9     |
| loss/critic2                       | 75.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1981000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.64e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.988    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.84     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 87.3     |
| loss/critic2                       | 84.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1982000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-11 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.117     |
| adv_dynamics_update/all_loss       | -57       |
| adv_dynamics_update/sl_loss        | -57       |
| alpha                              | 0.177     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 13.3      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.112    |
| loss/critic1                       | 68.7      |
| loss/critic2                       | 66.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1983000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0104
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6     |
| adv_dynamics_update/adv_loss       | -0.017   |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0114   |
| loss/critic1                       | 68.6     |
| loss/critic2                       | 66.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1984000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.05e-11 |
| adv_dynamics_update/adv_log_prob   | 47.1     |
| adv_dynamics_update/adv_loss       | 0.895    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 76.5     |
| loss/critic2                       | 76.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1985000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9979
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.81e-11 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | -0.671   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0362   |
| loss/critic1                       | 60.9     |
| loss/critic2                       | 59.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1986000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | -0.345   |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.00402  |
| loss/critic1                       | 55.8     |
| loss/critic2                       | 55.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1987000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-11  |
| adv_dynamics_update/adv_log_prob   | 48.2     |
| adv_dynamics_update/adv_loss       | 0.483    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.176    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -688     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 68.6     |
| loss/critic2                       | 67.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1988000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 48.6      |
| adv_dynamics_update/adv_loss       | 0.385     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0508   |
| loss/critic1                       | 86.7      |
| loss/critic2                       | 89.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1989000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.13e-10 |
| adv_dynamics_update/adv_log_prob   | 48.5     |
| adv_dynamics_update/adv_loss       | -0.73    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.46     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0723  |
| loss/critic1                       | 53.8     |
| loss/critic2                       | 53       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1990000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0008
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 48        |
| adv_dynamics_update/adv_loss       | 0.373     |
| adv_dynamics_update/all_loss       | -56.9     |
| adv_dynamics_update/sl_loss        | -56.9     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 6.72      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0615   |
| loss/critic1                       | 40.6      |
| loss/critic2                       | 40.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1991000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.64e-10 |
| adv_dynamics_update/adv_log_prob   | 47.6      |
| adv_dynamics_update/adv_loss       | 0.267     |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.17      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 20.4      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0134    |
| loss/critic1                       | 70.6      |
| loss/critic2                       | 71.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1992000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.56e-11 |
| adv_dynamics_update/adv_log_prob   | 48       |
| adv_dynamics_update/adv_loss       | 0.914    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 107      |
| loss/critic2                       | 105      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1993000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 47.3     |
| adv_dynamics_update/adv_loss       | 0.444    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.121    |
| loss/critic1                       | 71.9     |
| loss/critic2                       | 70.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1994000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.316    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.178    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.202    |
| loss/critic1                       | 118      |
| loss/critic2                       | 116      |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1995000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.587    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.179    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0726  |
| loss/critic1                       | 57.7     |
| loss/critic2                       | 59.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 1996000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.68e-11 |
| adv_dynamics_update/adv_log_prob   | 47.5      |
| adv_dynamics_update/adv_loss       | -0.582    |
| adv_dynamics_update/all_loss       | -57.1     |
| adv_dynamics_update/sl_loss        | -57.1     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.9      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -690      |
| loss/alpha                         | -0.0673   |
| loss/critic1                       | 101       |
| loss/critic2                       | 96.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1997000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.4e-11 |
| adv_dynamics_update/adv_log_prob   | 47.4     |
| adv_dynamics_update/adv_loss       | 0.113    |
| adv_dynamics_update/all_loss       | -57      |
| adv_dynamics_update/sl_loss        | -57      |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0706   |
| loss/critic1                       | 66.7     |
| loss/critic2                       | 65.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1998000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 5.0079
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.09e-10 |
| adv_dynamics_update/adv_log_prob   | 47.5     |
| adv_dynamics_update/adv_loss       | -0.839   |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0995   |
| loss/critic1                       | 83.5     |
| loss/critic2                       | 81.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 1999000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0029
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 47.7     |
| adv_dynamics_update/adv_loss       | 0.365    |
| adv_dynamics_update/all_loss       | -57.1    |
| adv_dynamics_update/sl_loss        | -57.1    |
| alpha                              | 0.181    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0465  |
| loss/critic1                       | 66.4     |
| loss/critic2                       | 65.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 2000000  |
----------------------------------------------------------------------------------
total time: 128392.88s
